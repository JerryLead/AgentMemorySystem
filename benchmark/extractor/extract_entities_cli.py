import argparse
import logging
import json
from pathlib import Path
from typing import Dict, Any
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from entity_relation_extractor import EntityRelationExtractor
from dev.semantic_graph import SemanticGraph
from benchmark.llm_utils.llm_client import LLMClient
# from benchmark.extractor.entity_relation_extractor import EntityRelationExtractor
from benchmark.extractor.semantic_graph_integrator import SemanticGraphIntegrator

def setup_logging(log_level: str = "INFO"):
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('entity_extraction.log'),
            logging.StreamHandler()
        ]
    )


def extract_entities_from_locomo_dataset(graph: SemanticGraph, sample_limit: int = 10) -> Dict[str, Any]:
    """
    ä»LoCoMoå¯¹è¯ä¸­æ‰¹é‡æŠ½å–å®ä½“å…³ç³»
    
    Args:
        graph: å·²åŠ è½½LoCoMoæ•°æ®çš„è¯­ä¹‰å›¾
        sample_limit: é™åˆ¶å¤„ç†çš„å¯¹è¯å•å…ƒæ•°é‡
        
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    integrator = SemanticGraphIntegrator(graph)
    
    # å®šä¹‰è¿‡æ»¤å™¨ï¼šåªå¤„ç†å¯¹è¯å†…å®¹ï¼Œä¸”æœªå¤„ç†è¿‡çš„
    def dialog_filter(unit):
        return (unit.metadata.get('data_source') == 'locomo_dialog' 
                and not unit.metadata.get('entities_extracted', False))
    
    # æ‰¹é‡æŠ½å–å®ä½“
    results = integrator.batch_extract_entities_from_space(
        space_name="locomo_dialogs",
        max_units=sample_limit,
        unit_filter=dialog_filter
    )
    
    return results

# def extract_from_locomo_dataset(dataset_path: str, sample_limit: int = None) -> Dict[str, Any]:
#     """ä»LoCoMoæ•°æ®é›†æŠ½å–å®ä½“å…³ç³»"""
#     from benchmark.task_eval.locomo_test_split import (
#         load_dataset, ingest_conversation_history
#     )
    
#     logging.info(f"åŠ è½½LoCoMoæ•°æ®é›†: {dataset_path}")
    
#     # åŠ è½½æ•°æ®é›†
#     raw_data = load_dataset(Path(dataset_path))
#     if not raw_data:
#         raise ValueError("æ•°æ®é›†åŠ è½½å¤±è´¥")
    
#     if sample_limit:
#         raw_data = raw_data[:sample_limit]
#         logging.info(f"é™åˆ¶å¤„ç†æ ·æœ¬æ•°é‡: {sample_limit}")
    
#     # åˆå§‹åŒ–è¯­ä¹‰å›¾
#     graph = SemanticGraph()
    
#     # æ³¨å…¥å¯¹è¯æ•°æ®
#     total_messages = ingest_conversation_history(graph, raw_data)
#     logging.info(f"æ³¨å…¥äº† {total_messages} ä¸ªå¯¹è¯æ¶ˆæ¯")
    
#     # æ„å»ºç´¢å¼•
#     graph.build_semantic_map_index()
    
#     # åˆå§‹åŒ–å®ä½“æŠ½å–é›†æˆå™¨
#     integrator = SemanticGraphIntegrator(graph)
    
#     # å®šä¹‰è¿‡æ»¤å™¨ï¼šåªå¤„ç†å¯¹è¯å†…å®¹
#     def dialog_filter(unit):
#         return unit.metadata.get('data_source') == 'locomo_dialog'
    
#     # æ‰¹é‡æŠ½å–å®ä½“
#     results = integrator.batch_extract_entities_from_graph(
#         space_name="locomo_dialogs",
#         max_units=100,  # é™åˆ¶å¤„ç†æ•°é‡
#         unit_filter=dialog_filter
#     )
    
#     # è·å–ç»Ÿè®¡ä¿¡æ¯
#     stats = integrator.get_entity_statistics()
    
#     # æ˜¾ç¤ºå›¾è°±ä¿¡æ¯
#     graph.display_graph_summary()
    
#     return {
#         "extraction_results": results,
#         "entity_statistics": stats,
#         "graph": graph
#     }

def extract_from_single_text(text: str, model_name: str = "deepseek-chat") -> Dict[str, Any]:
    """ä»å•ä¸ªæ–‡æœ¬æŠ½å–å®ä½“å…³ç³»"""
    logging.info("å¼€å§‹ä»å•ä¸ªæ–‡æœ¬æŠ½å–å®ä½“å…³ç³»")
    
    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å’ŒæŠ½å–å™¨
    llm_client = LLMClient(model_name=model_name)
    extractor = EntityRelationExtractor(llm_client)
    
    # æŠ½å–å®ä½“å’Œå…³ç³»
    entities, relationships, content_keywords = extractor.extract_entities_and_relations(text)
    
    return {
        "entities": [
            {
                "name": e.name,
                "type": e.entity_type,
                "description": e.description,
                "confidence": e.confidence
            } for e in entities
        ],
        "relationships": [
            {
                "source": r.source_entity,
                "target": r.target_entity,
                "type": r.relationship_type,
                "description": r.description,
                "keywords": r.keywords,
                "strength": r.strength
            } for r in relationships
        ],
        "content_keywords": content_keywords
    }

def extract_conv26_only(dataset_path: str, output_dir: str = None) -> Dict[str, Any]:
    """ä¸“é—¨å¤„ç†conv-26æ ·æœ¬"""
    from benchmark.task_eval.locomo_test_split import load_dataset, ingest_conversation_history
    from datetime import datetime
    
    # åŠ è½½æ•°æ®é›†
    raw_data = load_dataset(Path(dataset_path))
    if not raw_data:
        raise ValueError("æ•°æ®é›†åŠ è½½å¤±è´¥")
    
    # æ‰¾åˆ°conv-26æ ·æœ¬
    conv26_sample = None
    for sample in raw_data:
        if sample.get('sample_id') == 'conv-26':
            conv26_sample = sample
            break
    
    if not conv26_sample:
        raise ValueError("æœªæ‰¾åˆ°sample_idä¸ºconv-26çš„æ ·æœ¬")
    
    logging.info("æ‰¾åˆ°conv-26æ ·æœ¬ï¼Œå¼€å§‹å¤„ç†...")
    
    # åˆå§‹åŒ–è¯­ä¹‰å›¾
    graph = SemanticGraph()
    
    # æ³¨å…¥å¯¹è¯æ•°æ®
    total_messages = ingest_conversation_history(graph, [conv26_sample])
    logging.info(f"æ³¨å…¥äº† {total_messages} ä¸ªå¯¹è¯æ¶ˆæ¯")
    
    # æ„å»ºç´¢å¼•
    graph.build_semantic_map_index()
    
    # åˆå§‹åŒ–æŠ½å–å™¨
    integrator = SemanticGraphIntegrator(graph)
    
    # å®šä¹‰è¿‡æ»¤å™¨
    def conv26_filter(unit):
        return (unit.metadata.get('data_source') == 'locomo_dialog' 
                and unit.metadata.get('conversation_id') == 'conv-26'
                and not unit.metadata.get('entities_extracted', False))
    
    # æ‰§è¡Œå®ä½“å…³ç³»æŠ½å–
    extraction_results = integrator.batch_extract_entities_from_space(
        space_name="locomo_dialogs",
        max_units=50,  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
        unit_filter=conv26_filter
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    entity_stats = integrator.get_entity_statistics()
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if not output_dir:
        output_dir = Path(__file__).parent.parent / "results" / f"conv26_extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜è¯­ä¹‰å›¾
    graph_path = output_dir / "conv26_semantic_graph"
    graph.save_graph(str(graph_path))
    
    # å‡†å¤‡è¿”å›ç»“æœ
    results = {
        "sample_id": "conv-26",
        "total_messages": total_messages,
        "extraction_results": extraction_results,
        "entity_statistics": entity_stats,
        "graph_saved_path": str(graph_path),
        "output_directory": str(output_dir)
    }
    
    return results

# ä¿®æ”¹mainå‡½æ•°ï¼Œæ·»åŠ conv-26é€‰é¡¹
def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å®ä½“å…³ç³»æŠ½å–å·¥å…·")
    parser.add_argument("--mode", choices=["locomo", "text", "conv26"], required=True,
                       help="æŠ½å–æ¨¡å¼: locomo(æ•´ä¸ªæ•°æ®é›†) æˆ– text(å•ä¸ªæ–‡æœ¬) æˆ– conv26(ä¸“é—¨å¤„ç†conv-26)")
    parser.add_argument("--input", required=True,
                       help="è¾“å…¥: LoCoMoæ•°æ®é›†è·¯å¾„ æˆ– æ–‡æœ¬å†…å®¹")
    parser.add_argument("--output", help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("--model", default="deepseek-chat",
                       help="LLMæ¨¡å‹åç§°")
    parser.add_argument("--sample-limit", type=int,
                       help="é™åˆ¶å¤„ç†çš„æ ·æœ¬æ•°é‡(ä»…ç”¨äºlocomoæ¨¡å¼)")
    parser.add_argument("--log-level", default="INFO",
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="æ—¥å¿—çº§åˆ«")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)
    
    try:
        if args.mode == "conv26":
            results = extract_conv26_only(args.input, args.output)
            
            # ä¿å­˜ç»“æœæ‘˜è¦
            summary_file = Path(results["output_directory"]) / "extraction_summary.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"\nğŸ‰ Conv-26å®ä½“å…³ç³»æŠ½å–å®Œæˆ!")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {results['output_directory']}")
            print(f"ğŸ“Š æ€»æ¶ˆæ¯æ•°: {results['total_messages']}")
            print(f"ğŸ“Š å¤„ç†å•å…ƒæ•°: {results['extraction_results']['processed']}")
            print(f"ğŸ“Š æŠ½å–å®ä½“æ•°: {results['extraction_results']['total_entities']}")
            print(f"ğŸ“Š æŠ½å–å…³ç³»æ•°: {results['extraction_results']['total_relationships']}")
            print(f"ğŸ“Š å®ä½“ç±»å‹åˆ†å¸ƒ: {results['entity_statistics']['entity_types']}")
            print(f"ğŸ“Š å…³ç³»ç±»å‹åˆ†å¸ƒ: {results['entity_statistics']['relationship_types']}")
            
        elif args.mode == "locomo":
            # ç°æœ‰çš„locomoå¤„ç†é€»è¾‘
            pass
        elif args.mode == "text":
            # ç°æœ‰çš„textå¤„ç†é€»è¾‘
            results = extract_from_single_text(args.input, args.model)
        
        logging.info("ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
        
    except Exception as e:
        logging.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()