import argparse
import logging
import json
from pathlib import Path
from typing import Dict, Any
import sys
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from dev.semantic_graph import SemanticGraph
from dev.memory_unit import MemoryUnit
from benchmark.llm_utils.llm_client import LLMClient
from benchmark.extractor.entity_relation_extractor import EntityRelationExtractor
from benchmark.extractor.semantic_graph_integrator import SemanticGraphIntegrator

def setup_logging(log_level: str = "INFO"):
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('entity_extraction.log'),
            logging.StreamHandler()
        ]
    )

def extract_entities_from_locomo_dataset(graph: SemanticGraph, sample_limit: int = 10) -> Dict[str, Any]:
    """
    ä»LoCoMoå¯¹è¯ä¸­æ‰¹é‡æŠ½å–å®ä½“å…³ç³» - é€‚é…æ–°æ¶æ„
    
    Args:
        graph: å·²åŠ è½½LoCoMoæ•°æ®çš„è¯­ä¹‰å›¾
        sample_limit: é™åˆ¶å¤„ç†çš„å¯¹è¯å•å…ƒæ•°é‡
        
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    integrator = SemanticGraphIntegrator(graph)
    
    # å®šä¹‰è¿‡æ»¤å™¨ï¼šåªå¤„ç†å¯¹è¯å†…å®¹ï¼Œä¸”æœªå¤„ç†è¿‡çš„
    def dialog_filter(unit):
        return (unit.metadata.get('data_source') == 'locomo_dialog' 
                and not unit.metadata.get('entities_extracted', False))
    
    # æ‰¹é‡æŠ½å–å®ä½“
    results = integrator.batch_extract_entities_from_space(
        space_name="locomo_dialogs",
        max_units=sample_limit,
        unit_filter=dialog_filter
    )
    
    return results

def extract_from_locomo_dataset(dataset_path: str, sample_limit: int = None) -> Dict[str, Any]:
    """ä»LoCoMoæ•°æ®é›†æŠ½å–å®ä½“å…³ç³» - é€‚é…æ–°æ¶æ„"""
    from benchmark.task_eval.locomo_test_split import (
        load_dataset, ingest_conversation_history
    )
    
    logging.info(f"åŠ è½½LoCoMoæ•°æ®é›†: {dataset_path}")
    
    # åŠ è½½æ•°æ®é›†
    raw_data = load_dataset(Path(dataset_path))
    if not raw_data:
        raise ValueError("æ•°æ®é›†åŠ è½½å¤±è´¥")
    
    if sample_limit:
        raw_data = raw_data[:sample_limit]
        logging.info(f"é™åˆ¶å¤„ç†æ ·æœ¬æ•°é‡: {sample_limit}")
    
    # åˆå§‹åŒ–è¯­ä¹‰å›¾ - ä½¿ç”¨æ–°æ¶æ„
    graph = SemanticGraph()
    
    # æ³¨å…¥å¯¹è¯æ•°æ®
    total_messages = ingest_conversation_history(graph, raw_data)
    logging.info(f"æ³¨å…¥äº† {total_messages} ä¸ªå¯¹è¯æ¶ˆæ¯")
    
    # æ„å»ºç´¢å¼•
    graph.build_semantic_map_index()
    
    # æ˜¾ç¤ºå›¾è°±æ‘˜è¦
    graph.display_graph_summary()
    
    # æ‰¹é‡æŠ½å–å®ä½“å…³ç³»
    extraction_results = extract_entities_from_locomo_dataset(graph, sample_limit=100)
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    integrator = SemanticGraphIntegrator(graph)
    entity_stats = integrator.get_entity_statistics()
    
    return {
        "extraction_results": extraction_results,
        "entity_statistics": entity_stats,
        "graph": graph,
        "total_messages": total_messages
    }

def extract_from_single_text(text: str, model_name: str = "deepseek-chat") -> Dict[str, Any]:
    """ä»å•ä¸ªæ–‡æœ¬æŠ½å–å®ä½“å…³ç³» - é€‚é…æ–°æ¶æ„"""
    logging.info("å¼€å§‹ä»å•ä¸ªæ–‡æœ¬æŠ½å–å®ä½“å…³ç³»")
    
    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å’ŒæŠ½å–å™¨
    llm_client = LLMClient(model_name=model_name)
    extractor = EntityRelationExtractor(llm_client)
    
    # æŠ½å–å®ä½“å’Œå…³ç³»
    entities, relationships, content_keywords = extractor.extract_entities_and_relations(text)
    
    # åˆ›å»ºè¯­ä¹‰å›¾å¹¶æ·»åŠ ç»“æœ
    graph = SemanticGraph()
    integrator = SemanticGraphIntegrator(graph)
    
    # åˆ›å»ºæ–‡æ¡£å•å…ƒ
    doc_unit = MemoryUnit(
        uid="single_text_document",
        raw_data={
            "text_content": text,
            "content_type": "single_text_input"
        },
        metadata={
            "data_source": "single_text",
            "created": datetime.now().isoformat()
        }
    )
    
    # æ·»åŠ åˆ°å›¾ä¸­
    graph.add_unit(doc_unit, space_names=["single_text_documents"])
    
    # æ·»åŠ å®ä½“å’Œå…³ç³»
    entity_id_map = integrator.add_entities_to_graph(entities, doc_unit.uid)
    relationship_count = integrator.add_relationships_to_graph(relationships, entity_id_map, doc_unit.uid)
    
    # æ„å»ºç´¢å¼•
    graph.build_semantic_map_index()
    
    return {
        "entities": [
            {
                "name": e.name,
                "type": e.entity_type,
                "description": e.description,
                "confidence": e.confidence,
                "source_text": e.source_text[:100] + "..." if len(e.source_text) > 100 else e.source_text
            } for e in entities
        ],
        "relationships": [
            {
                "source": r.source_entity,
                "target": r.target_entity,
                "type": r.relationship_type,
                "description": r.description,
                "keywords": r.keywords,
                "strength": r.strength,
                "source_text": r.source_text[:100] + "..." if len(r.source_text) > 100 else r.source_text
            } for r in relationships
        ],
        "content_keywords": content_keywords,
        "graph": graph,
        "entity_count": len(entities),
        "relationship_count": relationship_count,
        "semantic_graph_info": {
            "total_memory_units": len(graph.semantic_map.memory_units),
            "total_memory_spaces": len(graph.semantic_map.memory_spaces),
            "networkx_nodes": graph.nx_graph.number_of_nodes(),
            "networkx_edges": graph.nx_graph.number_of_edges(),
            "faiss_vectors": graph.semantic_map.faiss_index.ntotal if graph.semantic_map.faiss_index else 0
        }
    }

def extract_conv26_only(dataset_path: str, output_dir: str = None) -> Dict[str, Any]:
    """ä¸“é—¨å¤„ç†conv-26æ ·æœ¬ - é€‚é…æ–°æ¶æ„"""
    from benchmark.task_eval.locomo_test_split import load_dataset, ingest_conversation_history
    from datetime import datetime
    
    # åŠ è½½æ•°æ®é›†
    raw_data = load_dataset(Path(dataset_path))
    if not raw_data:
        raise ValueError("æ•°æ®é›†åŠ è½½å¤±è´¥")
    
    # æ‰¾åˆ°conv-26æ ·æœ¬
    conv26_sample = None
    for sample in raw_data:
        if sample.get('sample_id') == 'conv-26':
            conv26_sample = sample
            break
    
    if not conv26_sample:
        raise ValueError("æœªæ‰¾åˆ°sample_idä¸ºconv-26çš„æ ·æœ¬")
    
    logging.info("æ‰¾åˆ°conv-26æ ·æœ¬ï¼Œå¼€å§‹å¤„ç†...")
    
    # åˆå§‹åŒ–è¯­ä¹‰å›¾ - ä½¿ç”¨æ–°æ¶æ„
    graph = SemanticGraph()
    
    # æ³¨å…¥å¯¹è¯æ•°æ®
    total_messages = ingest_conversation_history(graph, [conv26_sample])
    logging.info(f"æ³¨å…¥äº† {total_messages} ä¸ªå¯¹è¯æ¶ˆæ¯")
    
    # æ„å»ºç´¢å¼•
    graph.build_semantic_map_index()
    
    # æ˜¾ç¤ºå›¾è°±æ‘˜è¦
    graph.display_graph_summary()
    
    # åˆå§‹åŒ–æŠ½å–å™¨
    integrator = SemanticGraphIntegrator(graph)
    
    # å®šä¹‰è¿‡æ»¤å™¨
    def conv26_filter(unit):
        return (unit.metadata.get('data_source') == 'locomo_dialog' 
                and unit.metadata.get('conversation_id') == 'conv-26'
                and not unit.metadata.get('entities_extracted', False))
    
    # æ‰§è¡Œå®ä½“å…³ç³»æŠ½å–
    extraction_results = integrator.batch_extract_entities_from_space(
        space_name="locomo_dialogs",
        max_units=50,  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
        unit_filter=conv26_filter
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    entity_stats = integrator.get_entity_statistics()
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if not output_dir:
        output_dir = Path(__file__).parent.parent / "results" / f"conv26_extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜è¯­ä¹‰å›¾
    graph_path = output_dir / "conv26_semantic_graph"
    graph.save_graph(str(graph_path))
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_results = {
        "sample_id": "conv-26",
        "total_messages": total_messages,
        "extraction_results": extraction_results,
        "entity_statistics": entity_stats,
        "graph_structure": {
            "total_memory_units": len(graph.semantic_map.memory_units),
            "total_memory_spaces": len(graph.semantic_map.memory_spaces),
            "networkx_nodes": graph.nx_graph.number_of_nodes(),
            "networkx_edges": graph.nx_graph.number_of_edges(),
            "faiss_vectors": graph.semantic_map.faiss_index.ntotal if graph.semantic_map.faiss_index else 0
        },
        "memory_space_analysis": {
            space_name: len(space.get_unit_uids())
            for space_name, space in graph.semantic_map.memory_spaces.items()
        }
    }
    
    # ä¿å­˜ç»“æœæ–‡ä»¶
    results_file = output_dir / "conv26_extraction_results.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(detailed_results, f, ensure_ascii=False, indent=2, default=str)
    
    # å‡†å¤‡è¿”å›ç»“æœ
    results = {
        "sample_id": "conv-26",
        "total_messages": total_messages,
        "extraction_results": extraction_results,
        "entity_statistics": entity_stats,
        "graph_saved_path": str(graph_path),
        "output_directory": str(output_dir),
        "results_file": str(results_file),
        "graph_structure": detailed_results["graph_structure"],
        "memory_space_analysis": detailed_results["memory_space_analysis"]
    }
    
    return results

def save_extraction_results(results: Dict[str, Any], output_path: str, mode: str):
    """ä¿å­˜æŠ½å–ç»“æœåˆ°æ–‡ä»¶ - é€‚é…æ–°æ¶æ„"""
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # å‡†å¤‡ä¿å­˜æ•°æ®ï¼Œæ’é™¤ä¸èƒ½åºåˆ—åŒ–çš„å¯¹è±¡
    save_data = {
        "metadata": {
            "mode": mode,
            "created_at": datetime.now().isoformat(),
            "semantic_graph_version": "new_architecture"
        }
    }
    
    # æ ¹æ®æ¨¡å¼ä¿å­˜ä¸åŒçš„æ•°æ®
    if mode == "locomo":
        save_data.update({
            "total_messages": results.get("total_messages", 0),
            "extraction_results": results.get("extraction_results", {}),
            "entity_statistics": results.get("entity_statistics", {}),
            "graph_summary": {
                "total_memory_units": len(results["graph"].semantic_map.memory_units) if results.get("graph") else 0,
                "total_memory_spaces": len(results["graph"].semantic_map.memory_spaces) if results.get("graph") else 0,
                "networkx_nodes": results["graph"].nx_graph.number_of_nodes() if results.get("graph") else 0,
                "networkx_edges": results["graph"].nx_graph.number_of_edges() if results.get("graph") else 0,
                "faiss_vectors": results["graph"].semantic_map.faiss_index.ntotal if results.get("graph") and results["graph"].semantic_map.faiss_index else 0
            }
        })
    elif mode == "text":
        save_data.update({
            "entities": results.get("entities", []),
            "relationships": results.get("relationships", []),
            "content_keywords": results.get("content_keywords", []),
            "entity_count": results.get("entity_count", 0),
            "relationship_count": results.get("relationship_count", 0),
            "semantic_graph_info": results.get("semantic_graph_info", {})
        })
    elif mode == "conv26":
        save_data.update({
            "sample_id": results.get("sample_id", ""),
            "total_messages": results.get("total_messages", 0),
            "extraction_results": results.get("extraction_results", {}),
            "entity_statistics": results.get("entity_statistics", {}),
            "graph_structure": results.get("graph_structure", {}),
            "memory_space_analysis": results.get("memory_space_analysis", {}),
            "output_directory": results.get("output_directory", ""),
            "graph_saved_path": results.get("graph_saved_path", "")
        })
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2, default=str)
    
    logging.info(f"æŠ½å–ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

def main():
    """ä¸»å‡½æ•° - é€‚é…æ–°æ¶æ„"""
    parser = argparse.ArgumentParser(description="å®ä½“å…³ç³»æŠ½å–å·¥å…· (æ–°æ¶æ„ç‰ˆæœ¬)")
    parser.add_argument("--mode", choices=["locomo", "text", "conv26"], required=True,
                       help="æŠ½å–æ¨¡å¼: locomo(æ•´ä¸ªæ•°æ®é›†) æˆ– text(å•ä¸ªæ–‡æœ¬) æˆ– conv26(ä¸“é—¨å¤„ç†conv-26)")
    parser.add_argument("--input", required=True,
                       help="è¾“å…¥: LoCoMoæ•°æ®é›†è·¯å¾„ æˆ– æ–‡æœ¬å†…å®¹")
    parser.add_argument("--output", help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("--model", default="deepseek-chat",
                       help="LLMæ¨¡å‹åç§°")
    parser.add_argument("--sample-limit", type=int,
                       help="é™åˆ¶å¤„ç†çš„æ ·æœ¬æ•°é‡(ä»…ç”¨äºlocomoæ¨¡å¼)")
    parser.add_argument("--log-level", default="INFO",
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="æ—¥å¿—çº§åˆ«")
    parser.add_argument("--save-graph", action="store_true",
                       help="ä¿å­˜è¯­ä¹‰å›¾åˆ°ç£ç›˜")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)
    
    try:
        if args.mode == "conv26":
            results = extract_conv26_only(args.input, args.output)
            
            # ä¿å­˜ç»“æœæ‘˜è¦ï¼ˆå·²åœ¨å‡½æ•°å†…ä¿å­˜ï¼‰
            print(f"\nğŸ‰ Conv-26å®ä½“å…³ç³»æŠ½å–å®Œæˆ!")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {results['output_directory']}")
            print(f"ğŸ“Š æ€»æ¶ˆæ¯æ•°: {results['total_messages']}")
            print(f"ğŸ“Š å¤„ç†å•å…ƒæ•°: {results['extraction_results']['processed']}")
            print(f"ğŸ“Š æŠ½å–å®ä½“æ•°: {results['extraction_results']['total_entities']}")
            print(f"ğŸ“Š æŠ½å–å…³ç³»æ•°: {results['extraction_results']['total_relationships']}")
            print(f"ğŸ“Š å®ä½“ç±»å‹åˆ†å¸ƒ: {results['entity_statistics']['entity_types']}")
            print(f"ğŸ“Š å…³ç³»ç±»å‹åˆ†å¸ƒ: {results['entity_statistics']['relationship_types']}")
            print(f"ğŸ“Š å›¾ç»“æ„ä¿¡æ¯:")
            print(f"   - å†…å­˜å•å…ƒæ•°: {results['graph_structure']['total_memory_units']}")
            print(f"   - å†…å­˜ç©ºé—´æ•°: {results['graph_structure']['total_memory_spaces']}")
            print(f"   - NetworkXèŠ‚ç‚¹æ•°: {results['graph_structure']['networkx_nodes']}")
            print(f"   - NetworkXè¾¹æ•°: {results['graph_structure']['networkx_edges']}")
            print(f"   - FAISSå‘é‡æ•°: {results['graph_structure']['faiss_vectors']}")
            
        elif args.mode == "locomo":
            results = extract_from_locomo_dataset(args.input, args.sample_limit)
            
            # ä¿å­˜ç»“æœ
            if args.output:
                output_file = Path(args.output) / f"locomo_extraction_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                save_extraction_results(results, output_file, "locomo")
            
            # å¯é€‰ï¼šä¿å­˜è¯­ä¹‰å›¾
            if args.save_graph and args.output:
                graph_path = Path(args.output) / f"locomo_semantic_graph_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                results["graph"].save_graph(str(graph_path))
                print(f"ğŸ“ è¯­ä¹‰å›¾å·²ä¿å­˜åˆ°: {graph_path}")
            
            print(f"\nğŸ‰ LoCoMoæ•°æ®é›†å®ä½“å…³ç³»æŠ½å–å®Œæˆ!")
            print(f"ğŸ“Š æ€»æ¶ˆæ¯æ•°: {results['total_messages']}")
            print(f"ğŸ“Š å¤„ç†å•å…ƒæ•°: {results['extraction_results']['processed']}")
            print(f"ğŸ“Š æŠ½å–å®ä½“æ•°: {results['extraction_results']['total_entities']}")
            print(f"ğŸ“Š æŠ½å–å…³ç³»æ•°: {results['extraction_results']['total_relationships']}")
            print(f"ğŸ“Š å®ä½“ç±»å‹åˆ†å¸ƒ: {results['entity_statistics']['entity_types']}")
            print(f"ğŸ“Š å…³ç³»ç±»å‹åˆ†å¸ƒ: {results['entity_statistics']['relationship_types']}")
            
        elif args.mode == "text":
            results = extract_from_single_text(args.input, args.model)
            
            # ä¿å­˜ç»“æœ
            if args.output:
                output_file = Path(args.output) / f"text_extraction_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                save_extraction_results(results, output_file, "text")
            
            # å¯é€‰ï¼šä¿å­˜è¯­ä¹‰å›¾
            if args.save_graph and args.output:
                graph_path = Path(args.output) / f"text_semantic_graph_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                results["graph"].save_graph(str(graph_path))
                print(f"ğŸ“ è¯­ä¹‰å›¾å·²ä¿å­˜åˆ°: {graph_path}")
            
            print(f"\nğŸ‰ å•æ–‡æœ¬å®ä½“å…³ç³»æŠ½å–å®Œæˆ!")
            print(f"ğŸ“Š æŠ½å–å®ä½“æ•°: {results['entity_count']}")
            print(f"ğŸ“Š æŠ½å–å…³ç³»æ•°: {results['relationship_count']}")
            print(f"ğŸ“Š å…³é”®è¯æ•°: {len(results['content_keywords'])}")
            print(f"ğŸ“Š è¯­ä¹‰å›¾ä¿¡æ¯: {results['semantic_graph_info']}")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªç»“æœ
            print("\nğŸ“‹ å®ä½“ç¤ºä¾‹:")
            for i, entity in enumerate(results['entities'][:3]):
                print(f"  {i+1}. {entity['name']} ({entity['type']}): {entity['description']}")
            
            print("\nğŸ”— å…³ç³»ç¤ºä¾‹:")
            for i, rel in enumerate(results['relationships'][:3]):
                print(f"  {i+1}. {rel['source']} -[{rel['type']}]-> {rel['target']}")
        
        logging.info("ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
        
    except Exception as e:
        logging.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logging.debug(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()