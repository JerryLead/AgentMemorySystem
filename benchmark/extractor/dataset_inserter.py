import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import sys

sys.path.append(str(Path(__file__).parent.parent.parent))
from dev.semantic_graph import SemanticGraph
from dev.memory_unit import MemoryUnit


class ConversationSemanticStorage:
    """
    å¯¹è¯è¯­ä¹‰å­˜å‚¨å™¨ - æŒ‰ sample_id å°† LoCoMo æ•°æ®å­˜å‚¨åˆ° semantic_map/graph ä¸­
    æ”¯æŒåŸå§‹æ•°æ®å’ŒæŠ½å–ç»“æœçš„æ··åˆå­˜å‚¨ï¼Œå¯æŒ‡å®šå­˜å‚¨ç‰¹å®šå¯¹è¯
    """
    
    def __init__(self, output_dir: str = "benchmark/conversation_semantic_storage"):
        """åˆå§‹åŒ–å¯¹è¯è¯­ä¹‰å­˜å‚¨å™¨"""
        # é¦–å…ˆè®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–è¯­ä¹‰å›¾è°±
        self.semantic_graph = SemanticGraph()
        
        self.logger.info("ConversationSemanticStorage åˆå§‹åŒ–å®Œæˆ")
    
    # def store_conversation(self, 
    #                       sample_id: str,
    #                       raw_dataset_file: str = "benchmark/dataset/locomo/locomo10.json",
    #                       extracted_dataset_file: str = "benchmark/dataset/locomo/extraction/locomo_extracted_full_dataset.json",
    #                       include_raw: bool = True,
    #                       include_extracted: bool = True) -> Dict[str, Any]:
    #     """
    #     å­˜å‚¨æŒ‡å®šå¯¹è¯åˆ°è¯­ä¹‰å›¾è°±ä¸­
        
    #     Args:
    #         sample_id: å¯¹è¯ID (å¦‚ "conv-26", "conv-30")
    #         raw_dataset_file: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
    #         extracted_dataset_file: æŠ½å–ç»“æœæ•°æ®æ–‡ä»¶è·¯å¾„
    #         include_raw: æ˜¯å¦åŒ…å«åŸå§‹æ•°æ®
    #         include_extracted: æ˜¯å¦åŒ…å«æŠ½å–ç»“æœ
            
    #     Returns:
    #         å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
    #     """
    #     self.logger.info(f"ğŸš€ å¼€å§‹å­˜å‚¨å¯¹è¯ {sample_id}")
        
    #     # åˆå§‹åŒ–å­˜å‚¨ç»Ÿè®¡
    #     storage_stats = {
    #         "sample_id": sample_id,
    #         "include_raw": include_raw,
    #         "include_extracted": include_extracted,
    #         "raw_dataset_file": raw_dataset_file,
    #         "extracted_dataset_file": extracted_dataset_file,
    #         "storage_breakdown": {
    #             # åŸå§‹æ•°æ®
    #             "conversations": 0,
    #             "observations": 0,
    #             "events": 0,
    #             "summaries": 0,
    #             "qa_pairs": 0,
    #             # æŠ½å–æ•°æ®
    #             "entities": 0,
    #             "relationships": 0,
    #             "keywords": 0,
    #             "statistics": 0
    #         },
    #         "namespace_usage": {},
    #         "processing_time": {
    #             "start_time": datetime.now().isoformat()
    #         }
    #     }
        
    #     start_time = datetime.now()
        
    #     try:
    #         # åŠ è½½æ•°æ®
    #         raw_data = None
    #         extracted_data = None
            
    #         if include_raw:
    #             raw_data = self._load_raw_conversation(raw_dataset_file, sample_id)
    #             if raw_data:
    #                 self.logger.info(f"âœ… æˆåŠŸåŠ è½½åŸå§‹æ•°æ®: {sample_id}")
    #             else:
    #                 self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŸå§‹æ•°æ®: {sample_id}")
            
    #         if include_extracted:
    #             extracted_data = self._load_extracted_conversation(extracted_dataset_file, sample_id)
    #             if extracted_data:
    #                 self.logger.info(f"âœ… æˆåŠŸåŠ è½½æŠ½å–æ•°æ®: {sample_id}")
    #             else:
    #                 self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æŠ½å–æ•°æ®: {sample_id}")
            
    #         # å­˜å‚¨åŸå§‹æ•°æ®
    #         if raw_data:
    #             raw_stats = self._store_raw_conversation_data(raw_data, sample_id)
    #             for key, value in raw_stats.items():
    #                 storage_stats["storage_breakdown"][key] += value
            
    #         # å­˜å‚¨æŠ½å–ç»“æœ
    #         if extracted_data:
    #             extracted_stats = self._store_extracted_conversation_data(extracted_data, sample_id)
    #             for key, value in extracted_stats.items():
    #                 storage_stats["storage_breakdown"][key] += value
                
    #             # å»ºç«‹å®ä½“å…³ç³»
    #             self._establish_entity_relationships(extracted_data, sample_id)
            
    #         # æ„å»ºè¯­ä¹‰ç´¢å¼•
    #         self.logger.info("æ„å»ºè¯­ä¹‰ç´¢å¼•...")
    #         self.semantic_graph.build_semantic_map_index()
            
    #         # ç»Ÿè®¡å‘½åç©ºé—´ä½¿ç”¨æƒ…å†µ
    #         storage_stats["namespace_usage"] = self._get_namespace_usage_stats()
            
    #         end_time = datetime.now()
    #         storage_stats["processing_time"]["end_time"] = end_time.isoformat()
    #         storage_stats["processing_time"]["duration_seconds"] = (end_time - start_time).total_seconds()
            
    #         # ä¿å­˜å­˜å‚¨ç»Ÿè®¡
    #         stats_file = self.output_dir / f"{sample_id}_storage_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    #         with open(stats_file, 'w', encoding='utf-8') as f:
    #             json.dump(storage_stats, f, ensure_ascii=False, indent=2)
            
    #         self.logger.info(f"ğŸ‰ å¯¹è¯ {sample_id} å­˜å‚¨å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯ä¿å­˜è‡³: {stats_file}")
    #         return storage_stats
            
    #     except Exception as e:
    #         self.logger.error(f"âŒ å­˜å‚¨å¯¹è¯ {sample_id} å¤±è´¥: {e}")
    #         raise

    # æ›´æ–°å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯ç»“æ„
    def store_conversation(self, 
                        sample_id: str,
                        raw_dataset_file: str = "benchmark/dataset/locomo/locomo10.json",
                        extracted_dataset_file: str = "benchmark/dataset/locomo/extraction/locomo_extracted_full_dataset.json",
                        include_raw: bool = True,
                        include_extracted: bool = True) -> Dict[str, Any]:
        """
        å­˜å‚¨æŒ‡å®šå¯¹è¯åˆ°è¯­ä¹‰å›¾è°±ä¸­
        
        Args:
            sample_id: å¯¹è¯ID (å¦‚ "conv-26", "conv-30")
            raw_dataset_file: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
            extracted_dataset_file: æŠ½å–ç»“æœæ•°æ®æ–‡ä»¶è·¯å¾„
            include_raw: æ˜¯å¦åŒ…å«åŸå§‹æ•°æ®
            include_extracted: æ˜¯å¦åŒ…å«æŠ½å–ç»“æœ
            
        Returns:
            å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        """
        self.logger.info(f"ğŸš€ å¼€å§‹å­˜å‚¨å¯¹è¯ {sample_id} (QAæ•°æ®å°†ä¿ç•™ä½œä¸ºæµ‹è¯•é›†)")
        
        # åˆå§‹åŒ–å­˜å‚¨ç»Ÿè®¡
        storage_stats = {
            "sample_id": sample_id,
            "include_raw": include_raw,
            "include_extracted": include_extracted,
            "raw_dataset_file": raw_dataset_file,
            "extracted_dataset_file": extracted_dataset_file,
            "storage_breakdown": {
                # åŸå§‹æ•°æ®
                "conversations": 0,
                "observations": 0,
                "events": 0,
                "summaries": 0,
                # "qa_pairs": 0,  # ç§»é™¤QAç»Ÿè®¡
                # æŠ½å–æ•°æ®
                "entities": 0,
                "relationships": 0,
                "keywords": 0,
                "statistics": 0
            },
            "qa_info": {
                "total_qa_pairs": 0,
                "note": "QAæ•°æ®ä¿ç•™ä½œä¸ºæµ‹è¯•é›†ï¼Œæœªæ’å…¥è¯­ä¹‰å›¾è°±"
            },
            "namespace_usage": {},
            "processing_time": {
                "start_time": datetime.now().isoformat()
            }
        }
        
        start_time = datetime.now()
        
        try:
            # åŠ è½½æ•°æ®
            raw_data = None
            extracted_data = None
            
            if include_raw:
                raw_data = self._load_raw_conversation(raw_dataset_file, sample_id)
                if raw_data:
                    self.logger.info(f"âœ… æˆåŠŸåŠ è½½åŸå§‹æ•°æ®: {sample_id}")
                    # ç»Ÿè®¡QAæ•°æ®ä½†ä¸æ’å…¥
                    qa_data = raw_data.get('qa', [])
                    storage_stats["qa_info"]["total_qa_pairs"] = len(qa_data)
                    self.logger.info(f"ğŸ“Š å‘ç° {len(qa_data)} ä¸ªQAå¯¹ï¼Œå°†ä¿ç•™ä½œä¸ºæµ‹è¯•é›†")
                else:
                    self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŸå§‹æ•°æ®: {sample_id}")
            
            if include_extracted:
                extracted_data = self._load_extracted_conversation(extracted_dataset_file, sample_id)
                if extracted_data:
                    self.logger.info(f"âœ… æˆåŠŸåŠ è½½æŠ½å–æ•°æ®: {sample_id}")
                else:
                    self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æŠ½å–æ•°æ®: {sample_id}")
            
            # å­˜å‚¨åŸå§‹æ•°æ®ï¼ˆä¸åŒ…æ‹¬QAï¼‰
            if raw_data:
                raw_stats = self._store_raw_conversation_data(raw_data, sample_id)
                for key, value in raw_stats.items():
                    storage_stats["storage_breakdown"][key] += value
            
            # å­˜å‚¨æŠ½å–ç»“æœ
            if extracted_data:
                extracted_stats = self._store_extracted_conversation_data(extracted_data, sample_id)
                for key, value in extracted_stats.items():
                    storage_stats["storage_breakdown"][key] += value
                
                # å»ºç«‹å®ä½“å…³ç³»
                self._establish_entity_relationships(extracted_data, sample_id)
            
            # æ„å»ºè¯­ä¹‰ç´¢å¼•
            self.logger.info("æ„å»ºè¯­ä¹‰ç´¢å¼•...")
            self.semantic_graph.build_semantic_map_index()
            
            # ç»Ÿè®¡å‘½åç©ºé—´ä½¿ç”¨æƒ…å†µ
            storage_stats["namespace_usage"] = self._get_namespace_usage_stats()
            
            end_time = datetime.now()
            storage_stats["processing_time"]["end_time"] = end_time.isoformat()
            storage_stats["processing_time"]["duration_seconds"] = (end_time - start_time).total_seconds()
            
            # ä¿å­˜å­˜å‚¨ç»Ÿè®¡
            stats_file = self.output_dir / f"{sample_id}_storage_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(storage_stats, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ‰ å¯¹è¯ {sample_id} å­˜å‚¨å®Œæˆï¼QAæ•°æ®å·²ä¿ç•™ä½œä¸ºæµ‹è¯•é›†")
            self.logger.info(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯ä¿å­˜è‡³: {stats_file}")
            return storage_stats
            
        except Exception as e:
            self.logger.error(f"âŒ å­˜å‚¨å¯¹è¯ {sample_id} å¤±è´¥: {e}")
            raise
    
    # def store_all_conversations(self,
    #                            raw_dataset_file: str = "benchmark/dataset/locomo/locomo10.json",
    #                            extracted_dataset_file: str = "benchmark/dataset/locomo/extraction/locomo_extracted_full_dataset.json",
    #                            include_raw: bool = True,
    #                            include_extracted: bool = True) -> Dict[str, Any]:
    #     """
    #     å­˜å‚¨æ‰€æœ‰å¯¹è¯åˆ°è¯­ä¹‰å›¾è°±ä¸­
        
    #     Args:
    #         raw_dataset_file: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
    #         extracted_dataset_file: æŠ½å–ç»“æœæ•°æ®æ–‡ä»¶è·¯å¾„
    #         include_raw: æ˜¯å¦åŒ…å«åŸå§‹æ•°æ®
    #         include_extracted: æ˜¯å¦åŒ…å«æŠ½å–ç»“æœ
            
    #     Returns:
    #         æ‰€æœ‰å¯¹è¯çš„å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
    #     """
    #     self.logger.info("ğŸš€ å¼€å§‹å­˜å‚¨æ‰€æœ‰å¯¹è¯")
        
    #     # è·å–æ‰€æœ‰å¯ç”¨çš„sample_id
    #     all_sample_ids = set()
        
    #     if include_raw:
    #         with open(raw_dataset_file, 'r', encoding='utf-8') as f:
    #             raw_dataset = json.load(f)
    #         for sample in raw_dataset:
    #             sample_id = sample.get('sample_id')
    #             if sample_id:
    #                 all_sample_ids.add(sample_id)
        
    #     if include_extracted:
    #         with open(extracted_dataset_file, 'r', encoding='utf-8') as f:
    #             extracted_dataset = json.load(f)
    #         extracted_samples = extracted_dataset.get("samples", {})
    #         all_sample_ids.update(extracted_samples.keys())
        
    #     self.logger.info(f"å‘ç° {len(all_sample_ids)} ä¸ªå¯¹è¯: {sorted(all_sample_ids)}")
        
    #     # å­˜å‚¨æ¯ä¸ªå¯¹è¯
    #     all_stats = {
    #         "total_conversations": len(all_sample_ids),
    #         "processed_conversations": 0,
    #         "failed_conversations": [],
    #         "conversation_stats": {},
    #         "overall_storage_breakdown": {
    #             "conversations": 0,
    #             "observations": 0,
    #             "events": 0,
    #             "summaries": 0,
    #             "qa_pairs": 0,
    #             "entities": 0,
    #             "relationships": 0,
    #             "keywords": 0,
    #             "statistics": 0
    #         },
    #         "processing_time": {
    #             "start_time": datetime.now().isoformat()
    #         }
    #     }
        
    #     start_time = datetime.now()
        
    #     for sample_id in sorted(all_sample_ids):
    #         try:
    #             stats = self.store_conversation(
    #                 sample_id=sample_id,
    #                 raw_dataset_file=raw_dataset_file,
    #                 extracted_dataset_file=extracted_dataset_file,
    #                 include_raw=include_raw,
    #                 include_extracted=include_extracted
    #             )
                
    #             all_stats["conversation_stats"][sample_id] = stats
    #             all_stats["processed_conversations"] += 1
                
    #             # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
    #             for key, value in stats["storage_breakdown"].items():
    #                 all_stats["overall_storage_breakdown"][key] += value
                
    #             self.logger.info(f"âœ… {sample_id} å¤„ç†å®Œæˆ")
                
    #         except Exception as e:
    #             self.logger.error(f"âŒ {sample_id} å¤„ç†å¤±è´¥: {e}")
    #             all_stats["failed_conversations"].append(sample_id)
        
    #     end_time = datetime.now()
    #     all_stats["processing_time"]["end_time"] = end_time.isoformat()
    #     all_stats["processing_time"]["duration_seconds"] = (end_time - start_time).total_seconds()
        
    #     # ä¿å­˜æ€»ä½“ç»Ÿè®¡
    #     stats_file = self.output_dir / f"all_conversations_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    #     with open(stats_file, 'w', encoding='utf-8') as f:
    #         json.dump(all_stats, f, ensure_ascii=False, indent=2)
        
    #     self.logger.info(f"ğŸ‰ æ‰€æœ‰å¯¹è¯å­˜å‚¨å®Œæˆï¼æ€»ä½“ç»Ÿè®¡ä¿å­˜è‡³: {stats_file}")
    #     return all_stats

    # æ›´æ–° store_all_conversations æ–¹æ³•çš„ç»Ÿè®¡ç»“æ„
    def store_all_conversations(self,
                            raw_dataset_file: str = "benchmark/dataset/locomo/locomo10.json",
                            extracted_dataset_file: str = "benchmark/dataset/locomo/extraction/locomo_extracted_full_dataset.json",
                            include_raw: bool = True,
                            include_extracted: bool = True) -> Dict[str, Any]:
        """
        å­˜å‚¨æ‰€æœ‰å¯¹è¯åˆ°è¯­ä¹‰å›¾è°±ä¸­
        
        Args:
            raw_dataset_file: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
            extracted_dataset_file: æŠ½å–ç»“æœæ•°æ®æ–‡ä»¶è·¯å¾„
            include_raw: æ˜¯å¦åŒ…å«åŸå§‹æ•°æ®
            include_extracted: æ˜¯å¦åŒ…å«æŠ½å–ç»“æœ
            
        Returns:
            æ‰€æœ‰å¯¹è¯çš„å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        """
        self.logger.info("ğŸš€ å¼€å§‹å­˜å‚¨æ‰€æœ‰å¯¹è¯ (QAæ•°æ®å°†ä¿ç•™ä½œä¸ºæµ‹è¯•é›†)")
        
        # è·å–æ‰€æœ‰å¯ç”¨çš„sample_id
        all_sample_ids = set()
        
        if include_raw:
            with open(raw_dataset_file, 'r', encoding='utf-8') as f:
                raw_dataset = json.load(f)
            for sample in raw_dataset:
                sample_id = sample.get('sample_id')
                if sample_id:
                    all_sample_ids.add(sample_id)
        
        if include_extracted:
            with open(extracted_dataset_file, 'r', encoding='utf-8') as f:
                extracted_dataset = json.load(f)
            extracted_samples = extracted_dataset.get("samples", {})
            all_sample_ids.update(extracted_samples.keys())
        
        self.logger.info(f"å‘ç° {len(all_sample_ids)} ä¸ªå¯¹è¯: {sorted(all_sample_ids)}")
        
        # å­˜å‚¨æ¯ä¸ªå¯¹è¯
        all_stats = {
            "total_conversations": len(all_sample_ids),
            "processed_conversations": 0,
            "failed_conversations": [],
            "conversation_stats": {},
            "overall_storage_breakdown": {
                "conversations": 0,
                "observations": 0,
                "events": 0,
                "summaries": 0,
                # "qa_pairs": 0,  # ç§»é™¤QAç»Ÿè®¡
                "entities": 0,
                "relationships": 0,
                "keywords": 0,
                "statistics": 0
            },
            "overall_qa_info": {
                "total_qa_pairs": 0,
                "note": "æ‰€æœ‰QAæ•°æ®ä¿ç•™ä½œä¸ºæµ‹è¯•é›†ï¼Œæœªæ’å…¥è¯­ä¹‰å›¾è°±"
            },
            "processing_time": {
                "start_time": datetime.now().isoformat()
            }
        }
        
        start_time = datetime.now()
        
        for sample_id in sorted(all_sample_ids):
            try:
                stats = self.store_conversation(
                    sample_id=sample_id,
                    raw_dataset_file=raw_dataset_file,
                    extracted_dataset_file=extracted_dataset_file,
                    include_raw=include_raw,
                    include_extracted=include_extracted
                )
                
                all_stats["conversation_stats"][sample_id] = stats
                all_stats["processed_conversations"] += 1
                
                # ç´¯åŠ å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
                for key, value in stats["storage_breakdown"].items():
                    all_stats["overall_storage_breakdown"][key] += value
                
                # ç´¯åŠ QAä¿¡æ¯
                all_stats["overall_qa_info"]["total_qa_pairs"] += stats["qa_info"]["total_qa_pairs"]
                
                self.logger.info(f"âœ… {sample_id} å¤„ç†å®Œæˆ")
                
            except Exception as e:
                self.logger.error(f"âŒ {sample_id} å¤„ç†å¤±è´¥: {e}")
                all_stats["failed_conversations"].append(sample_id)
        
        end_time = datetime.now()
        all_stats["processing_time"]["end_time"] = end_time.isoformat()
        all_stats["processing_time"]["duration_seconds"] = (end_time - start_time).total_seconds()
        
        # ä¿å­˜æ€»ä½“ç»Ÿè®¡
        stats_file = self.output_dir / f"all_conversations_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(all_stats, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ‰ æ‰€æœ‰å¯¹è¯å­˜å‚¨å®Œæˆï¼")
        self.logger.info(f"ğŸ“Š æ€»è®¡ä¿ç•™ {all_stats['overall_qa_info']['total_qa_pairs']} ä¸ªQAå¯¹ä½œä¸ºæµ‹è¯•é›†")
        self.logger.info(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡ä¿å­˜è‡³: {stats_file}")
        return all_stats
    
    def _load_raw_conversation(self, file_path: str, sample_id: str) -> Optional[Dict]:
        """ä»åŸå§‹æ•°æ®é›†ä¸­åŠ è½½æŒ‡å®šå¯¹è¯"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
            
            for sample in dataset:
                if sample.get('sample_id') == sample_id:
                    return sample
            
            return None
        except Exception as e:
            self.logger.error(f"åŠ è½½åŸå§‹å¯¹è¯ {sample_id} å¤±è´¥: {e}")
            return None
    
    def _load_extracted_conversation(self, file_path: str, sample_id: str) -> Optional[Dict]:
        """ä»æŠ½å–ç»“æœæ•°æ®é›†ä¸­åŠ è½½æŒ‡å®šå¯¹è¯"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
            
            samples = dataset.get("samples", {})
            return samples.get(sample_id)
        except Exception as e:
            self.logger.error(f"åŠ è½½æŠ½å–å¯¹è¯ {sample_id} å¤±è´¥: {e}")
            return None
        
    # åœ¨ _store_raw_conversation_data æ–¹æ³•ä¸­ï¼Œæ³¨é‡Šæ‰ QA æ•°æ®å­˜å‚¨
    def _store_raw_conversation_data(self, sample: Dict, sample_id: str) -> Dict[str, int]:
        """å­˜å‚¨å•ä¸ªå¯¹è¯çš„åŸå§‹æ•°æ®"""
        stats = {
            "conversations": 0,
            "observations": 0,
            "events": 0,
            "summaries": 0,
            # "qa_pairs": 0  # ç§»é™¤QAç»Ÿè®¡
        }
        
        # åˆ›å»ºå¯¹è¯ä¸“ç”¨ç©ºé—´
        self.semantic_graph.create_memory_space_in_map(f"conversation_{sample_id}")
        self.semantic_graph.create_memory_space_in_map(f"raw_data_{sample_id}")
        
        # 1. å­˜å‚¨å¯¹è¯æ•°æ®
        conv_count = self._store_conversation_data(sample, sample_id)
        stats["conversations"] += conv_count
        
        # 2. å­˜å‚¨è§‚å¯Ÿè®°å½•
        obs_count = self._store_observation_data(sample, sample_id)
        stats["observations"] += obs_count
        
        # 3. å­˜å‚¨äº‹ä»¶è®°å½•
        event_count = self._store_event_data(sample, sample_id)
        stats["events"] += event_count
        
        # 4. å­˜å‚¨ä¼šè¯æ‘˜è¦
        summary_count = self._store_summary_data(sample, sample_id)
        stats["summaries"] += summary_count
        
        # 5. å­˜å‚¨é—®ç­”æ•°æ® - æ³¨é‡Šæ‰
        # qa_count = self._store_qa_data(sample, sample_id)
        # stats["qa_pairs"] += qa_count
        
        return stats
    
    # def _store_raw_conversation_data(self, sample: Dict, sample_id: str) -> Dict[str, int]:
    #     """å­˜å‚¨å•ä¸ªå¯¹è¯çš„åŸå§‹æ•°æ®"""
    #     stats = {
    #         "conversations": 0,
    #         "observations": 0,
    #         "events": 0,
    #         "summaries": 0,
    #         "qa_pairs": 0
    #     }
        
    #     # åˆ›å»ºå¯¹è¯ä¸“ç”¨ç©ºé—´
    #     self.semantic_graph.create_memory_space_in_map(f"conversation_{sample_id}")
    #     self.semantic_graph.create_memory_space_in_map(f"raw_data_{sample_id}")
        
    #     # 1. å­˜å‚¨å¯¹è¯æ•°æ®
    #     conv_count = self._store_conversation_data(sample, sample_id)
    #     stats["conversations"] += conv_count
        
    #     # 2. å­˜å‚¨è§‚å¯Ÿè®°å½•
    #     obs_count = self._store_observation_data(sample, sample_id)
    #     stats["observations"] += obs_count
        
    #     # 3. å­˜å‚¨äº‹ä»¶è®°å½•
    #     event_count = self._store_event_data(sample, sample_id)
    #     stats["events"] += event_count
        
    #     # 4. å­˜å‚¨ä¼šè¯æ‘˜è¦
    #     summary_count = self._store_summary_data(sample, sample_id)
    #     stats["summaries"] += summary_count
        
    #     # 5. å­˜å‚¨é—®ç­”æ•°æ®
    #     qa_count = self._store_qa_data(sample, sample_id)
    #     stats["qa_pairs"] += qa_count
        
    #     return stats
    
    def _store_extracted_conversation_data(self, sample_data: Dict, sample_id: str) -> Dict[str, int]:
        """å­˜å‚¨å•ä¸ªå¯¹è¯çš„æŠ½å–ç»“æœæ•°æ®"""
        stats = {
            "entities": 0,
            "relationships": 0,
            "keywords": 0,
            "statistics": 0
        }
        
        # åˆ›å»ºæŠ½å–æ•°æ®ä¸“ç”¨ç©ºé—´
        self.semantic_graph.create_memory_space_in_map(f"conversation_{sample_id}")
        self.semantic_graph.create_memory_space_in_map(f"extracted_data_{sample_id}")
        
        # 1. å­˜å‚¨æŠ½å–çš„å®ä½“
        entity_stats = self._store_extracted_entities(sample_data, sample_id)
        stats["entities"] += entity_stats
        
        # 2. å­˜å‚¨æŠ½å–çš„å…³ç³»
        rel_stats = self._store_extracted_relationships(sample_data, sample_id)
        stats["relationships"] += rel_stats
        
        # 3. å­˜å‚¨å…³é”®è¯
        keyword_stats = self._store_keywords(sample_data, sample_id)
        stats["keywords"] += keyword_stats
        
        # 4. å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        stats_count = self._store_statistics(sample_data, sample_id)
        stats["statistics"] += stats_count
        
        return stats
    
    def _store_conversation_data(self, sample: Dict, sample_id: str) -> int:
        """å­˜å‚¨å¯¹è¯æ•°æ®"""
        conversation = sample.get('conversation', {})
        if not conversation:
            return 0
        
        # è·å–è¯´è¯è€…ä¿¡æ¯
        speaker_a = conversation.get('speaker_a', 'Speaker A')
        speaker_b = conversation.get('speaker_b', 'Speaker B')
        
        # æŒ‰ä¼šè¯å­˜å‚¨å¯¹è¯
        session_keys = [k for k in conversation.keys() 
                       if k.startswith('session_') and not k.endswith('_date_time')]
        
        # 1. åˆ›å»ºå®Œæ•´å¯¹è¯è®°å¿†å•å…ƒ
        full_text_parts = []
        for session_key in sorted(session_keys, key=lambda x: int(x.split('_')[1])):
            session_messages = conversation.get(session_key, [])
            session_datetime = conversation.get(f"{session_key}_date_time", "")
            
            if session_messages and session_datetime:
                session_text = f"\n=== {session_key.upper()} ({session_datetime}) ===\n"
                for msg in session_messages:
                    if isinstance(msg, dict):
                        speaker = msg.get('speaker', 'Unknown')
                        content = msg.get('content', msg.get('text', ''))
                        session_text += f"{speaker}: {content}\n"
                full_text_parts.append(session_text)
        
        full_conversation = "\n".join(full_text_parts)
        
        if full_conversation.strip():
            unit = MemoryUnit(
                uid=f"{sample_id}_full_conversation",
                raw_data={
                    "text_content": full_conversation,
                    "speakers": [speaker_a, speaker_b],
                    "session_count": len(session_keys),
                    "total_length": len(full_conversation),
                    "sample_id": sample_id,
                    "data_type": "full_conversation",
                    "data_source": "raw"
                },
                metadata={
                    "conversation_id": sample_id,
                    "data_layer": "raw",
                    "content_type": "full_conversation",
                    "speakers": [speaker_a, speaker_b],
                    "created": datetime.now().isoformat()
                }
            )
            
            # æ·»åŠ åˆ°å¯¹è¯ä¸“ç”¨ç©ºé—´
            self.semantic_graph.add_unit(unit, space_names=[
                f"conversation_{sample_id}",  # å¯¹è¯ä¸“ç”¨ç©ºé—´
                f"raw_data_{sample_id}",      # åŸå§‹æ•°æ®ç©ºé—´
                "all_conversations"           # å…¨å±€å¯¹è¯ç©ºé—´
            ])
            
            return 1
        
        return 0
    
    def _store_observation_data(self, sample: Dict, sample_id: str) -> int:
        """å­˜å‚¨è§‚å¯Ÿè®°å½•"""
        observations = sample.get('observation', {})
        if not observations:
            return 0
        
        stored_count = 0
        
        for obs_session_key, obs_data in observations.items():
            if not isinstance(obs_data, dict):
                continue
                
            for speaker, obs_list in obs_data.items():
                if not isinstance(obs_list, list):
                    continue
                    
                for obs_idx, obs_item in enumerate(obs_list):
                    if isinstance(obs_item, list) and len(obs_item) >= 1:
                        obs_content = obs_item[0]
                        evidence_id = obs_item[1] if len(obs_item) > 1 else ""
                        
                        if not obs_content or not obs_content.strip():
                            continue
                        
                        # åˆ›å»ºè§‚å¯Ÿè®°å½•å•å…ƒ
                        unit = MemoryUnit(
                            uid=f"{sample_id}_{obs_session_key}_{speaker}_obs_{obs_idx}",
                            raw_data={
                                "text_content": obs_content,
                                "session_id": obs_session_key,
                                "speaker": speaker,
                                "evidence_id": evidence_id,
                                "observation_index": obs_idx,
                                "sample_id": sample_id,
                                "data_type": "observation",
                                "data_source": "raw"
                            },
                            metadata={
                                "conversation_id": sample_id,
                                "data_layer": "raw",
                                "content_type": "observation",
                                "session_id": obs_session_key,
                                "speaker": speaker,
                                "created": datetime.now().isoformat()
                            }
                        )
                        
                        # æ·»åŠ åˆ°å¯¹è¯ä¸“ç”¨ç©ºé—´
                        self.semantic_graph.add_unit(unit, space_names=[
                            f"conversation_{sample_id}",
                            f"raw_data_{sample_id}",
                            "all_observations"
                        ])
                        
                        stored_count += 1
        
        return stored_count
    
    def _store_event_data(self, sample: Dict, sample_id: str) -> int:
        """å­˜å‚¨äº‹ä»¶è®°å½•"""
        event_summary = sample.get('event_summary', {})
        if not event_summary:
            return 0
        
        stored_count = 0
        
        for event_session_key, event_data in event_summary.items():
            if not isinstance(event_data, dict):
                continue
                
            event_date = event_data.get('date', '')
            
            for speaker, events in event_data.items():
                if speaker == 'date' or not isinstance(events, list):
                    continue
                    
                for event_idx, event_desc in enumerate(events):
                    if not isinstance(event_desc, str) or not event_desc.strip():
                        continue
                    
                    # åˆ›å»ºäº‹ä»¶è®°å½•å•å…ƒ
                    unit = MemoryUnit(
                        uid=f"{sample_id}_{event_session_key}_{speaker}_event_{event_idx}",
                        raw_data={
                            "text_content": event_desc,
                            "session_id": event_session_key,
                            "event_date": event_date,
                            "speaker": speaker,
                            "event_index": event_idx,
                            "sample_id": sample_id,
                            "data_type": "event",
                            "data_source": "raw"
                        },
                        metadata={
                            "conversation_id": sample_id,
                            "data_layer": "raw",
                            "content_type": "event",
                            "session_id": event_session_key,
                            "speaker": speaker,
                            "event_date": event_date,
                            "created": datetime.now().isoformat()
                        }
                    )
                    
                    # æ·»åŠ åˆ°å¯¹è¯ä¸“ç”¨ç©ºé—´
                    self.semantic_graph.add_unit(unit, space_names=[
                        f"conversation_{sample_id}",
                        f"raw_data_{sample_id}",
                        "all_events"
                    ])
                    
                    stored_count += 1
        
        return stored_count
    
    def _store_summary_data(self, sample: Dict, sample_id: str) -> int:
        """å­˜å‚¨ä¼šè¯æ‘˜è¦"""
        session_summary = sample.get('session_summary', {})
        if not session_summary:
            return 0
        
        stored_count = 0
        
        for summary_key, summary_content in session_summary.items():
            if not isinstance(summary_content, str) or not summary_content.strip():
                continue
            
            # åˆ›å»ºæ‘˜è¦è®°å½•å•å…ƒ
            unit = MemoryUnit(
                uid=f"{sample_id}_{summary_key}_summary",
                raw_data={
                    "text_content": summary_content,
                    "summary_key": summary_key,
                    "sample_id": sample_id,
                    "data_type": "summary",
                    "data_source": "raw"
                },
                metadata={
                    "conversation_id": sample_id,
                    "data_layer": "raw", 
                    "content_type": "summary",
                    "summary_type": summary_key,
                    "created": datetime.now().isoformat()
                }
            )
            
            # æ·»åŠ åˆ°å¯¹è¯ä¸“ç”¨ç©ºé—´
            self.semantic_graph.add_unit(unit, space_names=[
                f"conversation_{sample_id}",
                f"raw_data_{sample_id}",
                "all_summaries"
            ])
            
            stored_count += 1
        
        return stored_count
    
    # ä¿®æ”¹ _store_qa_data æ–¹æ³•ä¸ºç©ºå®ç°
    def _store_qa_data(self, sample: Dict, sample_id: str) -> int:
        """
        å­˜å‚¨é—®ç­”æ•°æ® - å·²ç¦ç”¨
        QAæ•°æ®å°†ä½œä¸ºæµ‹è¯•é›†ä½¿ç”¨ï¼Œä¸æ’å…¥åˆ°è¯­ä¹‰å›¾è°±ä¸­
        """
        self.logger.info(f"è·³è¿‡QAæ•°æ®å­˜å‚¨ - QAæ•°æ®å°†ç”¨ä½œæµ‹è¯•é›† (sample_id: {sample_id})")
        return 0
    
    # æ·»åŠ ä¸€ä¸ªæ–°çš„æ–¹æ³•æ¥è·å–QAæ•°æ®ä½œä¸ºæµ‹è¯•é›†
    def get_qa_test_data(self, 
                        sample_ids: Optional[List[str]] = None,
                        raw_dataset_file: str = "benchmark/dataset/locomo/locomo10.json") -> Dict[str, List[Dict]]:
        """
        è·å–QAæ•°æ®ä½œä¸ºæµ‹è¯•é›†
        
        Args:
            sample_ids: æŒ‡å®šçš„å¯¹è¯IDåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è·å–æ‰€æœ‰å¯¹è¯çš„QA
            raw_dataset_file: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            QAæµ‹è¯•æ•°æ® {sample_id: [qa_items]}
        """
        self.logger.info("ğŸ“‹ æå–QAæ•°æ®ä½œä¸ºæµ‹è¯•é›†...")
        
        qa_test_data = {}
        
        try:
            with open(raw_dataset_file, 'r', encoding='utf-8') as f:
                raw_dataset = json.load(f)
            
            for sample in raw_dataset:
                sample_id = sample.get('sample_id')
                if not sample_id:
                    continue
                    
                # å¦‚æœæŒ‡å®šäº†sample_idsï¼Œåªå¤„ç†æŒ‡å®šçš„å¯¹è¯
                if sample_ids and sample_id not in sample_ids:
                    continue
                
                qa_data = sample.get('qa', [])
                if qa_data:
                    qa_test_data[sample_id] = qa_data
                    self.logger.info(f"âœ… {sample_id}: æå–äº† {len(qa_data)} ä¸ªQAå¯¹")
            
            total_qa = sum(len(qa_list) for qa_list in qa_test_data.values())
            self.logger.info(f"ğŸ“Š æ€»è®¡æå– {total_qa} ä¸ªQAå¯¹ä½œä¸ºæµ‹è¯•é›†")
            
            return qa_test_data
            
        except Exception as e:
            self.logger.error(f"âŒ æå–QAæµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            return {}
    
    # def _store_qa_data(self, sample: Dict, sample_id: str) -> int:
    #     """å­˜å‚¨é—®ç­”æ•°æ®"""
    #     qa_data = sample.get('qa', [])
    #     if not qa_data:
    #         return 0
        
    #     stored_count = 0
        
    #     # å°†QAæ•°æ®æŒ‰ç±»å‹åˆ†ç»„
    #     qa_by_category = {}
    #     qa_adversarial = []
        
    #     for qa_idx, qa_item in enumerate(qa_data):
    #         if isinstance(qa_item, dict):
    #             if 'category' in qa_item:
    #                 category = qa_item['category']
    #                 if category not in qa_by_category:
    #                     qa_by_category[category] = []
    #                 qa_by_category[category].append((qa_idx, qa_item))
    #             elif 'adversarial_answer' in qa_item:
    #                 qa_adversarial.append((qa_idx, qa_item))
        
    #     # å­˜å‚¨åˆ†ç±»é—®ç­”
    #     for category, qa_items in qa_by_category.items():
    #         if qa_items:
    #             category_text = f"Category {category} Questions for {sample_id}:\n"
    #             category_text += f"Total questions in this category: {len(qa_items)}\n"
    #             category_text += "Question indices: " + ", ".join([str(idx) for idx, _ in qa_items])
                
    #             unit = MemoryUnit(
    #                 uid=f"{sample_id}_qa_category_{category}",
    #                 raw_data={
    #                     "text_content": category_text,
    #                     "category": category,
    #                     "question_count": len(qa_items),
    #                     "question_indices": [idx for idx, _ in qa_items],
    #                     "sample_id": sample_id,
    #                     "data_type": "qa_category",
    #                     "data_source": "raw"
    #                 },
    #                 metadata={
    #                     "conversation_id": sample_id,
    #                     "data_layer": "raw",
    #                     "content_type": "qa_category",
    #                     "qa_category": category,
    #                     "created": datetime.now().isoformat()
    #                 }
    #             )
                
    #             self.semantic_graph.add_unit(unit, space_names=[
    #                 f"conversation_{sample_id}",
    #                 f"raw_data_{sample_id}",
    #                 "all_qa"
    #             ])
                
    #             stored_count += 1
        
    #     return stored_count
    
    def _store_extracted_entities(self, sample_data: Dict, sample_id: str) -> int:
        """å­˜å‚¨æŠ½å–çš„å®ä½“"""
        entities = sample_data.get('entities', [])
        if not entities:
            return 0
        
        stored_count = 0
        
        for entity_idx, entity in enumerate(entities):
            entity_name = entity.get('name', f'entity_{entity_idx}')
            entity_type = entity.get('type', 'unknown')
            description = entity.get('description', '')
            confidence = entity.get('confidence', 0.0)
            source_text = entity.get('source_text', '')
            
            # åˆ›å»ºå®ä½“çš„æ–‡æœ¬è¡¨ç¤º
            entity_text = f"Entity: {entity_name}\nType: {entity_type}\nDescription: {description}"
            if source_text:
                entity_text += f"\nSource: {source_text[:200]}..."
            
            # åˆ›å»ºå®ä½“è®°å¿†å•å…ƒ
            unit = MemoryUnit(
                uid=f"{sample_id}_entity_{entity_idx}_{self._safe_name(entity_name)}",
                raw_data={
                    "text_content": entity_text,
                    "entity_name": entity_name,
                    "entity_type": entity_type,
                    "description": description,
                    "confidence": confidence,
                    "source_text": source_text,
                    "sample_id": sample_id,
                    "data_type": "extracted_entity",
                    "data_source": "extracted"
                },
                metadata={
                    "conversation_id": sample_id,
                    "data_layer": "knowledge",
                    "content_type": "entity",
                    "entity_type": entity_type,
                    "confidence": confidence,
                    "created": datetime.now().isoformat()
                }
            )
            
            # æ·»åŠ åˆ°å¯¹è¯ä¸“ç”¨ç©ºé—´
            self.semantic_graph.add_unit(unit, space_names=[
                f"conversation_{sample_id}",
                f"extracted_data_{sample_id}",
                "all_entities"
            ])
            
            stored_count += 1
        
        return stored_count
    
    def _store_extracted_relationships(self, sample_data: Dict, sample_id: str) -> int:
        """å­˜å‚¨æŠ½å–çš„å…³ç³»"""
        relationships = sample_data.get('relationships', [])
        if not relationships:
            return 0
        
        stored_count = 0
        
        for rel_idx, relationship in enumerate(relationships):
            source = relationship.get('source', '')
            target = relationship.get('target', '')
            rel_type = relationship.get('type', 'RELATED_TO')
            description = relationship.get('description', '')
            strength = relationship.get('strength', 0.0)
            source_text = relationship.get('source_text', '')
            keywords = relationship.get('keywords', [])
            
            # åˆ›å»ºå…³ç³»çš„æ–‡æœ¬è¡¨ç¤º
            rel_text = f"Relationship: {source} --[{rel_type}]--> {target}\nDescription: {description}"
            if source_text:
                rel_text += f"\nSource: {source_text[:200]}..."
            if keywords:
                rel_text += f"\nKeywords: {', '.join(keywords)}"
            
            # åˆ›å»ºå…³ç³»è®°å¿†å•å…ƒ
            unit = MemoryUnit(
                uid=f"{sample_id}_rel_{rel_idx}_{self._safe_name(source)}_{self._safe_name(target)}",
                raw_data={
                    "text_content": rel_text,
                    "source_entity": source,
                    "target_entity": target,
                    "relationship_type": rel_type,
                    "description": description,
                    "strength": strength,
                    "keywords": keywords,
                    "source_text": source_text,
                    "sample_id": sample_id,
                    "data_type": "extracted_relationship",
                    "data_source": "extracted"
                },
                metadata={
                    "conversation_id": sample_id,
                    "data_layer": "knowledge",
                    "content_type": "relationship",
                    "relationship_type": rel_type,
                    "strength": strength,
                    "created": datetime.now().isoformat()
                }
            )
            
            # æ·»åŠ åˆ°å¯¹è¯ä¸“ç”¨ç©ºé—´
            self.semantic_graph.add_unit(unit, space_names=[
                f"conversation_{sample_id}",
                f"extracted_data_{sample_id}",
                "all_relationships"
            ])
            
            stored_count += 1
        
        return stored_count
    
    def _store_keywords(self, sample_data: Dict, sample_id: str) -> int:
        """å­˜å‚¨å…³é”®è¯"""
        keywords = sample_data.get('content_keywords', [])
        if not keywords:
            return 0
        
        keyword_text = f"Sample Keywords for {sample_id}: {', '.join(keywords)}"
        
        # åˆ›å»ºå…³é”®è¯è®°å¿†å•å…ƒ
        unit = MemoryUnit(
            uid=f"{sample_id}_keywords",
            raw_data={
                "text_content": keyword_text,
                "keywords": keywords,
                "keyword_count": len(keywords),
                "sample_id": sample_id,
                "data_type": "keywords",
                "data_source": "extracted"
            },
            metadata={
                "conversation_id": sample_id,
                "data_layer": "semantic",
                "content_type": "keywords",
                "created": datetime.now().isoformat()
            }
        )
        
        # æ·»åŠ åˆ°å¯¹è¯ä¸“ç”¨ç©ºé—´
        self.semantic_graph.add_unit(unit, space_names=[
            f"conversation_{sample_id}",
            f"extracted_data_{sample_id}",
            "all_keywords"
        ])
        
        return 1
    
    def _store_statistics(self, sample_data: Dict, sample_id: str) -> int:
        """å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        extraction_stats = sample_data.get('extraction_statistics', {})
        entity_stats = sample_data.get('entity_statistics', {})
        graph_structure = sample_data.get('graph_structure', {})
        
        if not any([extraction_stats, entity_stats, graph_structure]):
            return 0
        
        stats_text = f"Statistics for {sample_id}:\n"
        stats_text += f"- Entities: {extraction_stats.get('total_entities', 0)}\n"
        stats_text += f"- Relationships: {extraction_stats.get('total_relationships', 0)}\n"
        stats_text += f"- Keywords: {extraction_stats.get('total_keywords', 0)}\n"
        stats_text += f"- Graph Nodes: {graph_structure.get('networkx_nodes', 0)}\n"
        stats_text += f"- Graph Edges: {graph_structure.get('networkx_edges', 0)}\n"
        
        # åˆ›å»ºç»Ÿè®¡ä¿¡æ¯è®°å¿†å•å…ƒ
        unit = MemoryUnit(
            uid=f"{sample_id}_statistics",
            raw_data={
                "text_content": stats_text,
                "extraction_statistics": extraction_stats,
                "entity_statistics": entity_stats,
                "graph_structure": graph_structure,
                "sample_id": sample_id,
                "data_type": "statistics",
                "data_source": "extracted"
            },
            metadata={
                "conversation_id": sample_id,
                "data_layer": "semantic",
                "content_type": "statistics",
                "created": datetime.now().isoformat()
            }
        )
        
        # æ·»åŠ åˆ°å¯¹è¯ä¸“ç”¨ç©ºé—´
        self.semantic_graph.add_unit(unit, space_names=[
            f"conversation_{sample_id}",
            f"extracted_data_{sample_id}",
            "all_statistics"
        ])
        
        return 1
    
    def _establish_entity_relationships(self, sample_data: Dict, sample_id: str):
        """åœ¨è¯­ä¹‰å›¾ä¸­å»ºç«‹å®ä½“é—´çš„æ˜¾å¼å…³ç³»"""
        entities = sample_data.get('entities', [])
        relationships = sample_data.get('relationships', [])
        
        if not entities or not relationships:
            return
        
        # åˆ›å»ºå®ä½“åç§°åˆ°UIDçš„æ˜ å°„
        entity_name_to_uid = {}
        for entity_idx, entity in enumerate(entities):
            entity_name = entity.get('name', f'entity_{entity_idx}')
            entity_uid = f"{sample_id}_entity_{entity_idx}_{self._safe_name(entity_name)}"
            entity_name_to_uid[entity_name.lower()] = entity_uid
        
        # å»ºç«‹å…³ç³»
        for rel_idx, relationship in enumerate(relationships):
            source_name = relationship.get('source', '').lower()
            target_name = relationship.get('target', '').lower()
            rel_type = relationship.get('type', 'RELATED_TO')
            description = relationship.get('description', '')
            strength = relationship.get('strength', 0.0)
            keywords = relationship.get('keywords', [])
            
            # æŸ¥æ‰¾å¯¹åº”çš„å®ä½“UID
            source_uid = entity_name_to_uid.get(source_name)
            target_uid = entity_name_to_uid.get(target_name)
            
            if source_uid and target_uid:
                # åœ¨è¯­ä¹‰å›¾ä¸­æ·»åŠ å…³ç³»
                self.semantic_graph.add_relationship(
                    source_uid=source_uid,
                    target_uid=target_uid,
                    relationship_name=rel_type,
                    description=description,
                    strength=strength,
                    keywords=keywords,
                    conversation_id=sample_id,
                    created=datetime.now().isoformat()
                )
                self.logger.debug(f"å·²å»ºç«‹å…³ç³»: {source_name} --[{rel_type}]--> {target_name}")
    
    def _get_namespace_usage_stats(self) -> Dict[str, int]:
        """è·å–å‘½åç©ºé—´ä½¿ç”¨ç»Ÿè®¡"""
        stats = {}
        
        for space_name, space in self.semantic_graph.semantic_map.memory_spaces.items():
            stats[space_name] = len(space.get_memory_uids())
        
        return stats
    
    def _safe_name(self, name: str) -> str:
        """åˆ›å»ºå®‰å…¨çš„åç§°"""
        if not name:
            return "unknown"
        safe = "".join(c for c in name if c.isalnum() or c in "._-")
        return safe[:50]
    
    def get_conversation_list(self,
                             raw_dataset_file: str = "benchmark/dataset/locomo/locomo10.json",
                             extracted_dataset_file: str = "benchmark/dataset/locomo/extraction/locomo_extracted_full_dataset.json") -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„å¯¹è¯IDåˆ—è¡¨"""
        all_sample_ids = set()
        
        # ä»åŸå§‹æ•°æ®è·å–
        try:
            with open(raw_dataset_file, 'r', encoding='utf-8') as f:
                raw_dataset = json.load(f)
            for sample in raw_dataset:
                sample_id = sample.get('sample_id')
                if sample_id:
                    all_sample_ids.add(sample_id)
        except Exception as e:
            self.logger.warning(f"æ— æ³•åŠ è½½åŸå§‹æ•°æ®: {e}")
        
        # ä»æŠ½å–æ•°æ®è·å–
        try:
            with open(extracted_dataset_file, 'r', encoding='utf-8') as f:
                extracted_dataset = json.load(f)
            extracted_samples = extracted_dataset.get("samples", {})
            all_sample_ids.update(extracted_samples.keys())
        except Exception as e:
            self.logger.warning(f"æ— æ³•åŠ è½½æŠ½å–æ•°æ®: {e}")
        
        return sorted(list(all_sample_ids))


class ConversationSemanticQuerier:
    """
    å¯¹è¯è¯­ä¹‰æŸ¥è¯¢å™¨ - æ”¯æŒæŒ‰å¯¹è¯è¿›è¡ŒRAGæ£€ç´¢
    """
    
    def __init__(self, semantic_graph: SemanticGraph):
        """åˆå§‹åŒ–æŸ¥è¯¢å™¨"""
        self.semantic_graph = semantic_graph
        self.logger = logging.getLogger(__name__)
    
    def query_conversation(self, 
                          query_text: str,
                          conversation_id: str,
                          data_sources: List[str] = None,
                          data_types: List[str] = None,
                          k: int = 5) -> Dict[str, Any]:
        """
        æŸ¥è¯¢æŒ‡å®šå¯¹è¯
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            conversation_id: å¯¹è¯ID (å¦‚ "conv-26")
            data_sources: æ•°æ®æºè¿‡æ»¤ ("raw", "extracted")
            data_types: æ•°æ®ç±»å‹è¿‡æ»¤
            k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        results = {
            "query": query_text,
            "conversation_id": conversation_id,
            "results": []
        }
        
        try:
            # åœ¨å¯¹è¯ä¸“ç”¨ç©ºé—´ä¸­æœç´¢
            space_name = f"conversation_{conversation_id}"
            
            namespace_results = self.semantic_graph.search_similarity_in_graph(
                query_text=query_text,
                k=k * 2,
                space_name=space_name
            )
            
            # æŒ‰æ•°æ®æºè¿‡æ»¤
            if data_sources:
                filtered_results = []
                for unit, score in namespace_results:
                    unit_data_source = unit.raw_data.get('data_source', '')
                    if unit_data_source in data_sources:
                        filtered_results.append((unit, score))
                namespace_results = filtered_results
            
            # æŒ‰æ•°æ®ç±»å‹è¿‡æ»¤
            if data_types:
                filtered_results = []
                for unit, score in namespace_results:
                    unit_data_type = unit.raw_data.get('data_type', '')
                    if unit_data_type in data_types:
                        filtered_results.append((unit, score))
                namespace_results = filtered_results
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = [
                {
                    "unit_id": unit.uid,
                    "content": unit.raw_data.get("text_content", "")[:500],
                    "data_type": unit.raw_data.get("data_type", "unknown"),
                    "data_source": unit.raw_data.get("data_source", "unknown"),
                    "similarity_score": float(score),
                    "metadata": unit.metadata
                }
                for unit, score in namespace_results[:k]
            ]
            
            results["results"] = formatted_results
            
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢å¯¹è¯ {conversation_id} å¤±è´¥: {e}")
        
        return results
    
    def query_all_conversations(self, 
                               query_text: str,
                               data_sources: List[str] = None,
                               data_types: List[str] = None,
                               k: int = 5) -> Dict[str, Any]:
        """
        æŸ¥è¯¢æ‰€æœ‰å¯¹è¯
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            data_sources: æ•°æ®æºè¿‡æ»¤
            data_types: æ•°æ®ç±»å‹è¿‡æ»¤
            k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        results = {
            "query": query_text,
            "conversation_results": {}
        }
        
        # è·å–æ‰€æœ‰å¯¹è¯ç©ºé—´
        conversation_spaces = [
            space_name for space_name in self.semantic_graph.semantic_map.memory_spaces.keys()
            if space_name.startswith("conversation_")
        ]
        
        for space_name in conversation_spaces:
            conversation_id = space_name.replace("conversation_", "")
            conv_results = self.query_conversation(
                query_text=query_text,
                conversation_id=conversation_id,
                data_sources=data_sources,
                data_types=data_types,
                k=k
            )
            
            if conv_results["results"]:
                results["conversation_results"][conversation_id] = conv_results
        
        return results


# def main():
#     """ä¸»å‡½æ•° - æ¼”ç¤ºå¯¹è¯å­˜å‚¨å’ŒæŸ¥è¯¢"""
    
#     # 1. åˆ›å»ºå¯¹è¯å­˜å‚¨å™¨
#     storage = ConversationSemanticStorage()
    
#     # 2. è·å–å¯ç”¨å¯¹è¯åˆ—è¡¨
#     conversations = storage.get_conversation_list()
#     print(f"ğŸ” å‘ç° {len(conversations)} ä¸ªå¯¹è¯: {conversations}")
    
#     # 3. å­˜å‚¨æŒ‡å®šå¯¹è¯
#     test_conversation = "conv-26"
#     print(f"\nğŸš€ å­˜å‚¨å¯¹è¯ {test_conversation}")
    
#     stats = storage.store_conversation(
#         sample_id=test_conversation,
#         include_raw=True,
#         include_extracted=True
#     )
    
#     print(f"âœ… {test_conversation} å­˜å‚¨å®Œæˆï¼")
#     print(f"å­˜å‚¨ç»Ÿè®¡: {stats['storage_breakdown']}")
#     print(f"å‘½åç©ºé—´ä½¿ç”¨: {stats['namespace_usage']}")
    
#     # 4. åˆ›å»ºæŸ¥è¯¢å™¨å¹¶æµ‹è¯•
#     querier = ConversationSemanticQuerier(storage.semantic_graph)
    
#     print(f"\nğŸ” æŸ¥è¯¢å¯¹è¯ {test_conversation}")
    
#     # æŸ¥è¯¢åŸå§‹æ•°æ®
#     raw_results = querier.query_conversation(
#         query_text="Caroline LGBTQ support",
#         conversation_id=test_conversation,
#         data_sources=["raw"],
#         k=3
#     )
    
#     print(f"åŸå§‹æ•°æ®æŸ¥è¯¢ç»“æœ: {len(raw_results['results'])} æ¡")
#     for result in raw_results['results'][:2]:
#         print(f"- [{result['similarity_score']:.3f}] {result['data_type']}: {result['content'][:80]}...")
    
#     # æŸ¥è¯¢æŠ½å–ç»“æœ
#     extracted_results = querier.query_conversation(
#         query_text="Caroline",
#         conversation_id=test_conversation,
#         data_sources=["extracted"],
#         data_types=["extracted_entity"],
#         k=3
#     )
    
#     print(f"\næŠ½å–ç»“æœæŸ¥è¯¢: {len(extracted_results['results'])} æ¡")
#     for result in extracted_results['results'][:2]:
#         print(f"- [{result['similarity_score']:.3f}] {result['data_type']}: {result['content'][:80]}...")
    
#     print(f"\nğŸ¯ æ¼”ç¤ºç‰¹å®šå¯¹è¯å­˜å‚¨å’ŒæŸ¥è¯¢å®Œæˆï¼")
    
#     # 5. å¯é€‰ï¼šå­˜å‚¨æ‰€æœ‰å¯¹è¯
#     choice = input("\næ˜¯å¦å­˜å‚¨æ‰€æœ‰å¯¹è¯ï¼Ÿ(y/n): ")
#     if choice.lower() == 'y':
#         print("\nğŸš€ å¼€å§‹å­˜å‚¨æ‰€æœ‰å¯¹è¯")
#         all_stats = storage.store_all_conversations()
#         print(f"âœ… æ‰€æœ‰å¯¹è¯å­˜å‚¨å®Œæˆï¼å¤„ç†äº† {all_stats['processed_conversations']}/{all_stats['total_conversations']} ä¸ªå¯¹è¯")
#         print(f"æ€»ä½“å­˜å‚¨ç»Ÿè®¡: {all_stats['overall_storage_breakdown']}")

# æ›´æ–°mainå‡½æ•°
def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå¯¹è¯å­˜å‚¨å’ŒQAæµ‹è¯•é›†æå–"""
    
    # 1. åˆ›å»ºå¯¹è¯å­˜å‚¨å™¨
    storage = ConversationSemanticStorage()
    
    # 2. è·å–å¯ç”¨å¯¹è¯åˆ—è¡¨
    conversations = storage.get_conversation_list()
    print(f"ğŸ” å‘ç° {len(conversations)} ä¸ªå¯¹è¯: {conversations}")
    
    # 3. å­˜å‚¨æŒ‡å®šå¯¹è¯ï¼ˆä¸åŒ…æ‹¬QAï¼‰
    test_conversation = "conv-26"
    print(f"\nğŸš€ å­˜å‚¨å¯¹è¯ {test_conversation} (ä¸åŒ…æ‹¬QA)")
    
    stats = storage.store_conversation(
        sample_id=test_conversation,
        include_raw=True,
        include_extracted=True
    )
    
    print(f"âœ… {test_conversation} å­˜å‚¨å®Œæˆï¼")
    print(f"å­˜å‚¨ç»Ÿè®¡: {stats['storage_breakdown']}")
    print(f"QAä¿¡æ¯: {stats['qa_info']}")
    print(f"å‘½åç©ºé—´ä½¿ç”¨: {stats['namespace_usage']}")
    
    # 4. æå–QAæ•°æ®ä½œä¸ºæµ‹è¯•é›†
    print(f"\nğŸ“‹ æå–QAæ•°æ®ä½œä¸ºæµ‹è¯•é›†...")
    qa_test_data = storage.get_qa_test_data([test_conversation])
    
    if test_conversation in qa_test_data:
        qa_count = len(qa_test_data[test_conversation])
        print(f"ğŸ“Š {test_conversation} çš„QAæµ‹è¯•é›†åŒ…å« {qa_count} ä¸ªé—®ç­”å¯¹")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªQAæ ·ä¾‹
        for i, qa in enumerate(qa_test_data[test_conversation][:3]):
            category = qa.get('category', 'unknown')
            question = qa.get('question', 'N/A')[:50] + '...' if len(qa.get('question', '')) > 50 else qa.get('question', 'N/A')
            print(f"  QA[{i}] (cat-{category}): {question}")
    
    # 5. åˆ›å»ºæŸ¥è¯¢å™¨å¹¶æµ‹è¯•æ£€ç´¢
    print(f"\nğŸ” æµ‹è¯•è¯­ä¹‰æ£€ç´¢åŠŸèƒ½...")
    querier = ConversationSemanticQuerier(storage.semantic_graph)
    
    # åœ¨åŸå§‹æ•°æ®ä¸­æœç´¢
    search_results = querier.query_conversation(
        query_text="Caroline LGBTQ support",
        conversation_id=test_conversation,
        data_sources=["raw"],
        k=3
    )
    
    print(f"åŸå§‹æ•°æ®æ£€ç´¢ç»“æœ: {len(search_results['results'])} æ¡")
    for i, result in enumerate(search_results['results'][:2]):
        content_preview = result['content'][:60] + '...' if len(result['content']) > 60 else result['content']
        print(f"  [{i}] [{result['similarity_score']:.3f}] {result['data_type']}: {content_preview}")
    
    print(f"\nğŸ¯ æ¼”ç¤ºå®Œæˆï¼")
    print(f"âœ… å¯¹è¯æ•°æ®å·²å­˜å‚¨åˆ°è¯­ä¹‰å›¾è°±ï¼ˆä¸åŒ…æ‹¬QAï¼‰")
    print(f"âœ… QAæ•°æ®å·²å‡†å¤‡å¥½ä½œä¸ºæµ‹è¯•é›†ä½¿ç”¨")
    
    # 6. å¯é€‰ï¼šå­˜å‚¨æ‰€æœ‰å¯¹è¯
    choice = input("\næ˜¯å¦å­˜å‚¨æ‰€æœ‰å¯¹è¯ï¼Ÿ(y/n): ")
    if choice.lower() == 'y':
        print("\nğŸš€ å¼€å§‹å­˜å‚¨æ‰€æœ‰å¯¹è¯ï¼ˆä¸åŒ…æ‹¬QAï¼‰")
        all_stats = storage.store_all_conversations()
        print(f"âœ… æ‰€æœ‰å¯¹è¯å­˜å‚¨å®Œæˆï¼å¤„ç†äº† {all_stats['processed_conversations']}/{all_stats['total_conversations']} ä¸ªå¯¹è¯")
        print(f"ğŸ“Š æ€»ä½“å­˜å‚¨ç»Ÿè®¡: {all_stats['overall_storage_breakdown']}")
        print(f"ğŸ“‹ æ€»è®¡QAæµ‹è¯•é›†: {all_stats['overall_qa_info']['total_qa_pairs']} ä¸ªé—®ç­”å¯¹")

if __name__ == "__main__":
    main()

# # åªå­˜å‚¨åŸå§‹æ•°æ®
# storage.store_conversation("conv-26", include_raw=True, include_extracted=False)

# # åªå­˜å‚¨æŠ½å–ç»“æœ
# storage.store_conversation("conv-26", include_raw=False, include_extracted=True)

# # å­˜å‚¨ä¸¤ç§æ•°æ®
# storage.store_conversation("conv-26", include_raw=True, include_extracted=True)

# # æŸ¥è¯¢æŒ‡å®šå¯¹è¯
# querier.query_conversation(
#     query_text="Caroline LGBTQ",
#     conversation_id="conv-26",
#     data_sources=["raw", "extracted"],
#     k=5
# )

# # æŸ¥è¯¢æ‰€æœ‰å¯¹è¯
# querier.query_all_conversations(
#     query_text="LGBTQ support",
#     data_sources=["raw"],
#     k=5
# )

# 1. å­˜å‚¨ç‰¹å®šå¯¹è¯
storage = ConversationSemanticStorage()
storage.store_conversation("conv-26")  # åªå­˜å‚¨ conv-26

# 2. å­˜å‚¨å¤šä¸ªå¯¹è¯
for conv_id in ["conv-26", "conv-30", "conv-41"]:
    storage.store_conversation(conv_id)

# # 3. æŸ¥è¯¢ç‰¹å®šå¯¹è¯
# querier = ConversationSemanticQuerier(storage.semantic_graph)
# results = querier.query_conversation("Caroline", "conv-26")