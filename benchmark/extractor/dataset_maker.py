import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import argparse

class LoCoMoDatasetMaker:
    """
    LoCoMo æ•°æ®é›†åˆ¶ä½œå™¨
    å°†æ‰€æœ‰æ ·æœ¬çš„å®ä½“å…³ç³»æŠ½å–ç»“æœæ•´åˆæˆç»Ÿä¸€çš„æ•°æ®é›†æ–‡ä»¶
    """
    
    def __init__(self, results_dir: str, output_dir: str = None):
        """
        åˆå§‹åŒ–æ•°æ®é›†åˆ¶ä½œå™¨
        
        Args:
            results_dir: æŠ½å–ç»“æœç›®å½•è·¯å¾„ (å¦‚ benchmark/results/fixed_full_extraction)
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤ä¸º benchmark/dataset)
        """
        self.results_dir = Path(results_dir)
        self.output_dir = Path(output_dir) if output_dir else Path("benchmark/dataset")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def collect_extraction_results(self) -> Dict[str, Any]:
        """
        æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„æŠ½å–ç»“æœ
        
        Returns:
            åŒ…å«æ‰€æœ‰æ ·æœ¬æ•°æ®çš„å­—å…¸
        """
        dataset = {
            "metadata": {
                "created_at": datetime.now().isoformat(),
                "source_directory": str(self.results_dir),
                "total_samples": 0,
                "extraction_summary": {}
            },
            "samples": {}
        }
        
        # æŸ¥æ‰¾æ‰€æœ‰æ ·æœ¬çš„æŠ½å–ç»“æœç›®å½•
        sample_dirs = [d for d in self.results_dir.iterdir() 
                      if d.is_dir() and d.name.startswith('conv-')]
        
        self.logger.info(f"å‘ç° {len(sample_dirs)} ä¸ªæ ·æœ¬ç›®å½•")
        
        total_entities = 0
        total_relationships = 0
        total_keywords = 0
        
        for sample_dir in sorted(sample_dirs):
            try:
                sample_data = self._process_sample_directory(sample_dir)
                if sample_data:
                    sample_id = sample_data["sample_info"]["sample_id"]
                    dataset["samples"][sample_id] = sample_data
                    
                    # ç´¯è®¡ç»Ÿè®¡
                    stats = sample_data.get("extraction_statistics", {})
                    total_entities += stats.get("total_entities", 0)
                    total_relationships += stats.get("total_relationships", 0)
                    total_keywords += stats.get("total_keywords", 0)
                    
                    self.logger.info(f"âœ… æˆåŠŸå¤„ç†æ ·æœ¬: {sample_id}")
                else:
                    self.logger.warning(f"âš ï¸  è·³è¿‡æ— æ•ˆæ ·æœ¬ç›®å½•: {sample_dir.name}")
                    
            except Exception as e:
                self.logger.error(f"âŒ å¤„ç†æ ·æœ¬ç›®å½• {sample_dir.name} æ—¶å‡ºé”™: {e}")
        
        # æ›´æ–°å…ƒæ•°æ®
        dataset["metadata"]["total_samples"] = len(dataset["samples"])
        dataset["metadata"]["extraction_summary"] = {
            "total_entities": total_entities,
            "total_relationships": total_relationships,
            "total_keywords": total_keywords,
            "avg_entities_per_sample": total_entities / len(dataset["samples"]) if dataset["samples"] else 0,
            "avg_relationships_per_sample": total_relationships / len(dataset["samples"]) if dataset["samples"] else 0
        }
        
        return dataset
    
    def _process_sample_directory(self, sample_dir: Path) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†å•ä¸ªæ ·æœ¬ç›®å½•
        
        Args:
            sample_dir: æ ·æœ¬ç›®å½•è·¯å¾„
            
        Returns:
            æ ·æœ¬æ•°æ®å­—å…¸ï¼Œå¦‚æœå¤„ç†å¤±è´¥è¿”å›None
        """
        # æŸ¥æ‰¾æŠ½å–ç»“æœæ–‡ä»¶
        result_files = list(sample_dir.glob("*_extraction_results.json"))
        
        if not result_files:
            self.logger.warning(f"æœªæ‰¾åˆ°æŠ½å–ç»“æœæ–‡ä»¶: {sample_dir}")
            return None
        
        # ä½¿ç”¨æœ€æ–°çš„ç»“æœæ–‡ä»¶
        result_file = sorted(result_files)[-1]
        
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                sample_data = json.load(f)
            
            # æ·»åŠ æ–‡ä»¶è·¯å¾„ä¿¡æ¯
            sample_data["source_files"] = {
                "extraction_results": str(result_file.relative_to(self.results_dir)),
                "directory": str(sample_dir.relative_to(self.results_dir))
            }
            
            # æŸ¥æ‰¾å…¶ä»–ç›¸å…³æ–‡ä»¶
            additional_files = {}
            for pattern in ["*_full_conversation.txt", "*_samples.json", "*_analysis.json", 
                           "*_summary_report.txt", "*semantic_graph*"]:
                files = list(sample_dir.glob(pattern))
                if files:
                    additional_files[pattern.replace("*", "").replace(".", "")] = [
                        str(f.relative_to(self.results_dir)) for f in files
                    ]
            
            if additional_files:
                sample_data["source_files"].update(additional_files)
            
            return sample_data
            
        except Exception as e:
            self.logger.error(f"è¯»å–ç»“æœæ–‡ä»¶ {result_file} æ—¶å‡ºé”™: {e}")
            return None
    
    def create_entity_relationship_summary(self, dataset: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ›å»ºå®ä½“å…³ç³»æ±‡æ€»
        
        Args:
            dataset: å®Œæ•´æ•°æ®é›†
            
        Returns:
            å®ä½“å…³ç³»æ±‡æ€»æ•°æ®
        """
        summary = {
            "metadata": {
                "created_at": datetime.now().isoformat(),
                "total_samples": len(dataset["samples"])
            },
            "entity_summary": {
                "all_entities": {},
                "entity_types": {},
                "entity_frequency": {}
            },
            "relationship_summary": {
                "all_relationships": {},
                "relationship_types": {},
                "relationship_patterns": {}
            },
            "cross_sample_analysis": {
                "common_entities": {},
                "common_relationships": {},
                "entity_co_occurrence": {}
            }
        }
        
        all_entities = {}
        all_relationships = {}
        entity_types_count = {}
        relationship_types_count = {}
        
        # éå†æ‰€æœ‰æ ·æœ¬
        for sample_id, sample_data in dataset["samples"].items():
            entities = sample_data.get("entities", [])
            relationships = sample_data.get("relationships", [])
            
            # å¤„ç†å®ä½“
            for entity in entities:
                entity_name = entity.get("name", "").lower()
                entity_type = entity.get("type", "unknown")
                
                if entity_name:
                    # å®ä½“é¢‘ç‡ç»Ÿè®¡
                    if entity_name not in all_entities:
                        all_entities[entity_name] = {
                            "name": entity.get("name", ""),
                            "type": entity_type,
                            "descriptions": [],
                            "samples": [],
                            "frequency": 0
                        }
                    
                    all_entities[entity_name]["descriptions"].append({
                        "sample_id": sample_id,
                        "description": entity.get("description", ""),
                        "confidence": entity.get("confidence", 0.0)
                    })
                    all_entities[entity_name]["samples"].append(sample_id)
                    all_entities[entity_name]["frequency"] += 1
                
                # å®ä½“ç±»å‹ç»Ÿè®¡
                entity_types_count[entity_type] = entity_types_count.get(entity_type, 0) + 1
            
            # å¤„ç†å…³ç³»
            for relationship in relationships:
                source = relationship.get("source", "").lower()
                target = relationship.get("target", "").lower()
                rel_type = relationship.get("type", "unknown")
                
                rel_key = f"{source}-{rel_type}-{target}"
                
                if rel_key not in all_relationships:
                    all_relationships[rel_key] = {
                        "source": relationship.get("source", ""),
                        "target": relationship.get("target", ""),
                        "type": rel_type,
                        "descriptions": [],
                        "samples": [],
                        "frequency": 0,
                        "avg_strength": 0.0
                    }
                
                all_relationships[rel_key]["descriptions"].append({
                    "sample_id": sample_id,
                    "description": relationship.get("description", ""),
                    "strength": relationship.get("strength", 0.0)
                })
                all_relationships[rel_key]["samples"].append(sample_id)
                all_relationships[rel_key]["frequency"] += 1
                
                # å…³ç³»ç±»å‹ç»Ÿè®¡
                relationship_types_count[rel_type] = relationship_types_count.get(rel_type, 0) + 1
        
        # è®¡ç®—å¹³å‡å¼ºåº¦
        for rel_data in all_relationships.values():
            strengths = [desc["strength"] for desc in rel_data["descriptions"] if desc["strength"]]
            rel_data["avg_strength"] = sum(strengths) / len(strengths) if strengths else 0.0
        
        # å¡«å……æ±‡æ€»æ•°æ®
        summary["entity_summary"]["all_entities"] = dict(sorted(
            all_entities.items(), key=lambda x: x[1]["frequency"], reverse=True
        ))
        summary["entity_summary"]["entity_types"] = dict(sorted(
            entity_types_count.items(), key=lambda x: x[1], reverse=True
        ))
        summary["entity_summary"]["entity_frequency"] = {
            "total_unique_entities": len(all_entities),
            "most_frequent": max(all_entities.values(), key=lambda x: x["frequency"]) if all_entities else None,
            "cross_sample_entities": {k: v for k, v in all_entities.items() if v["frequency"] > 1}
        }
        
        summary["relationship_summary"]["all_relationships"] = dict(sorted(
            all_relationships.items(), key=lambda x: x[1]["frequency"], reverse=True
        ))
        summary["relationship_summary"]["relationship_types"] = dict(sorted(
            relationship_types_count.items(), key=lambda x: x[1], reverse=True
        ))
        summary["relationship_summary"]["relationship_patterns"] = {
            "total_unique_relationships": len(all_relationships),
            "most_frequent": max(all_relationships.values(), key=lambda x: x["frequency"]) if all_relationships else None,
            "cross_sample_relationships": {k: v for k, v in all_relationships.items() if v["frequency"] > 1}
        }
        
        return summary
    
    def create_sample_index(self, dataset: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ›å»ºæ ·æœ¬ç´¢å¼•æ–‡ä»¶
        
        Args:
            dataset: å®Œæ•´æ•°æ®é›†
            
        Returns:
            æ ·æœ¬ç´¢å¼•æ•°æ®
        """
        index = {
            "metadata": {
                "created_at": datetime.now().isoformat(),
                "total_samples": len(dataset["samples"])
            },
            "sample_index": {}
        }
        
        for sample_id, sample_data in dataset["samples"].items():
            sample_info = sample_data.get("sample_info", {})
            extraction_stats = sample_data.get("extraction_statistics", {})
            
            index["sample_index"][sample_id] = {
                "speakers": sample_info.get("speakers", []),
                "session_count": sample_info.get("session_count", 0),
                "total_messages": sample_info.get("total_messages", 0),
                "entity_count": extraction_stats.get("total_entities", 0),
                "relationship_count": extraction_stats.get("total_relationships", 0),
                "keyword_count": extraction_stats.get("total_keywords", 0),
                "processing_time": extraction_stats.get("processing_time", ""),
                "source_directory": sample_data.get("source_files", {}).get("directory", "")
            }
        
        return index
    
    def generate_dataset_files(self, output_prefix: str = "locomo_extracted") -> Dict[str, Path]:
        """
        ç”Ÿæˆå®Œæ•´çš„æ•°æ®é›†æ–‡ä»¶
        
        Args:
            output_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€
            
        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        self.logger.info("ğŸš€ å¼€å§‹ç”Ÿæˆ LoCoMo æŠ½å–æ•°æ®é›†...")
        
        # 1. æ”¶é›†æ‰€æœ‰æŠ½å–ç»“æœ
        self.logger.info("ğŸ“Š æ”¶é›†æŠ½å–ç»“æœ...")
        dataset = self.collect_extraction_results()
        
        if not dataset["samples"]:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æŠ½å–ç»“æœ")
        
        # 2. ç”Ÿæˆå®Œæ•´æ•°æ®é›†æ–‡ä»¶
        full_dataset_path = self.output_dir / f"{output_prefix}_full_dataset.json"
        with open(full_dataset_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        self.logger.info(f"âœ… å®Œæ•´æ•°æ®é›†å·²ä¿å­˜: {full_dataset_path}")
        
        # 3. ç”Ÿæˆå®ä½“å…³ç³»æ±‡æ€»
        self.logger.info("ğŸ“ˆ ç”Ÿæˆå®ä½“å…³ç³»æ±‡æ€»...")
        summary = self.create_entity_relationship_summary(dataset)
        summary_path = self.output_dir / f"{output_prefix}_entity_relationship_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        self.logger.info(f"âœ… å®ä½“å…³ç³»æ±‡æ€»å·²ä¿å­˜: {summary_path}")
        
        # 4. ç”Ÿæˆæ ·æœ¬ç´¢å¼•
        self.logger.info("ğŸ“‹ ç”Ÿæˆæ ·æœ¬ç´¢å¼•...")
        index = self.create_sample_index(dataset)
        index_path = self.output_dir / f"{output_prefix}_sample_index.json"
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        self.logger.info(f"âœ… æ ·æœ¬ç´¢å¼•å·²ä¿å­˜: {index_path}")
        
        # 5. ç”Ÿæˆè½»é‡çº§ç‰ˆæœ¬ï¼ˆä»…åŒ…å«æ ¸å¿ƒæ•°æ®ï¼‰
        self.logger.info("ğŸ’¾ ç”Ÿæˆè½»é‡çº§æ•°æ®é›†...")
        lightweight_dataset = self._create_lightweight_dataset(dataset)
        lightweight_path = self.output_dir / f"{output_prefix}_lightweight.json"
        with open(lightweight_path, 'w', encoding='utf-8') as f:
            json.dump(lightweight_dataset, f, ensure_ascii=False, indent=2)
        self.logger.info(f"âœ… è½»é‡çº§æ•°æ®é›†å·²ä¿å­˜: {lightweight_path}")
        
        # 6. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.logger.info("ğŸ“Š ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
        report = self._generate_statistics_report(dataset, summary)
        report_path = self.output_dir / f"{output_prefix}_statistics_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        self.logger.info(f"âœ… ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        generated_files = {
            "full_dataset": full_dataset_path,
            "entity_relationship_summary": summary_path,
            "sample_index": index_path,
            "lightweight_dataset": lightweight_path,
            "statistics_report": report_path
        }
        
        self.logger.info(f"ğŸ‰ æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len(generated_files)} ä¸ªæ–‡ä»¶")
        return generated_files
    
    def _create_lightweight_dataset(self, dataset: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºè½»é‡çº§æ•°æ®é›†ï¼ˆç§»é™¤source_textç­‰å¤§å­—æ®µï¼‰"""
        lightweight = {
            "metadata": dataset["metadata"],
            "samples": {}
        }
        
        for sample_id, sample_data in dataset["samples"].items():
            lightweight_sample = {
                "sample_info": sample_data.get("sample_info", {}),
                "extraction_statistics": sample_data.get("extraction_statistics", {}),
                "entities": [],
                "relationships": [],
                "content_keywords": sample_data.get("content_keywords", []),
                "entity_statistics": sample_data.get("entity_statistics", {}),
                "graph_structure": sample_data.get("graph_structure", {})
            }
            
            # ç®€åŒ–å®ä½“ä¿¡æ¯
            for entity in sample_data.get("entities", []):
                lightweight_sample["entities"].append({
                    "name": entity.get("name", ""),
                    "type": entity.get("type", ""),
                    "description": entity.get("description", ""),
                    "confidence": entity.get("confidence", 0.0)
                })
            
            # ç®€åŒ–å…³ç³»ä¿¡æ¯
            for relationship in sample_data.get("relationships", []):
                lightweight_sample["relationships"].append({
                    "source": relationship.get("source", ""),
                    "target": relationship.get("target", ""),
                    "type": relationship.get("type", ""),
                    "description": relationship.get("description", ""),
                    "strength": relationship.get("strength", 0.0)
                })
            
            lightweight["samples"][sample_id] = lightweight_sample
        
        return lightweight
    
    def _generate_statistics_report(self, dataset: Dict[str, Any], summary: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        metadata = dataset["metadata"]
        extraction_summary = metadata["extraction_summary"]
        
        report = f"""
        # LoCoMo å®ä½“å…³ç³»æŠ½å–æ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š

        ## åŸºæœ¬ä¿¡æ¯
        - ç”Ÿæˆæ—¶é—´: {metadata['created_at']}
        - æºç›®å½•: {metadata['source_directory']}
        - æ ·æœ¬æ€»æ•°: {metadata['total_samples']}

        ## æŠ½å–ç»Ÿè®¡
        - å®ä½“æ€»æ•°: {extraction_summary['total_entities']}
        - å…³ç³»æ€»æ•°: {extraction_summary['total_relationships']}
        - å…³é”®è¯æ€»æ•°: {extraction_summary['total_keywords']}
        - å¹³å‡æ¯æ ·æœ¬å®ä½“æ•°: {extraction_summary['avg_entities_per_sample']:.2f}
        - å¹³å‡æ¯æ ·æœ¬å…³ç³»æ•°: {extraction_summary['avg_relationships_per_sample']:.2f}

        ## å®ä½“ç±»å‹åˆ†å¸ƒ
        """
        
        entity_types = summary["entity_summary"]["entity_types"]
        for entity_type, count in list(entity_types.items())[:10]:
            report += f"- {entity_type}: {count}\n"
        
        report += f"""
        ## å…³ç³»ç±»å‹åˆ†å¸ƒ
        """
        
        rel_types = summary["relationship_summary"]["relationship_types"]
        for rel_type, count in list(rel_types.items())[:10]:
            report += f"- {rel_type}: {count}\n"
        
        report += f"""
        ## è·¨æ ·æœ¬åˆ†æ
        - å”¯ä¸€å®ä½“æ•°: {summary['entity_summary']['entity_frequency']['total_unique_entities']}
        - è·¨æ ·æœ¬å®ä½“æ•°: {len(summary['entity_summary']['entity_frequency']['cross_sample_entities'])}
        - å”¯ä¸€å…³ç³»æ•°: {summary['relationship_summary']['relationship_patterns']['total_unique_relationships']}
        - è·¨æ ·æœ¬å…³ç³»æ•°: {len(summary['relationship_summary']['relationship_patterns']['cross_sample_relationships'])}

        ## æ ·æœ¬è¯¦æƒ…
        """
        
        for sample_id, sample_data in dataset["samples"].items():
            stats = sample_data.get("extraction_statistics", {})
            info = sample_data.get("sample_info", {})
            report += f"""
            ### {sample_id}
            - è¯´è¯è€…: {', '.join(info.get('speakers', []))}
            - ä¼šè¯æ•°: {info.get('session_count', 0)}
            - æ¶ˆæ¯æ•°: {info.get('total_messages', 0)}
            - å®ä½“æ•°: {stats.get('total_entities', 0)}
            - å…³ç³»æ•°: {stats.get('total_relationships', 0)}
            - å¤„ç†æ—¶é—´: {stats.get('processing_time', 'N/A')}
            """
        
        return report


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description="LoCoMo æ•°æ®é›†åˆ¶ä½œå™¨")
    parser.add_argument(
        "--results-dir", 
        required=True,
        help="æŠ½å–ç»“æœç›®å½•è·¯å¾„ (å¦‚ benchmark/results/fixed_full_extraction)"
    )
    parser.add_argument(
        "--output-dir", 
        default="benchmark/dataset",
        help="è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: benchmark/dataset)"
    )
    parser.add_argument(
        "--output-prefix", 
        default="locomo_extracted",
        help="è¾“å‡ºæ–‡ä»¶å‰ç¼€ (é»˜è®¤: locomo_extracted)"
    )
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºæ•°æ®é›†åˆ¶ä½œå™¨
        maker = LoCoMoDatasetMaker(
            results_dir=args.results_dir,
            output_dir=args.output_dir
        )
        
        # ç”Ÿæˆæ•°æ®é›†æ–‡ä»¶
        generated_files = maker.generate_dataset_files(
            output_prefix=args.output_prefix
        )
        
        print("\nğŸ‰ æ•°æ®é›†ç”ŸæˆæˆåŠŸï¼")
        print("\nç”Ÿæˆçš„æ–‡ä»¶:")
        for file_type, file_path in generated_files.items():
            print(f"  {file_type}: {file_path}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†ç”Ÿæˆå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()