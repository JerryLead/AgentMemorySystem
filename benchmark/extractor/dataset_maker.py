import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import argparse
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from dev.semantic_graph import SemanticGraph
from dev.memory_unit import MemoryUnit
from dev.memory_space import MemorySpace

class LoCoMoDatasetMaker:
    """
    LoCoMo æ•°æ®é›†åˆ¶ä½œå™¨ - é€‚é…æ–°çš„ semantic_map/graph æ¶æ„
    å°†æ‰€æœ‰æ ·æœ¬çš„å®ä½“å…³ç³»æŠ½å–ç»“æœæ•´åˆæˆç»Ÿä¸€çš„æ•°æ®é›†æ–‡ä»¶
    """
    
    def __init__(self, results_dir: str, output_dir: str = None):
        """
        åˆå§‹åŒ–æ•°æ®é›†åˆ¶ä½œå™¨
        
        Args:
            results_dir: æŠ½å–ç»“æœç›®å½•è·¯å¾„ (å¦‚ benchmark/extractor/results)
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤ä¸º benchmark/dataset/locomo/extraction)
        """
        self.results_dir = Path(results_dir)
        self.output_dir = Path(output_dir) if output_dir else Path("benchmark/dataset/locomo/extraction")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        self.logger.info(f"æ•°æ®é›†åˆ¶ä½œå™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ç»“æœç›®å½•: {self.results_dir}")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def collect_extraction_results(self) -> Dict[str, Any]:
        """
        æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„æŠ½å–ç»“æœ - é€‚é…æ–°æ¶æ„
        
        Returns:
            åŒ…å«æ‰€æœ‰æ ·æœ¬æ•°æ®çš„å­—å…¸
        """
        dataset = {
            "metadata": {
                "created_at": datetime.now().isoformat(),
                "source_directory": str(self.results_dir),
                "total_samples": 0,
                "extraction_summary": {},
                "semantic_graph_version": "new_architecture"
            },
            "samples": {}
        }
        
        # æŸ¥æ‰¾æ‰€æœ‰æ ·æœ¬çš„æŠ½å–ç»“æœç›®å½•
        sample_dirs = [d for d in self.results_dir.iterdir() 
                      if d.is_dir() and ('conv-' in d.name or 'entity_extraction' in d.name)]
        
        self.logger.info(f"å‘ç° {len(sample_dirs)} ä¸ªæ ·æœ¬ç›®å½•")
        
        total_entities = 0
        total_relationships = 0
        total_keywords = 0
        total_memory_units = 0
        total_memory_spaces = 0
        
        for sample_dir in sorted(sample_dirs):
            try:
                sample_data = self._process_sample_directory(sample_dir)
                if sample_data:
                    sample_id = sample_data["sample_info"]["sample_id"]
                    dataset["samples"][sample_id] = sample_data
                    
                    # ç´¯è®¡ç»Ÿè®¡ - é€‚é…æ–°æ¶æ„
                    stats = sample_data.get("extraction_statistics", {})
                    total_entities += stats.get("total_entities", 0)
                    total_relationships += stats.get("total_relationships", 0)
                    total_keywords += stats.get("total_keywords", 0)
                    
                    # æ–°æ¶æ„ç‰¹æœ‰çš„ç»Ÿè®¡
                    graph_structure = sample_data.get("graph_structure", {})
                    total_memory_units += graph_structure.get("total_memory_units", 0)
                    total_memory_spaces += graph_structure.get("total_memory_spaces", 0)
                    
                    self.logger.info(f"âœ… æˆåŠŸå¤„ç†æ ·æœ¬: {sample_id}")
                else:
                    self.logger.warning(f"âš ï¸  è·³è¿‡æ— æ•ˆæ ·æœ¬ç›®å½•: {sample_dir.name}")
                    
            except Exception as e:
                self.logger.error(f"âŒ å¤„ç†æ ·æœ¬ç›®å½• {sample_dir.name} æ—¶å‡ºé”™: {e}")
        
        # æ›´æ–°å…ƒæ•°æ® - åŒ…å«æ–°æ¶æ„ä¿¡æ¯
        dataset["metadata"]["total_samples"] = len(dataset["samples"])
        dataset["metadata"]["extraction_summary"] = {
            "total_entities": total_entities,
            "total_relationships": total_relationships,
            "total_keywords": total_keywords,
            "total_memory_units": total_memory_units,
            "total_memory_spaces": total_memory_spaces,
            "avg_entities_per_sample": total_entities / len(dataset["samples"]) if dataset["samples"] else 0,
            "avg_relationships_per_sample": total_relationships / len(dataset["samples"]) if dataset["samples"] else 0,
            "avg_memory_units_per_sample": total_memory_units / len(dataset["samples"]) if dataset["samples"] else 0
        }
        
        return dataset
    
    def _process_sample_directory(self, sample_dir: Path) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†å•ä¸ªæ ·æœ¬ç›®å½• - é€‚é…æ–°æ¶æ„
        
        Args:
            sample_dir: æ ·æœ¬ç›®å½•è·¯å¾„
            
        Returns:
            æ ·æœ¬æ•°æ®å­—å…¸ï¼Œå¦‚æœå¤„ç†å¤±è´¥è¿”å›None
        """
        # æŸ¥æ‰¾æŠ½å–ç»“æœæ–‡ä»¶
        result_files = list(sample_dir.glob("*_extraction_results.json"))
        
        if not result_files:
            self.logger.warning(f"æœªæ‰¾åˆ°æŠ½å–ç»“æœæ–‡ä»¶: {sample_dir}")
            return None
        
        # ä½¿ç”¨æœ€æ–°çš„ç»“æœæ–‡ä»¶
        result_file = sorted(result_files)[-1]
        
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                sample_data = json.load(f)
            
            # æ·»åŠ æ–‡ä»¶è·¯å¾„ä¿¡æ¯
            sample_data["source_files"] = {
                "extraction_results": str(result_file.relative_to(self.results_dir)),
                "directory": str(sample_dir.relative_to(self.results_dir))
            }
            
            # æŸ¥æ‰¾å…¶ä»–ç›¸å…³æ–‡ä»¶ - é€‚é…æ–°æ¶æ„
            additional_files = {}
            file_patterns = [
                "*_full_conversation.txt",
                "*_detailed_analysis.json", 
                "*_summary_report.txt",
                "*semantic_graph*",
                "*semantic_map_data*",
                "*management_state.pkl"
            ]
            
            for pattern in file_patterns:
                files = list(sample_dir.glob(pattern))
                if files:
                    pattern_key = pattern.replace("*", "").replace(".", "").replace("/", "_")
                    additional_files[pattern_key] = [
                        str(f.relative_to(self.results_dir)) for f in files
                    ]
            
            if additional_files:
                sample_data["source_files"].update(additional_files)
            
            # å°è¯•åŠ è½½è¯­ä¹‰å›¾è°±ä¿¡æ¯ - æ–°æ¶æ„
            semantic_graph_dirs = list(sample_dir.glob("*semantic_graph*"))
            if semantic_graph_dirs:
                graph_info = self._extract_semantic_graph_info(semantic_graph_dirs[0])
                if graph_info:
                    sample_data["semantic_graph_info"] = graph_info
            
            return sample_data
            
        except Exception as e:
            self.logger.error(f"è¯»å–ç»“æœæ–‡ä»¶ {result_file} æ—¶å‡ºé”™: {e}")
            return None
    
    def _extract_semantic_graph_info(self, graph_dir: Path) -> Optional[Dict[str, Any]]:
        """
        ä»è¯­ä¹‰å›¾è°±ç›®å½•ä¸­æå–ä¿¡æ¯ - æ–°æ¶æ„
        
        Args:
            graph_dir: è¯­ä¹‰å›¾è°±ç›®å½•
            
        Returns:
            è¯­ä¹‰å›¾è°±ä¿¡æ¯å­—å…¸
        """
        try:
            # æ£€æŸ¥è¯­ä¹‰å›¾è°±æ–‡ä»¶
            semantic_map_dir = graph_dir / "semantic_map_data"
            if semantic_map_dir.exists():
                # å°è¯•è¯»å–è¯­ä¹‰å›¾è°±åŸºæœ¬ä¿¡æ¯
                data_file = semantic_map_dir / "semantic_map_data.pkl"
                if data_file.exists():
                    # ç”±äºæ˜¯pickleæ–‡ä»¶ï¼Œæˆ‘ä»¬åªè®°å½•å­˜åœ¨ä¿¡æ¯
                    return {
                        "has_semantic_map": True,
                        "semantic_map_path": str(semantic_map_dir),
                        "data_file_size": data_file.stat().st_size,
                        "last_modified": datetime.fromtimestamp(data_file.stat().st_mtime).isoformat()
                    }
            
            # æ£€æŸ¥NetworkXå›¾æ–‡ä»¶
            nx_files = list(graph_dir.glob("*.gml")) + list(graph_dir.glob("*.pkl"))
            if nx_files:
                return {
                    "has_networkx_graph": True,
                    "graph_files": [str(f.relative_to(graph_dir)) for f in nx_files],
                    "graph_file_sizes": {f.name: f.stat().st_size for f in nx_files}
                }
            
            return None
            
        except Exception as e:
            self.logger.warning(f"æå–è¯­ä¹‰å›¾è°±ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def create_entity_relationship_summary(self, dataset: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ›å»ºå®ä½“å…³ç³»æ±‡æ€» - é€‚é…æ–°æ¶æ„
        
        Args:
            dataset: å®Œæ•´æ•°æ®é›†
            
        Returns:
            å®ä½“å…³ç³»æ±‡æ€»æ•°æ®
        """
        summary = {
            "metadata": {
                "created_at": datetime.now().isoformat(),
                "total_samples": len(dataset["samples"]),
                "semantic_graph_version": "new_architecture"
            },
            "entity_summary": {
                "all_entities": {},
                "entity_types": {},
                "entity_frequency": {},
                "entity_confidence_stats": {}
            },
            "relationship_summary": {
                "all_relationships": {},
                "relationship_types": {},
                "relationship_patterns": {},
                "relationship_strength_stats": {}
            },
            "memory_space_analysis": {
                "space_types": {},
                "space_unit_distribution": {},
                "average_units_per_space": 0
            },
            "cross_sample_analysis": {
                "common_entities": {},
                "common_relationships": {},
                "entity_co_occurrence": {},
                "semantic_similarity_clusters": {}
            }
        }
        
        all_entities = {}
        all_relationships = {}
        entity_types_count = {}
        relationship_types_count = {}
        entity_confidences = []
        relationship_strengths = []
        space_info = {}
        
        # éå†æ‰€æœ‰æ ·æœ¬
        for sample_id, sample_data in dataset["samples"].items():
            entities = sample_data.get("entities", [])
            relationships = sample_data.get("relationships", [])
            graph_structure = sample_data.get("graph_structure", {})
            
            # å¤„ç†å®ä½“ - åŒ…å«æ–°æ¶æ„çš„ç½®ä¿¡åº¦ä¿¡æ¯
            for entity in entities:
                entity_name = entity.get("name", "").lower()
                entity_type = entity.get("type", "unknown")
                confidence = entity.get("confidence", 0.0)
                
                if entity_name:
                    # å®ä½“é¢‘ç‡ç»Ÿè®¡
                    if entity_name not in all_entities:
                        all_entities[entity_name] = {
                            "name": entity.get("name", ""),
                            "type": entity_type,
                            "descriptions": [],
                            "samples": [],
                            "frequency": 0,
                            "avg_confidence": 0.0,
                            "source_texts": []
                        }
                    
                    all_entities[entity_name]["descriptions"].append({
                        "sample_id": sample_id,
                        "description": entity.get("description", ""),
                        "confidence": confidence,
                        "source_text": entity.get("source_text", "")[:100] + "..." if len(entity.get("source_text", "")) > 100 else entity.get("source_text", "")
                    })
                    all_entities[entity_name]["samples"].append(sample_id)
                    all_entities[entity_name]["frequency"] += 1
                    
                    # ç½®ä¿¡åº¦ç»Ÿè®¡
                    entity_confidences.append(confidence)
                
                # å®ä½“ç±»å‹ç»Ÿè®¡
                entity_types_count[entity_type] = entity_types_count.get(entity_type, 0) + 1
            
            # å¤„ç†å…³ç³» - åŒ…å«æ–°æ¶æ„çš„å¼ºåº¦ä¿¡æ¯
            for relationship in relationships:
                source = relationship.get("source", "").lower()
                target = relationship.get("target", "").lower()
                rel_type = relationship.get("type", "unknown")
                strength = relationship.get("strength", 0.0)
                
                rel_key = f"{source}-{rel_type}-{target}"
                
                if rel_key not in all_relationships:
                    all_relationships[rel_key] = {
                        "source": relationship.get("source", ""),
                        "target": relationship.get("target", ""),
                        "type": rel_type,
                        "descriptions": [],
                        "samples": [],
                        "frequency": 0,
                        "avg_strength": 0.0,
                        "keywords": set()
                    }
                
                all_relationships[rel_key]["descriptions"].append({
                    "sample_id": sample_id,
                    "description": relationship.get("description", ""),
                    "strength": strength,
                    "keywords": relationship.get("keywords", [])
                })
                all_relationships[rel_key]["samples"].append(sample_id)
                all_relationships[rel_key]["frequency"] += 1
                all_relationships[rel_key]["keywords"].update(relationship.get("keywords", []))
                
                # å¼ºåº¦ç»Ÿè®¡
                relationship_strengths.append(strength)
                
                # å…³ç³»ç±»å‹ç»Ÿè®¡
                relationship_types_count[rel_type] = relationship_types_count.get(rel_type, 0) + 1
            
            # å¤„ç†å†…å­˜ç©ºé—´ä¿¡æ¯ - æ–°æ¶æ„ç‰¹æœ‰
            memory_spaces = graph_structure.get("total_memory_spaces", 0)
            memory_units = graph_structure.get("total_memory_units", 0)
            if memory_spaces > 0:
                space_info[sample_id] = {
                    "spaces": memory_spaces,
                    "units": memory_units,
                    "units_per_space": memory_units / memory_spaces if memory_spaces > 0 else 0
                }
        
        # è®¡ç®—å¹³å‡å€¼å’Œç»Ÿè®¡ä¿¡æ¯
        for entity_data in all_entities.values():
            confidences = [desc["confidence"] for desc in entity_data["descriptions"] if desc["confidence"] > 0]
            entity_data["avg_confidence"] = sum(confidences) / len(confidences) if confidences else 0.0
        
        for rel_data in all_relationships.values():
            strengths = [desc["strength"] for desc in rel_data["descriptions"] if desc["strength"] > 0]
            rel_data["avg_strength"] = sum(strengths) / len(strengths) if strengths else 0.0
            rel_data["keywords"] = list(rel_data["keywords"])
        
        # å¡«å……æ±‡æ€»æ•°æ®
        summary["entity_summary"]["all_entities"] = dict(sorted(
            all_entities.items(), key=lambda x: x[1]["frequency"], reverse=True
        ))
        summary["entity_summary"]["entity_types"] = dict(sorted(
            entity_types_count.items(), key=lambda x: x[1], reverse=True
        ))
        summary["entity_summary"]["entity_frequency"] = {
            "total_unique_entities": len(all_entities),
            "most_frequent": max(all_entities.values(), key=lambda x: x["frequency"]) if all_entities else None,
            "cross_sample_entities": {k: v for k, v in all_entities.items() if v["frequency"] > 1}
        }
        summary["entity_summary"]["entity_confidence_stats"] = {
            "avg_confidence": sum(entity_confidences) / len(entity_confidences) if entity_confidences else 0.0,
            "min_confidence": min(entity_confidences) if entity_confidences else 0.0,
            "max_confidence": max(entity_confidences) if entity_confidences else 0.0
        }
        
        summary["relationship_summary"]["all_relationships"] = dict(sorted(
            all_relationships.items(), key=lambda x: x[1]["frequency"], reverse=True
        ))
        summary["relationship_summary"]["relationship_types"] = dict(sorted(
            relationship_types_count.items(), key=lambda x: x[1], reverse=True
        ))
        summary["relationship_summary"]["relationship_patterns"] = {
            "total_unique_relationships": len(all_relationships),
            "most_frequent": max(all_relationships.values(), key=lambda x: x["frequency"]) if all_relationships else None,
            "cross_sample_relationships": {k: v for k, v in all_relationships.items() if v["frequency"] > 1}
        }
        summary["relationship_summary"]["relationship_strength_stats"] = {
            "avg_strength": sum(relationship_strengths) / len(relationship_strengths) if relationship_strengths else 0.0,
            "min_strength": min(relationship_strengths) if relationship_strengths else 0.0,
            "max_strength": max(relationship_strengths) if relationship_strengths else 0.0
        }
        
        # å†…å­˜ç©ºé—´åˆ†æ - æ–°æ¶æ„ç‰¹æœ‰
        if space_info:
            total_spaces = sum(info["spaces"] for info in space_info.values())
            total_units = sum(info["units"] for info in space_info.values())
            
            summary["memory_space_analysis"] = {
                "total_spaces_across_samples": total_spaces,
                "total_units_across_samples": total_units,
                "average_spaces_per_sample": total_spaces / len(space_info) if space_info else 0,
                "average_units_per_sample": total_units / len(space_info) if space_info else 0,
                "average_units_per_space": total_units / total_spaces if total_spaces > 0 else 0,
                "sample_space_distribution": space_info
            }
        
        return summary
    
    def create_sample_index(self, dataset: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ›å»ºæ ·æœ¬ç´¢å¼•æ–‡ä»¶ - é€‚é…æ–°æ¶æ„
        
        Args:
            dataset: å®Œæ•´æ•°æ®é›†
            
        Returns:
            æ ·æœ¬ç´¢å¼•æ•°æ®
        """
        index = {
            "metadata": {
                "created_at": datetime.now().isoformat(),
                "total_samples": len(dataset["samples"]),
                "semantic_graph_version": "new_architecture"
            },
            "sample_index": {}
        }
        
        for sample_id, sample_data in dataset["samples"].items():
            sample_info = sample_data.get("sample_info", {})
            extraction_stats = sample_data.get("extraction_statistics", {})
            graph_structure = sample_data.get("graph_structure", {})
            semantic_graph_info = sample_data.get("semantic_graph_info", {})
            
            index["sample_index"][sample_id] = {
                # åŸºæœ¬ä¿¡æ¯
                "speakers": sample_info.get("speakers", []),
                "session_count": sample_info.get("session_count", 0),
                "total_messages": sample_info.get("total_messages", 0),
                
                # æŠ½å–ç»Ÿè®¡
                "entity_count": extraction_stats.get("total_entities", 0),
                "relationship_count": extraction_stats.get("total_relationships", 0),
                "keyword_count": extraction_stats.get("total_keywords", 0),
                "processing_time": extraction_stats.get("processing_time", ""),
                
                # æ–°æ¶æ„ç‰¹æœ‰ä¿¡æ¯
                "memory_units": graph_structure.get("total_memory_units", 0),
                "memory_spaces": graph_structure.get("total_memory_spaces", 0),
                "networkx_nodes": graph_structure.get("networkx_nodes", 0),
                "networkx_edges": graph_structure.get("networkx_edges", 0),
                "faiss_vectors": graph_structure.get("faiss_vectors", 0),
                
                # è¯­ä¹‰å›¾è°±ä¿¡æ¯
                "has_semantic_graph": bool(semantic_graph_info),
                "semantic_graph_size": semantic_graph_info.get("data_file_size", 0) if semantic_graph_info else 0,
                
                # æ–‡ä»¶è·¯å¾„
                "source_directory": sample_data.get("source_files", {}).get("directory", "")
            }
        
        return index
    
    def generate_dataset_files(self, output_prefix: str = "locomo_extracted") -> Dict[str, Path]:
        """
        ç”Ÿæˆå®Œæ•´çš„æ•°æ®é›†æ–‡ä»¶ - é€‚é…æ–°æ¶æ„
        
        Args:
            output_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€
            
        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        self.logger.info("ğŸš€ å¼€å§‹ç”Ÿæˆ LoCoMo æŠ½å–æ•°æ®é›†ï¼ˆæ–°æ¶æ„ç‰ˆæœ¬ï¼‰...")
        
        # 1. æ”¶é›†æ‰€æœ‰æŠ½å–ç»“æœ
        self.logger.info("ğŸ“Š æ”¶é›†æŠ½å–ç»“æœ...")
        dataset = self.collect_extraction_results()
        
        if not dataset["samples"]:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æŠ½å–ç»“æœ")
        
        # 2. ç”Ÿæˆå®Œæ•´æ•°æ®é›†æ–‡ä»¶
        full_dataset_path = self.output_dir / f"{output_prefix}_full_dataset.json"
        with open(full_dataset_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        self.logger.info(f"âœ… å®Œæ•´æ•°æ®é›†å·²ä¿å­˜: {full_dataset_path}")
        
        # 3. ç”Ÿæˆå®ä½“å…³ç³»æ±‡æ€»
        self.logger.info("ğŸ“ˆ ç”Ÿæˆå®ä½“å…³ç³»æ±‡æ€»...")
        summary = self.create_entity_relationship_summary(dataset)
        summary_path = self.output_dir / f"{output_prefix}_entity_relationship_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        self.logger.info(f"âœ… å®ä½“å…³ç³»æ±‡æ€»å·²ä¿å­˜: {summary_path}")
        
        # 4. ç”Ÿæˆæ ·æœ¬ç´¢å¼•
        self.logger.info("ğŸ“‹ ç”Ÿæˆæ ·æœ¬ç´¢å¼•...")
        index = self.create_sample_index(dataset)
        index_path = self.output_dir / f"{output_prefix}_sample_index.json"
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        self.logger.info(f"âœ… æ ·æœ¬ç´¢å¼•å·²ä¿å­˜: {index_path}")
        
        # 5. ç”Ÿæˆè½»é‡çº§ç‰ˆæœ¬ï¼ˆä»…åŒ…å«æ ¸å¿ƒæ•°æ®ï¼‰
        self.logger.info("ğŸ’¾ ç”Ÿæˆè½»é‡çº§æ•°æ®é›†...")
        lightweight_dataset = self._create_lightweight_dataset(dataset)
        lightweight_path = self.output_dir / f"{output_prefix}_lightweight.json"
        with open(lightweight_path, 'w', encoding='utf-8') as f:
            json.dump(lightweight_dataset, f, ensure_ascii=False, indent=2)
        self.logger.info(f"âœ… è½»é‡çº§æ•°æ®é›†å·²ä¿å­˜: {lightweight_path}")
        
        # 6. ç”Ÿæˆæ¶æ„å…¼å®¹æ€§æŠ¥å‘Š - æ–°åŠŸèƒ½
        self.logger.info("ğŸ”„ ç”Ÿæˆæ¶æ„å…¼å®¹æ€§æŠ¥å‘Š...")
        compatibility_report = self._generate_compatibility_report(dataset)
        compatibility_path = self.output_dir / f"{output_prefix}_architecture_compatibility.json"
        with open(compatibility_path, 'w', encoding='utf-8') as f:
            json.dump(compatibility_report, f, ensure_ascii=False, indent=2)
        self.logger.info(f"âœ… æ¶æ„å…¼å®¹æ€§æŠ¥å‘Šå·²ä¿å­˜: {compatibility_path}")
        
        # 7. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.logger.info("ğŸ“Š ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
        report = self._generate_statistics_report(dataset, summary)
        report_path = self.output_dir / f"{output_prefix}_statistics_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        self.logger.info(f"âœ… ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        generated_files = {
            "full_dataset": full_dataset_path,
            "entity_relationship_summary": summary_path,
            "sample_index": index_path,
            "lightweight_dataset": lightweight_path,
            "architecture_compatibility": compatibility_path,
            "statistics_report": report_path
        }
        
        self.logger.info(f"ğŸ‰ æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len(generated_files)} ä¸ªæ–‡ä»¶")
        return generated_files
    
    def _generate_compatibility_report(self, dataset: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ¶æ„å…¼å®¹æ€§æŠ¥å‘Š - æ–°åŠŸèƒ½
        
        Args:
            dataset: å®Œæ•´æ•°æ®é›†
            
        Returns:
            å…¼å®¹æ€§æŠ¥å‘Šæ•°æ®
        """
        report = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "architecture_version": "new_semantic_graph",
                "total_samples": len(dataset["samples"])
            },
            "compatibility_analysis": {
                "memory_space_usage": {},
                "memory_unit_distribution": {},
                "semantic_graph_features": {},
                "faiss_index_usage": {}
            },
            "migration_recommendations": [],
            "performance_metrics": {}
        }
        
        memory_space_counts = []
        memory_unit_counts = []
        faiss_vector_counts = []
        samples_with_semantic_graph = 0
        
        for sample_id, sample_data in dataset["samples"].items():
            graph_structure = sample_data.get("graph_structure", {})
            semantic_graph_info = sample_data.get("semantic_graph_info", {})
            
            # å†…å­˜ç©ºé—´ä½¿ç”¨æƒ…å†µ
            memory_spaces = graph_structure.get("total_memory_spaces", 0)
            memory_units = graph_structure.get("total_memory_units", 0)
            faiss_vectors = graph_structure.get("faiss_vectors", 0)
            
            memory_space_counts.append(memory_spaces)
            memory_unit_counts.append(memory_units)
            faiss_vector_counts.append(faiss_vectors)
            
            if semantic_graph_info:
                samples_with_semantic_graph += 1
        
        # ç»Ÿè®¡åˆ†æ
        if memory_space_counts:
            report["compatibility_analysis"]["memory_space_usage"] = {
                "average_spaces_per_sample": sum(memory_space_counts) / len(memory_space_counts),
                "min_spaces": min(memory_space_counts),
                "max_spaces": max(memory_space_counts),
                "total_spaces": sum(memory_space_counts)
            }
        
        if memory_unit_counts:
            report["compatibility_analysis"]["memory_unit_distribution"] = {
                "average_units_per_sample": sum(memory_unit_counts) / len(memory_unit_counts),
                "min_units": min(memory_unit_counts),
                "max_units": max(memory_unit_counts),
                "total_units": sum(memory_unit_counts)
            }
        
        if faiss_vector_counts:
            report["compatibility_analysis"]["faiss_index_usage"] = {
                "average_vectors_per_sample": sum(faiss_vector_counts) / len(faiss_vector_counts),
                "samples_with_vectors": len([c for c in faiss_vector_counts if c > 0]),
                "total_vectors": sum(faiss_vector_counts)
            }
        
        report["compatibility_analysis"]["semantic_graph_features"] = {
            "samples_with_semantic_graph": samples_with_semantic_graph,
            "semantic_graph_coverage": samples_with_semantic_graph / len(dataset["samples"]) if dataset["samples"] else 0
        }
        
        # è¿ç§»å»ºè®®
        if samples_with_semantic_graph < len(dataset["samples"]):
            report["migration_recommendations"].append({
                "issue": "Incomplete semantic graph coverage",
                "description": f"Only {samples_with_semantic_graph}/{len(dataset['samples'])} samples have semantic graph data",
                "recommendation": "Re-run entity extraction with semantic graph persistence enabled"
            })
        
        if sum(faiss_vector_counts) == 0:
            report["migration_recommendations"].append({
                "issue": "No FAISS vectors found",
                "description": "Samples do not contain FAISS index vectors",
                "recommendation": "Rebuild semantic map indexes to enable vector similarity search"
            })
        
        return report
    
    def _create_lightweight_dataset(self, dataset: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºè½»é‡çº§æ•°æ®é›†ï¼ˆç§»é™¤source_textç­‰å¤§å­—æ®µï¼‰- é€‚é…æ–°æ¶æ„"""
        lightweight = {
            "metadata": dataset["metadata"],
            "samples": {}
        }
        
        for sample_id, sample_data in dataset["samples"].items():
            lightweight_sample = {
                "sample_info": sample_data.get("sample_info", {}),
                "extraction_statistics": sample_data.get("extraction_statistics", {}),
                "entities": [],
                "relationships": [],
                "content_keywords": sample_data.get("content_keywords", []),
                "entity_statistics": sample_data.get("entity_statistics", {}),
                "graph_structure": sample_data.get("graph_structure", {}),
                # æ–°æ¶æ„ç‰¹æœ‰å­—æ®µ
                "memory_space_summary": self._summarize_memory_spaces(sample_data),
                "semantic_graph_summary": self._summarize_semantic_graph(sample_data)
            }
            
            # ç®€åŒ–å®ä½“ä¿¡æ¯
            for entity in sample_data.get("entities", []):
                lightweight_sample["entities"].append({
                    "name": entity.get("name", ""),
                    "type": entity.get("type", ""),
                    "description": entity.get("description", ""),
                    "confidence": entity.get("confidence", 0.0)
                    # ç§»é™¤ source_text ä»¥å‡å°æ–‡ä»¶å¤§å°
                })
            
            # ç®€åŒ–å…³ç³»ä¿¡æ¯
            for relationship in sample_data.get("relationships", []):
                lightweight_sample["relationships"].append({
                    "source": relationship.get("source", ""),
                    "target": relationship.get("target", ""),
                    "type": relationship.get("type", ""),
                    "description": relationship.get("description", ""),
                    "strength": relationship.get("strength", 0.0),
                    "keywords": relationship.get("keywords", [])[:5]  # é™åˆ¶å…³é”®è¯æ•°é‡
                    # ç§»é™¤ source_text
                })
            
            lightweight["samples"][sample_id] = lightweight_sample
        
        return lightweight
    
    def _summarize_memory_spaces(self, sample_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ€»ç»“å†…å­˜ç©ºé—´ä¿¡æ¯ - æ–°æ¶æ„ç‰¹æœ‰ - ä¿®å¤ç‰ˆæœ¬"""
        graph_structure = sample_data.get("graph_structure", {})
        memory_space_analysis = sample_data.get("memory_space_analysis", {})
        
        return {
            "total_memory_spaces": graph_structure.get("total_memory_spaces", 0),
            "total_memory_units": graph_structure.get("total_memory_units", 0),
            "space_types_detected": list(memory_space_analysis.keys()) if memory_space_analysis else ["entities", "extracted_data", "conversations"],  # åŸºäºå®é™…æ•°æ®æˆ–å¸¸è§æ¨¡å¼
            "space_unit_distribution": memory_space_analysis
        }
    
    def _summarize_semantic_graph(self, sample_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ€»ç»“è¯­ä¹‰å›¾è°±ä¿¡æ¯ - æ–°æ¶æ„ç‰¹æœ‰"""
        graph_structure = sample_data.get("graph_structure", {})
        semantic_graph_info = sample_data.get("semantic_graph_info", {})
        
        return {
            "has_networkx_graph": graph_structure.get("networkx_nodes", 0) > 0,
            "has_faiss_index": graph_structure.get("faiss_vectors", 0) > 0,
            "has_persistent_storage": bool(semantic_graph_info),
            "graph_density": graph_structure.get("networkx_edges", 0) / max(1, graph_structure.get("networkx_nodes", 1)),
            "vector_coverage": min(1.0, graph_structure.get("faiss_vectors", 0) / max(1, graph_structure.get("total_memory_units", 1)))
        }
    
    def _generate_statistics_report(self, dataset: Dict[str, Any], summary: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š - é€‚é…æ–°æ¶æ„"""
        metadata = dataset["metadata"]
        extraction_summary = metadata["extraction_summary"]
        
        report = f"""
        # LoCoMo å®ä½“å…³ç³»æŠ½å–æ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š
        ## è¯­ä¹‰å›¾è°±æ–°æ¶æ„ç‰ˆæœ¬

        ## åŸºæœ¬ä¿¡æ¯
        - ç”Ÿæˆæ—¶é—´: {metadata['created_at']}
        - æ¶æ„ç‰ˆæœ¬: {metadata.get('semantic_graph_version', 'new_architecture')}
        - æºç›®å½•: {metadata['source_directory']}
        - æ ·æœ¬æ€»æ•°: {metadata['total_samples']}

        ## æŠ½å–ç»Ÿè®¡
        - å®ä½“æ€»æ•°: {extraction_summary['total_entities']}
        - å…³ç³»æ€»æ•°: {extraction_summary['total_relationships']}
        - å…³é”®è¯æ€»æ•°: {extraction_summary['total_keywords']}
        - å†…å­˜å•å…ƒæ€»æ•°: {extraction_summary.get('total_memory_units', 0)}
        - å†…å­˜ç©ºé—´æ€»æ•°: {extraction_summary.get('total_memory_spaces', 0)}
        - å¹³å‡æ¯æ ·æœ¬å®ä½“æ•°: {extraction_summary['avg_entities_per_sample']:.2f}
        - å¹³å‡æ¯æ ·æœ¬å…³ç³»æ•°: {extraction_summary['avg_relationships_per_sample']:.2f}
        - å¹³å‡æ¯æ ·æœ¬å†…å­˜å•å…ƒæ•°: {extraction_summary.get('avg_memory_units_per_sample', 0):.2f}

        ## å®ä½“ç±»å‹åˆ†å¸ƒ
        """
        
        entity_types = summary["entity_summary"]["entity_types"]
        for entity_type, count in list(entity_types.items())[:10]:
            report += f"- {entity_type}: {count}\n"
        
        # æ·»åŠ ç½®ä¿¡åº¦ç»Ÿè®¡
        confidence_stats = summary["entity_summary"].get("entity_confidence_stats", {})
        if confidence_stats:
            report += f"""
            ## å®ä½“ç½®ä¿¡åº¦ç»Ÿè®¡
            - å¹³å‡ç½®ä¿¡åº¦: {confidence_stats.get('avg_confidence', 0):.3f}
            - æœ€å°ç½®ä¿¡åº¦: {confidence_stats.get('min_confidence', 0):.3f}
            - æœ€å¤§ç½®ä¿¡åº¦: {confidence_stats.get('max_confidence', 0):.3f}
            """
        
        report += f"""
        ## å…³ç³»ç±»å‹åˆ†å¸ƒ
        """
        
        rel_types = summary["relationship_summary"]["relationship_types"]
        for rel_type, count in list(rel_types.items())[:10]:
            report += f"- {rel_type}: {count}\n"
        
        # æ·»åŠ å…³ç³»å¼ºåº¦ç»Ÿè®¡
        strength_stats = summary["relationship_summary"].get("relationship_strength_stats", {})
        if strength_stats:
            report += f"""
            ## å…³ç³»å¼ºåº¦ç»Ÿè®¡
            - å¹³å‡å¼ºåº¦: {strength_stats.get('avg_strength', 0):.3f}
            - æœ€å°å¼ºåº¦: {strength_stats.get('min_strength', 0):.3f}
            - æœ€å¤§å¼ºåº¦: {strength_stats.get('max_strength', 0):.3f}
            """
        
        # æ·»åŠ å†…å­˜ç©ºé—´åˆ†æ
        memory_analysis = summary.get("memory_space_analysis", {})
        if memory_analysis:
            report += f"""
            ## å†…å­˜ç©ºé—´åˆ†æ (æ–°æ¶æ„ç‰¹æœ‰)
            - æ ·æœ¬é—´æ€»ç©ºé—´æ•°: {memory_analysis.get('total_spaces_across_samples', 0)}
            - æ ·æœ¬é—´æ€»å•å…ƒæ•°: {memory_analysis.get('total_units_across_samples', 0)}
            - å¹³å‡æ¯æ ·æœ¬ç©ºé—´æ•°: {memory_analysis.get('average_spaces_per_sample', 0):.2f}
            - å¹³å‡æ¯æ ·æœ¬å•å…ƒæ•°: {memory_analysis.get('average_units_per_sample', 0):.2f}
            - å¹³å‡æ¯ç©ºé—´å•å…ƒæ•°: {memory_analysis.get('average_units_per_space', 0):.2f}
            """
        
        report += f"""
        ## è·¨æ ·æœ¬åˆ†æ
        - å”¯ä¸€å®ä½“æ•°: {summary['entity_summary']['entity_frequency']['total_unique_entities']}
        - è·¨æ ·æœ¬å®ä½“æ•°: {len(summary['entity_summary']['entity_frequency']['cross_sample_entities'])}
        - å”¯ä¸€å…³ç³»æ•°: {summary['relationship_summary']['relationship_patterns']['total_unique_relationships']}
        - è·¨æ ·æœ¬å…³ç³»æ•°: {len(summary['relationship_summary']['relationship_patterns']['cross_sample_relationships'])}

        ## æ ·æœ¬è¯¦æƒ…
        """
        
        for sample_id, sample_data in dataset["samples"].items():
            stats = sample_data.get("extraction_statistics", {})
            info = sample_data.get("sample_info", {})
            graph_structure = sample_data.get("graph_structure", {})
            semantic_graph_info = sample_data.get("semantic_graph_info", {})
            
            report += f"""
            ### {sample_id}
            - è¯´è¯è€…: {', '.join(info.get('speakers', []))}
            - ä¼šè¯æ•°: {info.get('session_count', 0)}
            - æ¶ˆæ¯æ•°: {info.get('total_messages', 0)}
            - å®ä½“æ•°: {stats.get('total_entities', 0)}
            - å…³ç³»æ•°: {stats.get('total_relationships', 0)}
            - å†…å­˜å•å…ƒæ•°: {graph_structure.get('total_memory_units', 0)}
            - å†…å­˜ç©ºé—´æ•°: {graph_structure.get('total_memory_spaces', 0)}
            - NetworkXèŠ‚ç‚¹æ•°: {graph_structure.get('networkx_nodes', 0)}
            - NetworkXè¾¹æ•°: {graph_structure.get('networkx_edges', 0)}
            - FAISSå‘é‡æ•°: {graph_structure.get('faiss_vectors', 0)}
            - è¯­ä¹‰å›¾è°±: {'å·²ä¿å­˜' if semantic_graph_info else 'æœªä¿å­˜'}
            - å¤„ç†æ—¶é—´: {stats.get('processing_time', 'N/A')}
            """
        
        return report


def main():
    """å‘½ä»¤è¡Œå…¥å£ - é€‚é…æ–°æ¶æ„"""
    parser = argparse.ArgumentParser(description="LoCoMo æ•°æ®é›†åˆ¶ä½œå™¨ (æ–°æ¶æ„ç‰ˆæœ¬)")
    parser.add_argument(
        "--results-dir", 
        required=True,
        help="æŠ½å–ç»“æœç›®å½•è·¯å¾„ (å¦‚ benchmark/extractor/results)"
    )
    parser.add_argument(
        "--output-dir", 
        default="benchmark/dataset/locomo/extraction",
        help="è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: benchmark/dataset/locomo/extraction)"
    )
    parser.add_argument(
        "--output-prefix", 
        default="locomo_extracted",
        help="è¾“å‡ºæ–‡ä»¶å‰ç¼€ (é»˜è®¤: locomo_extracted)"
    )
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºæ•°æ®é›†åˆ¶ä½œå™¨
        maker = LoCoMoDatasetMaker(
            results_dir=args.results_dir,
            output_dir=args.output_dir
        )
        
        # ç”Ÿæˆæ•°æ®é›†æ–‡ä»¶
        generated_files = maker.generate_dataset_files(
            output_prefix=args.output_prefix
        )
        
        print("\nğŸ‰ æ•°æ®é›†ç”ŸæˆæˆåŠŸï¼")
        print("\nç”Ÿæˆçš„æ–‡ä»¶:")
        for file_type, file_path in generated_files.items():
            print(f"  {file_type}: {file_path}")
        
        print(f"\nâœ¨ æ–°æ¶æ„ç‰¹æ€§:")
        print(f"  - å†…å­˜ç©ºé—´ç®¡ç†å’Œç»Ÿè®¡")
        print(f"  - è¯­ä¹‰å›¾è°±æŒä¹…åŒ–æ”¯æŒ")
        print(f"  - FAISSå‘é‡ç´¢å¼•è¿½è¸ª")
        print(f"  - æ¶æ„å…¼å®¹æ€§åˆ†æ")
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†ç”Ÿæˆå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
