import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Tuple  # æ·»åŠ  Tuple
from datetime import datetime
import networkx as nx
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from dev.semantic_graph import SemanticGraph
from dev.memory_unit import MemoryUnit
from benchmark.llm_utils.llm_client import LLMClient
from benchmark.extractor.entity_relation_extractor import EntityRelationExtractor
from benchmark.extractor.semantic_graph_integrator import SemanticGraphIntegrator
from benchmark.task_eval.locomo_test_split import load_dataset, ingest_conversation_history

class LoCoMoEntityExtractor:
    """LoCoMoæ•°æ®é›†å®ä½“å…³ç³»æŠ½å–å™¨ - ä¸“é—¨å¤„ç†å•ä¸ªsample_idçš„å®Œæ•´å®ä½“å…³ç³»æŠ½å–"""
    
    def __init__(self, dataset_path: str, output_base_dir: str = None):
        self.dataset_path = Path(dataset_path)
        # self.output_base_dir = Path(output_base_dir) if output_base_dir else Path(__file__).parent.parent / "results"
        self.output_base_dir = Path(output_base_dir) if output_base_dir else Path(__file__).parent / "results"
        self.raw_data = None
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å’ŒæŠ½å–å™¨
        self.llm_client = LLMClient(model_name="deepseek-chat")
        self.entity_extractor = EntityRelationExtractor(self.llm_client)
        
        # åŠ è½½æ•°æ®é›†
        self._load_dataset()
        
    def _load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        logging.info(f"åŠ è½½æ•°æ®é›†: {self.dataset_path}")
        self.raw_data = load_dataset(self.dataset_path)
        if not self.raw_data:
            raise ValueError("æ•°æ®é›†åŠ è½½å¤±è´¥")
        logging.info(f"æˆåŠŸåŠ è½½ {len(self.raw_data)} ä¸ªæ ·æœ¬")
    
    def list_available_samples(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„sample_id"""
        sample_ids = []
        for sample in self.raw_data:
            sample_id = sample.get('sample_id')
            if sample_id:
                sample_ids.append(sample_id)
        return sample_ids
    
    def get_sample_info(self, sample_id: str) -> Dict[str, Any]:
        """è·å–æŒ‡å®šæ ·æœ¬çš„åŸºæœ¬ä¿¡æ¯"""
        sample = self._find_sample(sample_id)
        if not sample:
            return {}
        
        conversation = sample.get('conversation', {})
        speaker_a = conversation.get('speaker_a', 'Unknown')
        speaker_b = conversation.get('speaker_b', 'Unknown')
        
        # ç»Ÿè®¡ä¼šè¯æ•°é‡
        session_count = sum(1 for key in conversation.keys() 
                           if key.startswith('session_') and not key.endswith('_date_time'))
        
        # ç»Ÿè®¡å®é™…å¯¹è¯å†…å®¹
        total_messages = 0
        for key, value in conversation.items():
            if key.startswith('session_') and not key.endswith('_date_time') and isinstance(value, list):
                total_messages += len(value)
        
        return {
            "sample_id": sample_id,
            "speakers": [speaker_a, speaker_b],
            "session_count": session_count,
            "total_messages": total_messages,
            "qa_count": len(sample.get('qa', [])),
            "event_summary_count": len(sample.get('event_summary', {})),
            "observation_count": len(sample.get('observation', {})),
            "session_summary_count": len(sample.get('session_summary', {}))
        }
    
    def _find_sample(self, sample_id: str) -> Optional[dict]:
        """æŸ¥æ‰¾æŒ‡å®šçš„æ ·æœ¬"""
        for sample in self.raw_data:
            if sample.get('sample_id') == sample_id:
                return sample
        return None
    
    # def _extract_full_conversation_text(self, sample: dict) -> str:
    #     """æå–æ ·æœ¬çš„å®Œæ•´å¯¹è¯æ–‡æœ¬"""
    #     conversation = sample.get('conversation', {})
    #     conversation_texts = []
        
    #     # è·å–è¯´è¯è€…ä¿¡æ¯
    #     speaker_a = conversation.get('speaker_a', 'Speaker A')
    #     speaker_b = conversation.get('speaker_b', 'Speaker B')
        
    #     # æŒ‰ä¼šè¯é¡ºåºæå–å¯¹è¯å†…å®¹
    #     session_keys = [key for key in conversation.keys() 
    #                    if key.startswith('session_') and not key.endswith('_date_time')]
    #     session_keys.sort(key=lambda x: int(x.split('_')[1]))
        
    #     for session_key in session_keys:
    #         session_messages = conversation.get(session_key, [])
    #         session_datetime = conversation.get(f"{session_key}_date_time", "")
            
    #         if session_messages and session_datetime:
    #             conversation_texts.append(f"\n=== {session_key.upper()} ({session_datetime}) ===")
                
    #             for message in session_messages:
    #                 if isinstance(message, dict):
    #                     speaker = message.get('speaker', 'Unknown')
    #                     content = message.get('content', '')
    #                     if speaker and content:
    #                         conversation_texts.append(f"{speaker}: {content}")
    #                 elif isinstance(message, str):
    #                     # æœ‰äº›æ ¼å¼å¯èƒ½æ˜¯ç®€å•çš„å­—ç¬¦ä¸²
    #                     conversation_texts.append(message)
        
    #     # æ·»åŠ ä¼šè¯æ‘˜è¦ä¿¡æ¯
    #     session_summary = sample.get('session_summary', {})
    #     if session_summary:
    #         conversation_texts.append("\n=== SESSION SUMMARIES ===")
    #         for summary_key, summary_content in session_summary.items():
    #             if isinstance(summary_content, str):
    #                 conversation_texts.append(f"{summary_key}: {summary_content}")
        
    #     # æ·»åŠ äº‹ä»¶æ‘˜è¦
    #     event_summary = sample.get('event_summary', {})
    #     if event_summary:
    #         conversation_texts.append("\n=== EVENT SUMMARIES ===")
    #         for event_key, event_content in event_summary.items():
    #             if isinstance(event_content, dict):
    #                 for sub_key, sub_content in event_content.items():
    #                     if isinstance(sub_content, str):
    #                         conversation_texts.append(f"{event_key}.{sub_key}: {sub_content}")
        
    #     # æ·»åŠ è§‚å¯Ÿè®°å½•
    #     observations = sample.get('observation', {})
    #     if observations:
    #         conversation_texts.append("\n=== OBSERVATIONS ===")
    #         for obs_key, obs_content in observations.items():
    #             if isinstance(obs_content, dict):
    #                 for sub_key, sub_content in obs_content.items():
    #                     if isinstance(sub_content, str):
    #                         conversation_texts.append(f"{obs_key}.{sub_key}: {sub_content}")
        
    #     full_text = "\n".join(conversation_texts)
        
    #     # æ·»åŠ åŸºæœ¬ä¸Šä¸‹æ–‡ä¿¡æ¯
    #     context_info = f"""
    #     === CONVERSATION CONTEXT ===
    #     Sample ID: {sample.get('sample_id', 'Unknown')}
    #     Participants: {speaker_a} and {speaker_b}
    #     Total Sessions: {len(session_keys)}
    #     Total Messages: {sum(len(conversation.get(key, [])) for key in session_keys)}

    #     === FULL CONVERSATION CONTENT ===
    #     {full_text}
    #     """
        
    #     return context_info
    # def _extract_full_conversation_text(self, sample: dict) -> Tuple[str, Dict[str, Tuple[int, int]]]:
    #     """æå–æ ·æœ¬çš„å®Œæ•´å¯¹è¯æ–‡æœ¬ï¼ŒåŒæ—¶è¿”å›ä½ç½®ç´¢å¼•"""
    #     conversation = sample.get('conversation', {})
    #     conversation_texts = []
    #     position_map = {}  # è®°å½•æ¯ä¸ªç‰‡æ®µåœ¨å®Œæ•´æ–‡æœ¬ä¸­çš„ä½ç½®
        
    #     # è·å–è¯´è¯è€…ä¿¡æ¯
    #     speaker_a = conversation.get('speaker_a', 'Speaker A')
    #     speaker_b = conversation.get('speaker_b', 'Speaker B')
        
    #     # æ·»åŠ åŸºæœ¬ä¸Šä¸‹æ–‡ä¿¡æ¯
    #     context_info = f"""=== CONVERSATION CONTEXT ===
    #     Sample ID: {sample.get('sample_id', 'Unknown')}
    #     Participants: {speaker_a} and {speaker_b}
    #     Total Sessions: {len([k for k in conversation.keys() if k.startswith('session_') and not k.endswith('_date_time')])}

    #     === FULL CONVERSATION CONTENT ===

    #     """
    #     conversation_texts.append(context_info)
    #     current_position = len(context_info)
        
    #     # æŒ‰ä¼šè¯é¡ºåºæå–å¯¹è¯å†…å®¹
    #     session_keys = [key for key in conversation.keys() 
    #                 if key.startswith('session_') and not key.endswith('_date_time')]
    #     session_keys.sort(key=lambda x: int(x.split('_')[1]))
        
    #     for session_key in session_keys:
    #         session_messages = conversation.get(session_key, [])
    #         session_datetime = conversation.get(f"{session_key}_date_time", "")
            
    #         if session_messages and session_datetime:
    #             session_header = f"\n=== {session_key.upper()} ({session_datetime}) ===\n"
    #             conversation_texts.append(session_header)
    #             current_position += len(session_header)
                
    #             for msg_idx, message in enumerate(session_messages):
    #                 if isinstance(message, dict):
    #                     speaker = message.get('speaker', 'Unknown')
    #                     content = message.get('content', '')
    #                     if speaker and content:
    #                         message_text = f"{speaker}: {content}\n"
                            
    #                         # è®°å½•è¿™æ¡æ¶ˆæ¯çš„ä½ç½®
    #                         start_pos = current_position
    #                         end_pos = current_position + len(message_text)
    #                         position_map[f"{session_key}_msg_{msg_idx}"] = (start_pos, end_pos, {
    #                             "session": session_key,
    #                             "session_datetime": session_datetime,
    #                             "speaker": speaker,
    #                             "content": content,
    #                             "message_index": msg_idx
    #                         })
                            
    #                         conversation_texts.append(message_text)
    #                         current_position = end_pos
    #                 elif isinstance(message, str):
    #                     message_text = f"{message}\n"
    #                     start_pos = current_position
    #                     end_pos = current_position + len(message_text)
    #                     position_map[f"{session_key}_msg_{msg_idx}"] = (start_pos, end_pos, {
    #                         "session": session_key,
    #                         "session_datetime": session_datetime,
    #                         "content": message,
    #                         "message_index": msg_idx
    #                     })
    #                     conversation_texts.append(message_text)
    #                     current_position = end_pos
        
    #     # æ·»åŠ å…¶ä»–éƒ¨åˆ†ï¼ˆæ‘˜è¦ç­‰ï¼‰
    #     session_summary = sample.get('session_summary', {})
    #     if session_summary:
    #         summary_header = "\n=== SESSION SUMMARIES ===\n"
    #         conversation_texts.append(summary_header)
    #         current_position += len(summary_header)
            
    #         for summary_key, summary_content in session_summary.items():
    #             if isinstance(summary_content, str):
    #                 summary_text = f"{summary_key}: {summary_content}\n"
    #                 start_pos = current_position
    #                 end_pos = current_position + len(summary_text)
    #                 position_map[f"summary_{summary_key}"] = (start_pos, end_pos, {
    #                     "type": "session_summary",
    #                     "key": summary_key,
    #                     "content": summary_content
    #                 })
    #                 conversation_texts.append(summary_text)
    #                 current_position = end_pos
        
    #     # äº‹ä»¶æ‘˜è¦å’Œè§‚å¯Ÿä¹Ÿç±»ä¼¼å¤„ç†...
        
    #     full_text = "".join(conversation_texts)
    #     return full_text, position_map
    # def _extract_full_conversation_text(self, sample: dict) -> Tuple[str, Dict[str, Tuple[int, int]]]:
    #     """æå–æ ·æœ¬çš„å®Œæ•´å¯¹è¯æ–‡æœ¬ï¼ŒåŒæ—¶è¿”å›ä½ç½®ç´¢å¼•"""
    #     conversation = sample.get('conversation', {})
    #     conversation_texts = []
    #     position_map = {}  # è®°å½•æ¯ä¸ªç‰‡æ®µåœ¨å®Œæ•´æ–‡æœ¬ä¸­çš„ä½ç½®
        
    #     # è·å–è¯´è¯è€…ä¿¡æ¯
    #     speaker_a = conversation.get('speaker_a', 'Speaker A')
    #     speaker_b = conversation.get('speaker_b', 'Speaker B')
        
    #     # æ·»åŠ åŸºæœ¬ä¸Šä¸‹æ–‡ä¿¡æ¯
    #     context_info = f"""=== CONVERSATION CONTEXT ===
    #     Sample ID: {sample.get('sample_id', 'Unknown')}
    #     Participants: {speaker_a} and {speaker_b}
    #     Total Sessions: {len([k for k in conversation.keys() if k.startswith('session_') and not k.endswith('_date_time')])}

    #     === FULL CONVERSATION CONTENT ===

    #     """
    #     conversation_texts.append(context_info)
    #     current_position = len(context_info)
        
    #     # 1. æŒ‰ä¼šè¯é¡ºåºæå–å®é™…å¯¹è¯å†…å®¹
    #     session_keys = [key for key in conversation.keys() 
    #                 if key.startswith('session_') and not key.endswith('_date_time')]
    #     session_keys.sort(key=lambda x: int(x.split('_')[1]))
        
    #     for session_key in session_keys:
    #         session_messages = conversation.get(session_key, [])
    #         session_datetime = conversation.get(f"{session_key}_date_time", "")
            
    #         if session_messages and session_datetime:
    #             session_header = f"\n=== {session_key.upper()} ({session_datetime}) ===\n"
    #             conversation_texts.append(session_header)
    #             current_position += len(session_header)
                
    #             for msg_idx, message in enumerate(session_messages):
    #                 if isinstance(message, dict):
    #                     speaker = message.get('speaker', 'Unknown')
    #                     content = message.get('content', '')
    #                     text_content = message.get('text', '')  # æœ‰äº›æ¶ˆæ¯å¯èƒ½ç”¨ 'text' å­—æ®µ
                        
    #                     # ä¼˜å…ˆä½¿ç”¨ contentï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ text
    #                     actual_content = content or text_content
                        
    #                     if speaker and actual_content:
    #                         message_text = f"{speaker}: {actual_content}\n"
                            
    #                         # è®°å½•è¿™æ¡æ¶ˆæ¯çš„ä½ç½®
    #                         start_pos = current_position
    #                         end_pos = current_position + len(message_text)
    #                         position_map[f"{session_key}_msg_{msg_idx}"] = (start_pos, end_pos, {
    #                             "session": session_key,
    #                             "session_datetime": session_datetime,
    #                             "speaker": speaker,
    #                             "content": actual_content,
    #                             "message_index": msg_idx
    #                         })
                            
    #                         conversation_texts.append(message_text)
    #                         current_position = end_pos
    #                 elif isinstance(message, str):
    #                     message_text = f"{message}\n"
    #                     start_pos = current_position
    #                     end_pos = current_position + len(message_text)
    #                     position_map[f"{session_key}_msg_{msg_idx}"] = (start_pos, end_pos, {
    #                         "session": session_key,
    #                         "session_datetime": session_datetime,
    #                         "content": message,
    #                         "message_index": msg_idx
    #                     })
    #                     conversation_texts.append(message_text)
    #                     current_position = end_pos
        
    #     # 2. æ·»åŠ äº‹ä»¶æ‘˜è¦ä¿¡æ¯ (event_summary)
    #     event_summary = sample.get('event_summary', {})
    #     if event_summary:
    #         event_header = "\n=== EVENT SUMMARIES ===\n"
    #         conversation_texts.append(event_header)
    #         current_position += len(event_header)
            
    #         # æŒ‰ä¼šè¯é¡ºåºå¤„ç†äº‹ä»¶æ‘˜è¦
    #         for event_key in sorted(event_summary.keys()):
    #             event_data = event_summary[event_key]
    #             if isinstance(event_data, dict):
    #                 # æ·»åŠ äº‹ä»¶æ—¥æœŸ
    #                 event_date = event_data.get('date', '')
    #                 if event_date:
    #                     date_text = f"\n--- {event_key.upper()} ({event_date}) ---\n"
    #                     conversation_texts.append(date_text)
    #                     current_position += len(date_text)
                    
    #                 # å¤„ç†æ¯ä¸ªè¯´è¯è€…çš„äº‹ä»¶
    #                 for speaker, events in event_data.items():
    #                     if speaker != 'date' and isinstance(events, list):
    #                         for event_idx, event_desc in enumerate(events):
    #                             if isinstance(event_desc, str) and event_desc.strip():
    #                                 event_text = f"{speaker}: {event_desc}\n"
    #                                 start_pos = current_position
    #                                 end_pos = current_position + len(event_text)
    #                                 position_map[f"{event_key}_{speaker}_{event_idx}"] = (start_pos, end_pos, {
    #                                     "type": "event_summary",
    #                                     "session": event_key,
    #                                     "date": event_date,
    #                                     "speaker": speaker,
    #                                     "content": event_desc,
    #                                     "event_index": event_idx
    #                                 })
    #                                 conversation_texts.append(event_text)
    #                                 current_position = end_pos
        
    #     # 3. æ·»åŠ è§‚å¯Ÿè®°å½• (observation)
    #     observations = sample.get('observation', {})
    #     if observations:
    #         obs_header = "\n=== OBSERVATIONS ===\n"
    #         conversation_texts.append(obs_header)
    #         current_position += len(obs_header)
            
    #         # æŒ‰ä¼šè¯é¡ºåºå¤„ç†è§‚å¯Ÿè®°å½•
    #         for obs_key in sorted(observations.keys()):
    #             obs_data = observations[obs_key]
    #             if isinstance(obs_data, dict):
    #                 obs_section_header = f"\n--- {obs_key.upper()} ---\n"
    #                 conversation_texts.append(obs_section_header)
    #                 current_position += len(obs_section_header)
                    
    #                 # å¤„ç†æ¯ä¸ªè¯´è¯è€…çš„è§‚å¯Ÿ
    #                 for speaker, obs_list in obs_data.items():
    #                     if isinstance(obs_list, list):
    #                         for obs_idx, observation in enumerate(obs_list):
    #                             if isinstance(observation, str) and observation.strip():
    #                                 obs_text = f"{speaker} observed: {observation}\n"
    #                                 start_pos = current_position
    #                                 end_pos = current_position + len(obs_text)
    #                                 position_map[f"{obs_key}_{speaker}_{obs_idx}"] = (start_pos, end_pos, {
    #                                     "type": "observation",
    #                                     "session": obs_key,
    #                                     "speaker": speaker,
    #                                     "content": observation,
    #                                     "observation_index": obs_idx
    #                                 })
    #                                 conversation_texts.append(obs_text)
    #                                 current_position = end_pos
        
    #     # 4. æ·»åŠ ä¼šè¯æ‘˜è¦ä¿¡æ¯ (session_summary) - ä½œä¸ºè¡¥å……ä¿¡æ¯
    #     session_summary = sample.get('session_summary', {})
    #     if session_summary:
    #         summary_header = "\n=== SESSION SUMMARIES ===\n"
    #         conversation_texts.append(summary_header)
    #         current_position += len(summary_header)
            
    #         # æŒ‰ä¼šè¯é¡ºåºå¤„ç†æ‘˜è¦
    #         for summary_key in sorted(session_summary.keys()):
    #             summary_content = session_summary[summary_key]
    #             if isinstance(summary_content, str) and summary_content.strip():
    #                 summary_text = f"{summary_key}: {summary_content}\n"
    #                 start_pos = current_position
    #                 end_pos = current_position + len(summary_text)
    #                 position_map[f"summary_{summary_key}"] = (start_pos, end_pos, {
    #                     "type": "session_summary",
    #                     "key": summary_key,
    #                     "content": summary_content
    #                 })
    #                 conversation_texts.append(summary_text)
    #                 current_position = end_pos
        
    #     full_text = "".join(conversation_texts)
    #     return full_text, position_map

    # def _extract_full_conversation_text(self, sample: dict) -> Tuple[str, Dict[str, Tuple[int, int]]]:
    #     """æå–æ ·æœ¬çš„å®Œæ•´å¯¹è¯æ–‡æœ¬ï¼ŒåŒæ—¶è¿”å›ä½ç½®ç´¢å¼•"""
    #     conversation = sample.get('conversation', {})
    #     conversation_texts = []
    #     position_map = {}  # è®°å½•æ¯ä¸ªç‰‡æ®µåœ¨å®Œæ•´æ–‡æœ¬ä¸­çš„ä½ç½®
        
    #     # è·å–è¯´è¯è€…ä¿¡æ¯
    #     speaker_a = conversation.get('speaker_a', 'Speaker A')
    #     speaker_b = conversation.get('speaker_b', 'Speaker B')
        
    #     # æ·»åŠ åŸºæœ¬ä¸Šä¸‹æ–‡ä¿¡æ¯
    #     context_info = f"""=== CONVERSATION CONTEXT ===
    #     Sample ID: {sample.get('sample_id', 'Unknown')}
    #     Participants: {speaker_a} and {speaker_b}

    #     === FULL CONVERSATION CONTENT ===

    #     """
    #     conversation_texts.append(context_info)
    #     current_position = len(context_info)
        
    #     # ç»Ÿè®¡å®é™…æœ‰å†…å®¹çš„éƒ¨åˆ†
    #     has_actual_conversations = False
    #     has_event_summaries = False
    #     has_observations = False
    #     has_session_summaries = False
        
    #     # 1. æŒ‰ä¼šè¯é¡ºåºæå–å®é™…å¯¹è¯å†…å®¹
    #     session_keys = [key for key in conversation.keys() 
    #                 if key.startswith('session_') and not key.endswith('_date_time')]
    #     session_keys.sort(key=lambda x: int(x.split('_')[1]))
        
    #     for session_key in session_keys:
    #         session_messages = conversation.get(session_key, [])
    #         session_datetime = conversation.get(f"{session_key}_date_time", "")
            
    #         if session_messages and session_datetime:
    #             has_actual_conversations = True
    #             session_header = f"\n=== {session_key.upper()} ({session_datetime}) ===\n"
    #             conversation_texts.append(session_header)
    #             current_position += len(session_header)
                
    #             for msg_idx, message in enumerate(session_messages):
    #                 if isinstance(message, dict):
    #                     # å¤„ç†ç»“æ„åŒ–æ¶ˆæ¯
    #                     speaker = message.get('speaker', '')
    #                     content = message.get('content', '')
    #                     text_content = message.get('text', '')
                        
    #                     actual_content = content or text_content
                        
    #                     if speaker and actual_content:
    #                         message_text = f"{speaker}: {actual_content}\n"
    #                     elif actual_content:
    #                         message_text = f"{actual_content}\n"
    #                     else:
    #                         continue
                            
    #                     # è®°å½•è¿™æ¡æ¶ˆæ¯çš„ä½ç½®
    #                     start_pos = current_position
    #                     end_pos = current_position + len(message_text)
    #                     position_map[f"{session_key}_msg_{msg_idx}"] = (start_pos, end_pos, {
    #                         "session": session_key,
    #                         "session_datetime": session_datetime,
    #                         "speaker": speaker or "Unknown",
    #                         "content": actual_content,
    #                         "message_index": msg_idx
    #                     })
                        
    #                     conversation_texts.append(message_text)
    #                     current_position = end_pos
                        
    #                 elif isinstance(message, str) and message.strip():
    #                     # å¤„ç†çº¯æ–‡æœ¬æ¶ˆæ¯
    #                     message_text = f"{message}\n"
    #                     start_pos = current_position
    #                     end_pos = current_position + len(message_text)
    #                     position_map[f"{session_key}_msg_{msg_idx}"] = (start_pos, end_pos, {
    #                         "session": session_key,
    #                         "session_datetime": session_datetime,
    #                         "content": message,
    #                         "message_index": msg_idx
    #                     })
    #                     conversation_texts.append(message_text)
    #                     current_position = end_pos
        
    #     # 2. æ·»åŠ äº‹ä»¶æ‘˜è¦ä¿¡æ¯ (event_summary)
    #     event_summary = sample.get('event_summary', {})
    #     if event_summary:
    #         has_event_summaries = True
    #         event_header = "\n=== EVENT SUMMARIES ===\n"
    #         conversation_texts.append(event_header)
    #         current_position += len(event_header)
            
    #         for event_key in sorted(event_summary.keys()):
    #             event_data = event_summary[event_key]
    #             if isinstance(event_data, dict):
    #                 event_date = event_data.get('date', '')
    #                 if event_date:
    #                     date_text = f"\n--- {event_key.upper()} ({event_date}) ---\n"
    #                     conversation_texts.append(date_text)
    #                     current_position += len(date_text)
                    
    #                 # å¤„ç†æ¯ä¸ªè¯´è¯è€…çš„äº‹ä»¶
    #                 for speaker, events in event_data.items():
    #                     if speaker != 'date' and isinstance(events, list) and events:
    #                         for event_idx, event_desc in enumerate(events):
    #                             if isinstance(event_desc, str) and event_desc.strip():
    #                                 event_text = f"{speaker}: {event_desc}\n"
    #                                 start_pos = current_position
    #                                 end_pos = current_position + len(event_text)
    #                                 position_map[f"{event_key}_{speaker}_{event_idx}"] = (start_pos, end_pos, {
    #                                     "type": "event_summary",
    #                                     "session": event_key,
    #                                     "date": event_date,
    #                                     "speaker": speaker,
    #                                     "content": event_desc,
    #                                     "event_index": event_idx
    #                                 })
    #                                 conversation_texts.append(event_text)
    #                                 current_position = end_pos
        
    #     # 3. æ·»åŠ è§‚å¯Ÿè®°å½• (observation)
    #     observations = sample.get('observation', {})
    #     if observations:
    #         has_observations = True
    #         obs_header = "\n=== OBSERVATIONS ===\n"
    #         conversation_texts.append(obs_header)
    #         current_position += len(obs_header)
            
    #         for obs_key in sorted(observations.keys()):
    #             obs_data = observations[obs_key]
    #             if isinstance(obs_data, dict) and obs_data:
    #                 obs_section_header = f"\n--- {obs_key.upper()} ---\n"
    #                 conversation_texts.append(obs_section_header)
    #                 current_position += len(obs_section_header)
                    
    #                 for speaker, obs_list in obs_data.items():
    #                     if isinstance(obs_list, list) and obs_list:
    #                         for obs_idx, observation in enumerate(obs_list):
    #                             if isinstance(observation, str) and observation.strip():
    #                                 obs_text = f"{speaker} observed: {observation}\n"
    #                                 start_pos = current_position
    #                                 end_pos = current_position + len(obs_text)
    #                                 position_map[f"{obs_key}_{speaker}_{obs_idx}"] = (start_pos, end_pos, {
    #                                     "type": "observation",
    #                                     "session": obs_key,
    #                                     "speaker": speaker,
    #                                     "content": observation,
    #                                     "observation_index": obs_idx
    #                                 })
    #                                 conversation_texts.append(obs_text)
    #                                 current_position = end_pos
        
    #     # 4. æ·»åŠ ä¼šè¯æ‘˜è¦ä¿¡æ¯ (session_summary)
    #     session_summary = sample.get('session_summary', {})
    #     if session_summary:
    #         has_session_summaries = True
    #         summary_header = "\n=== SESSION SUMMARIES ===\n"
    #         conversation_texts.append(summary_header)
    #         current_position += len(summary_header)
            
    #         for summary_key in sorted(session_summary.keys()):
    #             summary_content = session_summary[summary_key]
    #             if isinstance(summary_content, str) and summary_content.strip():
    #                 summary_text = f"{summary_key}: {summary_content}\n\n"
    #                 start_pos = current_position
    #                 end_pos = current_position + len(summary_text)
    #                 position_map[f"summary_{summary_key}"] = (start_pos, end_pos, {
    #                     "type": "session_summary",
    #                     "key": summary_key,
    #                     "content": summary_content
    #                 })
    #                 conversation_texts.append(summary_text)
    #                 current_position = end_pos
        
    #     # 5. æ·»åŠ æ•°æ®æ¥æºè¯´æ˜
    #     data_sources = []
    #     if has_actual_conversations:
    #         data_sources.append("å®é™…å¯¹è¯è®°å½•")
    #     if has_event_summaries:
    #         data_sources.append("äº‹ä»¶æ‘˜è¦")
    #     if has_observations:
    #         data_sources.append("è§‚å¯Ÿè®°å½•")
    #     if has_session_summaries:
    #         data_sources.append("ä¼šè¯æ‘˜è¦")
        
    #     if not data_sources:
    #         data_sources.append("æ— æœ‰æ•ˆå†…å®¹")
        
    #     data_source_info = f"\n=== æ•°æ®æ¥æº ===\næœ¬æ ·æœ¬åŒ…å«: {', '.join(data_sources)}\n"
    #     conversation_texts.append(data_source_info)
        
    #     full_text = "".join(conversation_texts)
    #     return full_text, position_map
    def _extract_full_conversation_text(self, sample: dict) -> Tuple[str, Dict[str, Tuple[int, int]]]:
        """æå–æ ·æœ¬çš„å®Œæ•´å¯¹è¯æ–‡æœ¬ï¼ŒåŒæ—¶è¿”å›ä½ç½®ç´¢å¼•"""
        conversation = sample.get('conversation', {})
        conversation_texts = []
        position_map = {}  # è®°å½•æ¯ä¸ªç‰‡æ®µåœ¨å®Œæ•´æ–‡æœ¬ä¸­çš„ä½ç½®
        
        # è·å–è¯´è¯è€…ä¿¡æ¯
        speaker_a = conversation.get('speaker_a', 'Speaker A')
        speaker_b = conversation.get('speaker_b', 'Speaker B')
        
        # æ·»åŠ åŸºæœ¬ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = f"""=== CONVERSATION CONTEXT ===
        Sample ID: {sample.get('sample_id', 'Unknown')}
        Participants: {speaker_a} and {speaker_b}

        === FULL CONVERSATION CONTENT ===

        """
        conversation_texts.append(context_info)
        current_position = len(context_info)
        
        # ç»Ÿè®¡å®é™…æœ‰å†…å®¹çš„éƒ¨åˆ†
        has_actual_conversations = False
        has_event_summaries = False
        has_observations = False
        has_session_summaries = False
        
        # 1. æŒ‰ä¼šè¯é¡ºåºæå–å®é™…å¯¹è¯å†…å®¹
        session_keys = [key for key in conversation.keys() 
                    if key.startswith('session_') and not key.endswith('_date_time')]
        session_keys.sort(key=lambda x: int(x.split('_')[1]))
        
        for session_key in session_keys:
            session_messages = conversation.get(session_key, [])
            session_datetime = conversation.get(f"{session_key}_date_time", "")
            
            if session_messages and session_datetime:
                has_actual_conversations = True
                session_header = f"\n=== {session_key.upper()} ({session_datetime}) ===\n"
                conversation_texts.append(session_header)
                current_position += len(session_header)
                
                for msg_idx, message in enumerate(session_messages):
                    if isinstance(message, dict):
                        # å¤„ç†ç»“æ„åŒ–æ¶ˆæ¯
                        speaker = message.get('speaker', '')
                        content = message.get('content', '')
                        text_content = message.get('text', '')
                        
                        actual_content = content or text_content
                        
                        if speaker and actual_content:
                            message_text = f"{speaker}: {actual_content}\n"
                        elif actual_content:
                            message_text = f"{actual_content}\n"
                        else:
                            continue
                            
                        # è®°å½•è¿™æ¡æ¶ˆæ¯çš„ä½ç½®
                        start_pos = current_position
                        end_pos = current_position + len(message_text)
                        position_map[f"{session_key}_msg_{msg_idx}"] = (start_pos, end_pos, {
                            "session": session_key,
                            "session_datetime": session_datetime,
                            "speaker": speaker or "Unknown",
                            "content": actual_content,
                            "message_index": msg_idx
                        })
                        
                        conversation_texts.append(message_text)
                        current_position = end_pos
                        
                    elif isinstance(message, str) and message.strip():
                        # å¤„ç†çº¯æ–‡æœ¬æ¶ˆæ¯
                        message_text = f"{message}\n"
                        start_pos = current_position
                        end_pos = current_position + len(message_text)
                        position_map[f"{session_key}_msg_{msg_idx}"] = (start_pos, end_pos, {
                            "session": session_key,
                            "session_datetime": session_datetime,
                            "content": message,
                            "message_index": msg_idx
                        })
                        conversation_texts.append(message_text)
                        current_position = end_pos

        # 2. æ·»åŠ è§‚å¯Ÿè®°å½• (observation) - ä¿®æ­£è¿™éƒ¨åˆ†
        observations = sample.get('observation', {})
        if observations:
            has_observations = True
            obs_header = "\n=== OBSERVATIONS ===\n"
            conversation_texts.append(obs_header)
            current_position += len(obs_header)
            
            # æŒ‰sessioné¡ºåºå¤„ç†è§‚å¯Ÿè®°å½•
            obs_session_keys = [key for key in observations.keys() if key.endswith('_observation')]
            obs_session_keys.sort(key=lambda x: int(x.split('_')[1]) if x.split('_')[1].isdigit() else 0)
            
            for obs_session_key in obs_session_keys:
                obs_session_data = observations[obs_session_key]
                if isinstance(obs_session_data, dict) and obs_session_data:
                    # æå–sessionç¼–å·
                    session_num = obs_session_key.replace('_observation', '')
                    obs_section_header = f"\n--- {session_num.upper()} OBSERVATIONS ---\n"
                    conversation_texts.append(obs_section_header)
                    current_position += len(obs_section_header)
                    
                    # å¤„ç†æ¯ä¸ªè¯´è¯è€…çš„è§‚å¯Ÿè®°å½•
                    for speaker, observations_list in obs_session_data.items():
                        if isinstance(observations_list, list) and observations_list:
                            speaker_header = f"\n{speaker} observations:\n"
                            conversation_texts.append(speaker_header)
                            current_position += len(speaker_header)
                            
                            for obs_idx, observation_item in enumerate(observations_list):
                                if isinstance(observation_item, list) and len(observation_item) >= 1:
                                    # è§‚å¯Ÿè®°å½•æ ¼å¼: [è§‚å¯Ÿå†…å®¹, è¯æ®ID(å¯é€‰)]
                                    obs_content = observation_item[0] if observation_item[0] else ""
                                    evidence_id = observation_item[1] if len(observation_item) > 1 else ""
                                    
                                    if obs_content.strip():
                                        if evidence_id:
                                            obs_text = f"  â€¢ {obs_content} (Evidence: {evidence_id})\n"
                                        else:
                                            obs_text = f"  â€¢ {obs_content}\n"
                                        
                                        start_pos = current_position
                                        end_pos = current_position + len(obs_text)
                                        position_map[f"{obs_session_key}_{speaker}_{obs_idx}"] = (start_pos, end_pos, {
                                            "type": "observation",
                                            "session": session_num,
                                            "speaker": speaker,
                                            "content": obs_content,
                                            "evidence_id": evidence_id,
                                            "observation_index": obs_idx
                                        })
                                        conversation_texts.append(obs_text)
                                        current_position = end_pos
                                elif isinstance(observation_item, str) and observation_item.strip():
                                    # ç®€å•å­—ç¬¦ä¸²æ ¼å¼çš„è§‚å¯Ÿè®°å½•
                                    obs_text = f"  â€¢ {observation_item}\n"
                                    start_pos = current_position
                                    end_pos = current_position + len(obs_text)
                                    position_map[f"{obs_session_key}_{speaker}_{obs_idx}"] = (start_pos, end_pos, {
                                        "type": "observation",
                                        "session": session_num,
                                        "speaker": speaker,
                                        "content": observation_item,
                                        "observation_index": obs_idx
                                    })
                                    conversation_texts.append(obs_text)
                                    current_position = end_pos

        # 3. æ·»åŠ äº‹ä»¶æ‘˜è¦ä¿¡æ¯ (event_summary)
        event_summary = sample.get('event_summary', {})
        if event_summary:
            has_event_summaries = True
            event_header = "\n=== EVENT SUMMARIES ===\n"
            conversation_texts.append(event_header)
            current_position += len(event_header)
            
            for event_key in sorted(event_summary.keys()):
                event_data = event_summary[event_key]
                if isinstance(event_data, dict):
                    event_date = event_data.get('date', '')
                    if event_date:
                        date_text = f"\n--- {event_key.upper()} ({event_date}) ---\n"
                        conversation_texts.append(date_text)
                        current_position += len(date_text)
                    
                    # å¤„ç†æ¯ä¸ªè¯´è¯è€…çš„äº‹ä»¶
                    for speaker, events in event_data.items():
                        if speaker != 'date' and isinstance(events, list) and events:
                            speaker_header = f"\n{speaker} events:\n"
                            conversation_texts.append(speaker_header)
                            current_position += len(speaker_header)
                            
                            for event_idx, event_desc in enumerate(events):
                                if isinstance(event_desc, str) and event_desc.strip():
                                    event_text = f"  â€¢ {event_desc}\n"
                                    start_pos = current_position
                                    end_pos = current_position + len(event_text)
                                    position_map[f"{event_key}_{speaker}_{event_idx}"] = (start_pos, end_pos, {
                                        "type": "event_summary",
                                        "session": event_key,
                                        "date": event_date,
                                        "speaker": speaker,
                                        "content": event_desc,
                                        "event_index": event_idx
                                    })
                                    conversation_texts.append(event_text)
                                    current_position = end_pos
        
        # 4. æ·»åŠ ä¼šè¯æ‘˜è¦ä¿¡æ¯ (session_summary)
        session_summary = sample.get('session_summary', {})
        if session_summary:
            has_session_summaries = True
            summary_header = "\n=== SESSION SUMMARIES ===\n"
            conversation_texts.append(summary_header)
            current_position += len(summary_header)
            
            for summary_key in sorted(session_summary.keys()):
                summary_content = session_summary[summary_key]
                if isinstance(summary_content, str) and summary_content.strip():
                    summary_text = f"\n{summary_key}: {summary_content}\n\n"
                    start_pos = current_position
                    end_pos = current_position + len(summary_text)
                    position_map[f"summary_{summary_key}"] = (start_pos, end_pos, {
                        "type": "session_summary",
                        "key": summary_key,
                        "content": summary_content
                    })
                    conversation_texts.append(summary_text)
                    current_position = end_pos
        
        # 5. æ·»åŠ æ•°æ®æ¥æºè¯´æ˜
        data_sources = []
        if has_actual_conversations:
            data_sources.append("å®é™…å¯¹è¯è®°å½•")
        if has_observations:
            data_sources.append("è§‚å¯Ÿè®°å½•")
        if has_event_summaries:
            data_sources.append("äº‹ä»¶æ‘˜è¦")
        if has_session_summaries:
            data_sources.append("ä¼šè¯æ‘˜è¦")
        
        if not data_sources:
            data_sources.append("æ— æœ‰æ•ˆå†…å®¹")
        
        data_source_info = f"\n=== æ•°æ®æ¥æºè¯´æ˜ ===\næœ¬æ ·æœ¬åŒ…å«: {', '.join(data_sources)}\n"
        conversation_texts.append(data_source_info)
        
        full_text = "".join(conversation_texts)
        return full_text, position_map
    
    def extract_entities_and_relations_for_sample(self, 
                                                sample_id: str, 
                                                use_chunking: bool = True,
                                                save_intermediate: bool = True) -> Dict[str, Any]:
        """
        ä¸ºæŒ‡å®šæ ·æœ¬è¿›è¡Œå®Œæ•´çš„å®ä½“å…³ç³»æŠ½å–
        
        Args:
            sample_id: æ ·æœ¬ID
            use_chunking: æ˜¯å¦ä½¿ç”¨æ–‡æœ¬åˆ†å—å¤„ç†
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
        """
        
        print(f"ğŸš€ å¼€å§‹å¯¹æ ·æœ¬ {sample_id} è¿›è¡Œå®ä½“å…³ç³»æŠ½å–")
        
        # 1. æŸ¥æ‰¾æ ·æœ¬
        sample = self._find_sample(sample_id)
        if not sample:
            raise ValueError(f"æœªæ‰¾åˆ°sample_idä¸º{sample_id}çš„æ ·æœ¬")
        
        # 2. åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = self.output_base_dir / f"{sample_id}_entity_extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        output_dir.mkdir(parents=True, exist_ok=True)
        self._setup_logging(output_dir)
        
        # 3. è·å–æ ·æœ¬ä¿¡æ¯
        sample_info = self.get_sample_info(sample_id)
        print(f"ğŸ“Š æ ·æœ¬ä¿¡æ¯: {sample_info}")
        
        # 4. æå–å®Œæ•´å¯¹è¯æ–‡æœ¬
        # print("\n=== æå–å®Œæ•´å¯¹è¯æ–‡æœ¬ ===")
        # full_conversation_text = self._extract_full_conversation_text(sample)
        print("\n=== æå–å®Œæ•´å¯¹è¯æ–‡æœ¬ ===")
        full_conversation_text, position_map = self._extract_full_conversation_text(sample)
        
        # ä¿å­˜åŸå§‹æ–‡æœ¬
        if save_intermediate:
            text_file = output_dir / f"{sample_id}_full_conversation.txt"
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(full_conversation_text)
            print(f"ğŸ’¾ å®Œæ•´å¯¹è¯æ–‡æœ¬å·²ä¿å­˜åˆ°: {text_file}")
        
        print(f"âœ… æå–çš„æ–‡æœ¬é•¿åº¦: {len(full_conversation_text)} å­—ç¬¦")
        
        # 5. ä½¿ç”¨EntityRelationExtractorè¿›è¡Œå®ä½“å…³ç³»æŠ½å–
        print(f"\n=== å¼€å§‹å®ä½“å…³ç³»æŠ½å– ===")
        
        try:
            if use_chunking:
                print("ä½¿ç”¨æ™ºèƒ½åˆ†å—å¤„ç†é•¿æ–‡æœ¬...")
                entities, relationships, content_keywords = self.entity_extractor.extract_entities_and_relations_chunked_with_position(
                    full_conversation_text, position_map
                )
            else:
                print("ä½¿ç”¨ç›´æ¥å¤„ç†æ–¹å¼...")
                entities, relationships, content_keywords = self.entity_extractor.extract_entities_and_relations_with_position(
                    full_conversation_text, position_map
                )
        ## 5. ä½¿ç”¨å¸¦ä½ç½®è¿½è¸ªçš„EntityRelationExtractorè¿›è¡Œå®ä½“å…³ç³»æŠ½å–
        # print(f"\n=== å¼€å§‹å®ä½“å…³ç³»æŠ½å– ===")
        
        # try:
        #     if use_chunking:
        #         print("ä½¿ç”¨æ™ºèƒ½åˆ†å—å¤„ç†é•¿æ–‡æœ¬...")
        #         entities, relationships, content_keywords = self.entity_extractor.extract_entities_and_relations_chunked(full_conversation_text)
        #     else:
        #         print("ä½¿ç”¨ç›´æ¥å¤„ç†æ–¹å¼...")
        #         entities, relationships, content_keywords = self.entity_extractor.extract_entities_and_relations(full_conversation_text)
            
            print(f"âœ… å®ä½“å…³ç³»æŠ½å–å®Œæˆ!")
            print(f"   æŠ½å–å®ä½“æ•°: {len(entities)}")
            print(f"   æŠ½å–å…³ç³»æ•°: {len(relationships)}")
            print(f"   å…³é”®è¯æ•°: {len(content_keywords)}")
            
        except Exception as e:
            logging.error(f"å®ä½“å…³ç³»æŠ½å–å¤±è´¥: {e}")
            print(f"âŒ å®ä½“å…³ç³»æŠ½å–å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
        
        # 6. åˆ›å»ºè¯­ä¹‰å›¾å¹¶æ·»åŠ æŠ½å–ç»“æœ
        print("\n=== æ„å»ºè¯­ä¹‰å›¾ ===")
        graph = SemanticGraph()
        
        # åˆ›å»ºåŸå§‹æ–‡æ¡£å•å…ƒ
        doc_unit = MemoryUnit(
            uid=f"{sample_id}_document",
            raw_data={
                "text_content": full_conversation_text,
                "sample_id": sample_id,
                "speakers": sample_info["speakers"],
                "session_count": sample_info["session_count"],
                "total_messages": sample_info["total_messages"]
            },
            metadata={
                "data_source": "locomo_full_conversation",
                "sample_id": sample_id,
                "conversation_type": "multi_session_dialog"
            }
        )
        
        # æ·»åŠ æ–‡æ¡£å•å…ƒåˆ°å›¾ä¸­
        graph.add_unit(doc_unit, space_names=["locomo_documents"])
        
        # 7. ä½¿ç”¨SemanticGraphIntegratorå°†å®ä½“å’Œå…³ç³»æ·»åŠ åˆ°å›¾ä¸­
        integrator = SemanticGraphIntegrator(graph)
        
        print("=== æ·»åŠ å®ä½“åˆ°è¯­ä¹‰å›¾ ===")
        entity_id_map = integrator.add_entities_to_graph(entities, doc_unit.uid)
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(entity_id_map)} ä¸ªå®ä½“")
        
        print("=== æ·»åŠ å…³ç³»åˆ°è¯­ä¹‰å›¾ ===")
        relationship_count = integrator.add_relationships_to_graph(relationships, entity_id_map, doc_unit.uid)
        print(f"âœ… æˆåŠŸæ·»åŠ  {relationship_count} ä¸ªå…³ç³»")
        
        # 8. æ„å»ºè¯­ä¹‰å›¾ç´¢å¼•
        print("=== æ„å»ºè¯­ä¹‰å›¾ç´¢å¼• ===")
        graph.build_semantic_map_index()
        
        # 9. æ˜¾ç¤ºå›¾è°±æ‘˜è¦
        print("\n=== è¯­ä¹‰å›¾æ‘˜è¦ ===")
        graph.display_graph_summary()
        
        # 10. ä¿å­˜å®Œæ•´çš„è¯­ä¹‰å›¾
        print("\n=== ä¿å­˜è¯­ä¹‰å›¾ ===")
        semantic_graph_path = output_dir / f"{sample_id}_semantic_graph"
        graph.save_graph(str(semantic_graph_path))
        print(f"âœ… è¯­ä¹‰å›¾å·²ä¿å­˜åˆ°: {semantic_graph_path}")
        
        # 11. è·å–å¹¶ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        entity_stats = integrator.get_entity_statistics()
        
        # 12. ä¿å­˜æŠ½å–ç»“æœ
        extraction_results = {
            "sample_info": sample_info,
            "extraction_statistics": {
                "total_entities": len(entities),
                "total_relationships": len(relationships),
                "total_keywords": len(content_keywords),
                "processing_time": str(datetime.now())
            },
            "entities": [
                {
                    "name": e.name,
                    "type": e.entity_type,
                    "description": e.description,
                    "confidence": e.confidence,
                    "source_text": e.source_text[:200] + "..." if len(e.source_text) > 200 else e.source_text
                } for e in entities
            ],
            "relationships": [
                {
                    "source": r.source_entity,
                    "target": r.target_entity,
                    "type": r.relationship_type,
                    "description": r.description,
                    "keywords": r.keywords,
                    "strength": r.strength,
                    "source_text": r.source_text[:200] + "..." if len(r.source_text) > 200 else r.source_text
                } for r in relationships
            ],
            "content_keywords": content_keywords,
            "entity_statistics": entity_stats,
            "graph_structure": {
                "total_memory_units": len(graph.semantic_map.memory_units),
                "total_memory_spaces": len(graph.semantic_map.memory_spaces),
                "networkx_nodes": graph.nx_graph.number_of_nodes(),
                "networkx_edges": graph.nx_graph.number_of_edges(),
                "faiss_vectors": graph.semantic_map.faiss_index.ntotal if graph.semantic_map.faiss_index else 0
            }
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = output_dir / f"{sample_id}_extraction_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(extraction_results, f, ensure_ascii=False, indent=2, default=str)
        print(f"âœ… æŠ½å–ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # 13. å¯¼å‡ºå®ä½“å’Œå…³ç³»çš„è¯¦ç»†ä¿¡æ¯
        self._export_detailed_analysis(graph, output_dir, sample_id)
        
        print(f"\nğŸ‰ æ ·æœ¬ {sample_id} çš„å®ä½“å…³ç³»æŠ½å–å®Œæˆ!")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
        return {
            "success": True,
            "sample_id": sample_id,
            "output_directory": output_dir,
            "graph": graph,
            "extraction_results": extraction_results,
            "entities_count": len(entities),
            "relationships_count": len(relationships),
            "keywords_count": len(content_keywords)
        }
    
    def _export_detailed_analysis(self, graph: SemanticGraph, output_dir: Path, sample_id: str):
        """å¯¼å‡ºè¯¦ç»†çš„å®ä½“å’Œå…³ç³»åˆ†æ"""
        
        analysis = {
            "sample_id": sample_id,
            "analysis_timestamp": str(datetime.now()),
            "entity_analysis": {},
            "relationship_analysis": {},
            "network_analysis": {}
        }
        
        # å®ä½“åˆ†æ
        entity_space = graph.semantic_map.get_memory_space("extracted_entities")
        if entity_space:
            entity_details = []
            entity_type_counts = {}
            
            for uid in entity_space.get_memory_uids():
                unit = graph.get_unit(uid)
                if unit:
                    entity_type = unit.raw_data.get('entity_type', 'Unknown')
                    entity_details.append({
                        "uid": uid,
                        "name": unit.raw_data.get('entity_name', 'Unknown'),
                        "type": entity_type,
                        "description": unit.raw_data.get('description', ''),
                        "confidence": unit.raw_data.get('confidence', 0.0)
                    })
                    entity_type_counts[entity_type] = entity_type_counts.get(entity_type, 0) + 1
            
            analysis["entity_analysis"] = {
                "total_entities": len(entity_details),
                "entity_type_distribution": entity_type_counts,
                "entities": entity_details
            }
        
        # å…³ç³»åˆ†æ
        relationship_details = []
        relationship_type_counts = {}
        
        for source, target, data in graph.nx_graph.edges(data=True):
            if data.get("source_unit_id"):  # åªåˆ†ææŠ½å–å‡ºçš„å…³ç³»
                rel_type = data.get("type", "UNKNOWN")
                
                # è·å–å®ä½“åç§°
                source_unit = graph.get_unit(source)
                target_unit = graph.get_unit(target)
                source_name = source_unit.raw_data.get('entity_name', source) if source_unit else source
                target_name = target_unit.raw_data.get('entity_name', target) if target_unit else target
                
                relationship_details.append({
                    "source_uid": source,
                    "target_uid": target,
                    "source_name": source_name,
                    "target_name": target_name,
                    "relationship_type": rel_type,
                    "description": data.get("description", ""),
                    "strength": data.get("strength", 0.0)
                })
                
                relationship_type_counts[rel_type] = relationship_type_counts.get(rel_type, 0) + 1
        
        analysis["relationship_analysis"] = {
            "total_relationships": len(relationship_details),
            "relationship_type_distribution": relationship_type_counts,
            "relationships": relationship_details
        }
        
        # ç½‘ç»œåˆ†æ
        analysis["network_analysis"] = {
            "graph_density": nx.density(graph.nx_graph),
            "number_of_nodes": graph.nx_graph.number_of_nodes(),
            "number_of_edges": graph.nx_graph.number_of_edges(),
            "is_connected": nx.is_connected(graph.nx_graph.to_undirected()),
            "number_of_connected_components": nx.number_connected_components(graph.nx_graph.to_undirected())
        }
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = output_dir / f"{sample_id}_detailed_analysis.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… è¯¦ç»†åˆ†æå·²ä¿å­˜åˆ°: {analysis_file}")
        
        # ç”Ÿæˆç®€è¦æŠ¥å‘Š
        report_lines = [
            f"=== {sample_id} å®ä½“å…³ç³»æŠ½å–æŠ¥å‘Š ===",
            f"å¤„ç†æ—¶é—´: {analysis['analysis_timestamp']}",
            f"",
            f"ğŸ“Š å®ä½“ç»Ÿè®¡:",
            f"  - æ€»å®ä½“æ•°: {analysis['entity_analysis'].get('total_entities', 0)}",
        ]
        
        for entity_type, count in analysis['entity_analysis'].get('entity_type_distribution', {}).items():
            report_lines.append(f"  - {entity_type}: {count} ä¸ª")
        
        report_lines.extend([
            f"",
            f"ğŸ”— å…³ç³»ç»Ÿè®¡:",
            f"  - æ€»å…³ç³»æ•°: {analysis['relationship_analysis'].get('total_relationships', 0)}",
        ])
        
        for rel_type, count in analysis['relationship_analysis'].get('relationship_type_distribution', {}).items():
            report_lines.append(f"  - {rel_type}: {count} ä¸ª")
        
        report_lines.extend([
            f"",
            f"ğŸ•¸ï¸ ç½‘ç»œç‰¹å¾:",
            f"  - èŠ‚ç‚¹æ•°: {analysis['network_analysis']['number_of_nodes']}",
            f"  - è¾¹æ•°: {analysis['network_analysis']['number_of_edges']}",
            f"  - å›¾å¯†åº¦: {analysis['network_analysis']['graph_density']:.4f}",
            f"  - è¿é€šæ€§: {'æ˜¯' if analysis['network_analysis']['is_connected'] else 'å¦'}",
            f"  - è¿é€šç»„ä»¶æ•°: {analysis['network_analysis']['number_of_connected_components']}",
        ])
        
        report_content = "\n".join(report_lines)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = output_dir / f"{sample_id}_summary_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        print("\n" + report_content)
    
    def _setup_logging(self, output_dir: Path):
        """è®¾ç½®æ—¥å¿—"""
        log_file = output_dir / "extraction.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
    
    def batch_extract_samples(self, 
                            sample_ids: list = None, 
                            use_chunking: bool = True,
                            save_intermediate: bool = False) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªæ ·æœ¬çš„å®ä½“å…³ç³»æŠ½å–
        
        Args:
            sample_ids: è¦å¤„ç†çš„æ ·æœ¬IDåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰æ ·æœ¬
            use_chunking: æ˜¯å¦ä½¿ç”¨æ–‡æœ¬åˆ†å—å¤„ç†
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
        """
        
        if sample_ids is None:
            sample_ids = self.list_available_samples()
        
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡å®ä½“å…³ç³»æŠ½å–ï¼Œå…± {len(sample_ids)} ä¸ªæ ·æœ¬")
        
        batch_results = {
            "processed_samples": [],
            "failed_samples": [],
            "total_entities": 0,
            "total_relationships": 0,
            "total_keywords": 0,
            "start_time": str(datetime.now())
        }
        
        for i, sample_id in enumerate(sample_ids):
            print(f"\n--- å¤„ç†æ ·æœ¬ {i+1}/{len(sample_ids)}: {sample_id} ---")
            
            try:
                result = self.extract_entities_and_relations_for_sample(
                    sample_id=sample_id,
                    use_chunking=use_chunking,
                    save_intermediate=save_intermediate
                )
                
                if result.get("success"):
                    batch_results["processed_samples"].append({
                        "sample_id": sample_id,
                        "entities_count": result.get("entities_count", 0),
                        "relationships_count": result.get("relationships_count", 0),
                        "keywords_count": result.get("keywords_count", 0),
                        "output_directory": str(result.get("output_directory", ""))
                    })
                    
                    batch_results["total_entities"] += result.get("entities_count", 0)
                    batch_results["total_relationships"] += result.get("relationships_count", 0)
                    batch_results["total_keywords"] += result.get("keywords_count", 0)
                    
                    print(f"âœ… æ ·æœ¬ {sample_id} å¤„ç†å®Œæˆ")
                else:
                    batch_results["failed_samples"].append({
                        "sample_id": sample_id,
                        "error": result.get("error", "Unknown error")
                    })
                    print(f"âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥")
                
            except Exception as e:
                print(f"âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
                batch_results["failed_samples"].append({
                    "sample_id": sample_id,
                    "error": str(e)
                })
                logging.error(f"æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥", exc_info=True)
        
        batch_results["end_time"] = str(datetime.now())
        
        # ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ
        batch_summary_file = self.output_base_dir / f"batch_extraction_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(batch_summary_file, 'w', encoding='utf-8') as f:
            json.dump(batch_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸŠ æ‰¹é‡å®ä½“å…³ç³»æŠ½å–å®Œæˆ!")
        print(f"âœ… æˆåŠŸå¤„ç†: {len(batch_results['processed_samples'])} ä¸ªæ ·æœ¬")
        print(f"âŒ å¤±è´¥: {len(batch_results['failed_samples'])} ä¸ªæ ·æœ¬")
        print(f"ğŸ“Š æ€»å®ä½“æ•°: {batch_results['total_entities']}")
        print(f"ğŸ“Š æ€»å…³ç³»æ•°: {batch_results['total_relationships']}")
        print(f"ğŸ“Š æ€»å…³é”®è¯æ•°: {batch_results['total_keywords']}")
        print(f"ğŸ“ æ‰¹é‡æ‘˜è¦å·²ä¿å­˜åˆ°: {batch_summary_file}")
        
        return batch_results

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    import networkx as nx  # æ·»åŠ è¿™ä¸ªå¯¼å…¥
    
    parser = argparse.ArgumentParser(description="LoCoMoæ•°æ®é›†å®ä½“å…³ç³»æŠ½å–å™¨")
    parser.add_argument("--dataset", required=True, help="LoCoMoæ•°æ®é›†è·¯å¾„")
    parser.add_argument("--sample-id", help="è¦å¤„ç†çš„æ ·æœ¬ID")
    parser.add_argument("--list-samples", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ ·æœ¬ID")
    parser.add_argument("--batch", nargs="+", help="æ‰¹é‡å¤„ç†æŒ‡å®šçš„æ ·æœ¬ID")
    parser.add_argument("--batch-all", action="store_true", help="æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬")
    parser.add_argument("--use-chunking", action="store_true", default=True, help="ä½¿ç”¨æ–‡æœ¬åˆ†å—å¤„ç†")
    parser.add_argument("--no-chunking", action="store_true", help="ä¸ä½¿ç”¨æ–‡æœ¬åˆ†å—å¤„ç†")
    parser.add_argument("--save-intermediate", action="store_true", help="ä¿å­˜ä¸­é—´ç»“æœ")
    parser.add_argument("--output", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–å¤„ç†å™¨
        extractor = LoCoMoEntityExtractor(
            dataset_path=args.dataset,
            output_base_dir=args.output
        )
        
        # ç¡®å®šæ˜¯å¦ä½¿ç”¨åˆ†å—
        use_chunking = not args.no_chunking if args.no_chunking else args.use_chunking
        
        # åˆ—å‡ºæ‰€æœ‰æ ·æœ¬
        if args.list_samples:
            samples = extractor.list_available_samples()
            print(f"æ•°æ®é›†ä¸­åŒ…å« {len(samples)} ä¸ªæ ·æœ¬:")
            for i, sample_id in enumerate(samples, 1):
                info = extractor.get_sample_info(sample_id)
                print(f"  {i:2d}. {sample_id} - {info['speakers'][0]} & {info['speakers'][1]} "
                      f"({info['session_count']} sessions, {info['total_messages']} messages)")
            return 0
        
        # å¤„ç†å•ä¸ªæ ·æœ¬
        if args.sample_id:
            start_time = datetime.now()
            print(f"â° å¼€å§‹æ—¶é—´: {start_time}")
            
            result = extractor.extract_entities_and_relations_for_sample(
                sample_id=args.sample_id,
                use_chunking=use_chunking,
                save_intermediate=args.save_intermediate
            )
            
            end_time = datetime.now()
            if result.get("success"):
                print(f"\nğŸ‰ æ ·æœ¬ {args.sample_id} å¤„ç†å®Œæˆ!")
                print(f"â° å®Œæˆæ—¶é—´: {end_time}")
                print(f"â±ï¸  æ€»è€—æ—¶: {end_time - start_time}")
                print(f"ğŸ“ è¾“å‡ºç›®å½•: {result['output_directory']}")
                print(f"ğŸ“Š æŠ½å–ç»Ÿè®¡: {result['entities_count']} å®ä½“, {result['relationships_count']} å…³ç³»")
            else:
                print(f"âŒ æ ·æœ¬ {args.sample_id} å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
        # æ‰¹é‡å¤„ç†æŒ‡å®šæ ·æœ¬
        elif args.batch:
            extractor.batch_extract_samples(
                sample_ids=args.batch,
                use_chunking=use_chunking,
                save_intermediate=args.save_intermediate
            )
            
        # æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬
        elif args.batch_all:
            extractor.batch_extract_samples(
                sample_ids=None,
                use_chunking=use_chunking,
                save_intermediate=args.save_intermediate
            )
            
        else:
            print("è¯·æŒ‡å®šè¦æ‰§è¡Œçš„æ“ä½œ:")
            print("  --list-samples: åˆ—å‡ºæ‰€æœ‰æ ·æœ¬")
            print("  --sample-id SAMPLE_ID: å¤„ç†å•ä¸ªæ ·æœ¬")
            print("  --batch SAMPLE1 SAMPLE2 ...: æ‰¹é‡å¤„ç†æŒ‡å®šæ ·æœ¬")
            print("  --batch-all: æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬")
            return 1
        
    except Exception as e:
        logging.error(f"æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
