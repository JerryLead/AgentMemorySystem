import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime
import networkx as nx
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from dev.semantic_graph import SemanticGraph
from dev.memory_unit import MemoryUnit
from benchmark.llm_utils.llm_client import LLMClient
from benchmark.extractor.entity_relation_extractor import EntityRelationExtractor
from benchmark.extractor.semantic_graph_integrator import SemanticGraphIntegrator
from benchmark.task_eval.locomo_test_split import load_dataset, ingest_conversation_history

class LoCoMoEntityExtractor:
    """LoCoMoæ•°æ®é›†å®ä½“å…³ç³»æŠ½å–å™¨ - ä¸“é—¨å¤„ç†å•ä¸ªsample_idçš„å®Œæ•´å®ä½“å…³ç³»æŠ½å–"""
    
    def __init__(self, dataset_path: str, output_base_dir: str = None):
        self.dataset_path = Path(dataset_path)
        # self.output_base_dir = Path(output_base_dir) if output_base_dir else Path(__file__).parent.parent / "results"
        self.output_base_dir = Path(output_base_dir) if output_base_dir else Path(__file__).parent / "results"
        self.raw_data = None
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å’ŒæŠ½å–å™¨
        self.llm_client = LLMClient(model_name="deepseek-chat")
        self.entity_extractor = EntityRelationExtractor(self.llm_client)
        
        # åŠ è½½æ•°æ®é›†
        self._load_dataset()
        
    def _load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        logging.info(f"åŠ è½½æ•°æ®é›†: {self.dataset_path}")
        self.raw_data = load_dataset(self.dataset_path)
        if not self.raw_data:
            raise ValueError("æ•°æ®é›†åŠ è½½å¤±è´¥")
        logging.info(f"æˆåŠŸåŠ è½½ {len(self.raw_data)} ä¸ªæ ·æœ¬")
    
    def list_available_samples(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„sample_id"""
        sample_ids = []
        for sample in self.raw_data:
            sample_id = sample.get('sample_id')
            if sample_id:
                sample_ids.append(sample_id)
        return sample_ids
    
    def get_sample_info(self, sample_id: str) -> Dict[str, Any]:
        """è·å–æŒ‡å®šæ ·æœ¬çš„åŸºæœ¬ä¿¡æ¯"""
        sample = self._find_sample(sample_id)
        if not sample:
            return {}
        
        conversation = sample.get('conversation', {})
        speaker_a = conversation.get('speaker_a', 'Unknown')
        speaker_b = conversation.get('speaker_b', 'Unknown')
        
        # ç»Ÿè®¡ä¼šè¯æ•°é‡
        session_count = sum(1 for key in conversation.keys() 
                           if key.startswith('session_') and not key.endswith('_date_time'))
        
        # ç»Ÿè®¡å®é™…å¯¹è¯å†…å®¹
        total_messages = 0
        for key, value in conversation.items():
            if key.startswith('session_') and not key.endswith('_date_time') and isinstance(value, list):
                total_messages += len(value)
        
        return {
            "sample_id": sample_id,
            "speakers": [speaker_a, speaker_b],
            "session_count": session_count,
            "total_messages": total_messages,
            "qa_count": len(sample.get('qa', [])),
            "event_summary_count": len(sample.get('event_summary', {})),
            "observation_count": len(sample.get('observation', {})),
            "session_summary_count": len(sample.get('session_summary', {}))
        }
    
    def _find_sample(self, sample_id: str) -> Optional[dict]:
        """æŸ¥æ‰¾æŒ‡å®šçš„æ ·æœ¬"""
        for sample in self.raw_data:
            if sample.get('sample_id') == sample_id:
                return sample
        return None
    
    def _extract_full_conversation_text(self, sample: dict) -> str:
        """æå–æ ·æœ¬çš„å®Œæ•´å¯¹è¯æ–‡æœ¬"""
        conversation = sample.get('conversation', {})
        conversation_texts = []
        
        # è·å–è¯´è¯è€…ä¿¡æ¯
        speaker_a = conversation.get('speaker_a', 'Speaker A')
        speaker_b = conversation.get('speaker_b', 'Speaker B')
        
        # æŒ‰ä¼šè¯é¡ºåºæå–å¯¹è¯å†…å®¹
        session_keys = [key for key in conversation.keys() 
                       if key.startswith('session_') and not key.endswith('_date_time')]
        session_keys.sort(key=lambda x: int(x.split('_')[1]))
        
        for session_key in session_keys:
            session_messages = conversation.get(session_key, [])
            session_datetime = conversation.get(f"{session_key}_date_time", "")
            
            if session_messages and session_datetime:
                conversation_texts.append(f"\n=== {session_key.upper()} ({session_datetime}) ===")
                
                for message in session_messages:
                    if isinstance(message, dict):
                        speaker = message.get('speaker', 'Unknown')
                        content = message.get('content', '')
                        if speaker and content:
                            conversation_texts.append(f"{speaker}: {content}")
                    elif isinstance(message, str):
                        # æœ‰äº›æ ¼å¼å¯èƒ½æ˜¯ç®€å•çš„å­—ç¬¦ä¸²
                        conversation_texts.append(message)
        
        # æ·»åŠ ä¼šè¯æ‘˜è¦ä¿¡æ¯
        session_summary = sample.get('session_summary', {})
        if session_summary:
            conversation_texts.append("\n=== SESSION SUMMARIES ===")
            for summary_key, summary_content in session_summary.items():
                if isinstance(summary_content, str):
                    conversation_texts.append(f"{summary_key}: {summary_content}")
        
        # æ·»åŠ äº‹ä»¶æ‘˜è¦
        event_summary = sample.get('event_summary', {})
        if event_summary:
            conversation_texts.append("\n=== EVENT SUMMARIES ===")
            for event_key, event_content in event_summary.items():
                if isinstance(event_content, dict):
                    for sub_key, sub_content in event_content.items():
                        if isinstance(sub_content, str):
                            conversation_texts.append(f"{event_key}.{sub_key}: {sub_content}")
        
        # æ·»åŠ è§‚å¯Ÿè®°å½•
        observations = sample.get('observation', {})
        if observations:
            conversation_texts.append("\n=== OBSERVATIONS ===")
            for obs_key, obs_content in observations.items():
                if isinstance(obs_content, dict):
                    for sub_key, sub_content in obs_content.items():
                        if isinstance(sub_content, str):
                            conversation_texts.append(f"{obs_key}.{sub_key}: {sub_content}")
        
        full_text = "\n".join(conversation_texts)
        
        # æ·»åŠ åŸºæœ¬ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = f"""
=== CONVERSATION CONTEXT ===
Sample ID: {sample.get('sample_id', 'Unknown')}
Participants: {speaker_a} and {speaker_b}
Total Sessions: {len(session_keys)}
Total Messages: {sum(len(conversation.get(key, [])) for key in session_keys)}

=== FULL CONVERSATION CONTENT ===
{full_text}
"""
        
        return context_info
    
    def extract_entities_and_relations_for_sample(self, 
                                                sample_id: str, 
                                                use_chunking: bool = True,
                                                save_intermediate: bool = True) -> Dict[str, Any]:
        """
        ä¸ºæŒ‡å®šæ ·æœ¬è¿›è¡Œå®Œæ•´çš„å®ä½“å…³ç³»æŠ½å–
        
        Args:
            sample_id: æ ·æœ¬ID
            use_chunking: æ˜¯å¦ä½¿ç”¨æ–‡æœ¬åˆ†å—å¤„ç†
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
        """
        
        print(f"ğŸš€ å¼€å§‹å¯¹æ ·æœ¬ {sample_id} è¿›è¡Œå®ä½“å…³ç³»æŠ½å–")
        
        # 1. æŸ¥æ‰¾æ ·æœ¬
        sample = self._find_sample(sample_id)
        if not sample:
            raise ValueError(f"æœªæ‰¾åˆ°sample_idä¸º{sample_id}çš„æ ·æœ¬")
        
        # 2. åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = self.output_base_dir / f"{sample_id}_entity_extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        output_dir.mkdir(parents=True, exist_ok=True)
        self._setup_logging(output_dir)
        
        # 3. è·å–æ ·æœ¬ä¿¡æ¯
        sample_info = self.get_sample_info(sample_id)
        print(f"ğŸ“Š æ ·æœ¬ä¿¡æ¯: {sample_info}")
        
        # 4. æå–å®Œæ•´å¯¹è¯æ–‡æœ¬
        print("\n=== æå–å®Œæ•´å¯¹è¯æ–‡æœ¬ ===")
        full_conversation_text = self._extract_full_conversation_text(sample)
        
        # ä¿å­˜åŸå§‹æ–‡æœ¬
        if save_intermediate:
            text_file = output_dir / f"{sample_id}_full_conversation.txt"
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(full_conversation_text)
            print(f"ğŸ’¾ å®Œæ•´å¯¹è¯æ–‡æœ¬å·²ä¿å­˜åˆ°: {text_file}")
        
        print(f"âœ… æå–çš„æ–‡æœ¬é•¿åº¦: {len(full_conversation_text)} å­—ç¬¦")
        
        # 5. ä½¿ç”¨EntityRelationExtractorè¿›è¡Œå®ä½“å…³ç³»æŠ½å–
        print(f"\n=== å¼€å§‹å®ä½“å…³ç³»æŠ½å– ===")
        
        try:
            if use_chunking:
                print("ä½¿ç”¨æ™ºèƒ½åˆ†å—å¤„ç†é•¿æ–‡æœ¬...")
                entities, relationships, content_keywords = self.entity_extractor.extract_entities_and_relations_chunked(full_conversation_text)
            else:
                print("ä½¿ç”¨ç›´æ¥å¤„ç†æ–¹å¼...")
                entities, relationships, content_keywords = self.entity_extractor.extract_entities_and_relations(full_conversation_text)
            
            print(f"âœ… å®ä½“å…³ç³»æŠ½å–å®Œæˆ!")
            print(f"   æŠ½å–å®ä½“æ•°: {len(entities)}")
            print(f"   æŠ½å–å…³ç³»æ•°: {len(relationships)}")
            print(f"   å…³é”®è¯æ•°: {len(content_keywords)}")
            
        except Exception as e:
            logging.error(f"å®ä½“å…³ç³»æŠ½å–å¤±è´¥: {e}")
            print(f"âŒ å®ä½“å…³ç³»æŠ½å–å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
        
        # 6. åˆ›å»ºè¯­ä¹‰å›¾å¹¶æ·»åŠ æŠ½å–ç»“æœ
        print("\n=== æ„å»ºè¯­ä¹‰å›¾ ===")
        graph = SemanticGraph()
        
        # åˆ›å»ºåŸå§‹æ–‡æ¡£å•å…ƒ
        doc_unit = MemoryUnit(
            uid=f"{sample_id}_document",
            raw_data={
                "text_content": full_conversation_text,
                "sample_id": sample_id,
                "speakers": sample_info["speakers"],
                "session_count": sample_info["session_count"],
                "total_messages": sample_info["total_messages"]
            },
            metadata={
                "data_source": "locomo_full_conversation",
                "sample_id": sample_id,
                "conversation_type": "multi_session_dialog"
            }
        )
        
        # æ·»åŠ æ–‡æ¡£å•å…ƒåˆ°å›¾ä¸­
        graph.add_unit(doc_unit, space_names=["locomo_documents"])
        
        # 7. ä½¿ç”¨SemanticGraphIntegratorå°†å®ä½“å’Œå…³ç³»æ·»åŠ åˆ°å›¾ä¸­
        integrator = SemanticGraphIntegrator(graph)
        
        print("=== æ·»åŠ å®ä½“åˆ°è¯­ä¹‰å›¾ ===")
        entity_id_map = integrator.add_entities_to_graph(entities, doc_unit.uid)
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(entity_id_map)} ä¸ªå®ä½“")
        
        print("=== æ·»åŠ å…³ç³»åˆ°è¯­ä¹‰å›¾ ===")
        relationship_count = integrator.add_relationships_to_graph(relationships, entity_id_map, doc_unit.uid)
        print(f"âœ… æˆåŠŸæ·»åŠ  {relationship_count} ä¸ªå…³ç³»")
        
        # 8. æ„å»ºè¯­ä¹‰å›¾ç´¢å¼•
        print("=== æ„å»ºè¯­ä¹‰å›¾ç´¢å¼• ===")
        graph.build_semantic_map_index()
        
        # 9. æ˜¾ç¤ºå›¾è°±æ‘˜è¦
        print("\n=== è¯­ä¹‰å›¾æ‘˜è¦ ===")
        graph.display_graph_summary()
        
        # 10. ä¿å­˜å®Œæ•´çš„è¯­ä¹‰å›¾
        print("\n=== ä¿å­˜è¯­ä¹‰å›¾ ===")
        semantic_graph_path = output_dir / f"{sample_id}_semantic_graph"
        graph.save_graph(str(semantic_graph_path))
        print(f"âœ… è¯­ä¹‰å›¾å·²ä¿å­˜åˆ°: {semantic_graph_path}")
        
        # 11. è·å–å¹¶ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        entity_stats = integrator.get_entity_statistics()
        
        # 12. ä¿å­˜æŠ½å–ç»“æœ
        extraction_results = {
            "sample_info": sample_info,
            "extraction_statistics": {
                "total_entities": len(entities),
                "total_relationships": len(relationships),
                "total_keywords": len(content_keywords),
                "processing_time": str(datetime.now())
            },
            "entities": [
                {
                    "name": e.name,
                    "type": e.entity_type,
                    "description": e.description,
                    "confidence": e.confidence,
                    "source_text": e.source_text[:200] + "..." if len(e.source_text) > 200 else e.source_text
                } for e in entities
            ],
            "relationships": [
                {
                    "source": r.source_entity,
                    "target": r.target_entity,
                    "type": r.relationship_type,
                    "description": r.description,
                    "keywords": r.keywords,
                    "strength": r.strength,
                    "source_text": r.source_text[:200] + "..." if len(r.source_text) > 200 else r.source_text
                } for r in relationships
            ],
            "content_keywords": content_keywords,
            "entity_statistics": entity_stats,
            "graph_structure": {
                "total_memory_units": len(graph.semantic_map.memory_units),
                "total_memory_spaces": len(graph.semantic_map.memory_spaces),
                "networkx_nodes": graph.nx_graph.number_of_nodes(),
                "networkx_edges": graph.nx_graph.number_of_edges(),
                "faiss_vectors": graph.semantic_map.faiss_index.ntotal if graph.semantic_map.faiss_index else 0
            }
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = output_dir / f"{sample_id}_extraction_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(extraction_results, f, ensure_ascii=False, indent=2, default=str)
        print(f"âœ… æŠ½å–ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # 13. å¯¼å‡ºå®ä½“å’Œå…³ç³»çš„è¯¦ç»†ä¿¡æ¯
        self._export_detailed_analysis(graph, output_dir, sample_id)
        
        print(f"\nğŸ‰ æ ·æœ¬ {sample_id} çš„å®ä½“å…³ç³»æŠ½å–å®Œæˆ!")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
        return {
            "success": True,
            "sample_id": sample_id,
            "output_directory": output_dir,
            "graph": graph,
            "extraction_results": extraction_results,
            "entities_count": len(entities),
            "relationships_count": len(relationships),
            "keywords_count": len(content_keywords)
        }
    
    def _export_detailed_analysis(self, graph: SemanticGraph, output_dir: Path, sample_id: str):
        """å¯¼å‡ºè¯¦ç»†çš„å®ä½“å’Œå…³ç³»åˆ†æ"""
        
        analysis = {
            "sample_id": sample_id,
            "analysis_timestamp": str(datetime.now()),
            "entity_analysis": {},
            "relationship_analysis": {},
            "network_analysis": {}
        }
        
        # å®ä½“åˆ†æ
        entity_space = graph.semantic_map.get_memory_space("extracted_entities")
        if entity_space:
            entity_details = []
            entity_type_counts = {}
            
            for uid in entity_space.get_memory_uids():
                unit = graph.get_unit(uid)
                if unit:
                    entity_type = unit.raw_data.get('entity_type', 'Unknown')
                    entity_details.append({
                        "uid": uid,
                        "name": unit.raw_data.get('entity_name', 'Unknown'),
                        "type": entity_type,
                        "description": unit.raw_data.get('description', ''),
                        "confidence": unit.raw_data.get('confidence', 0.0)
                    })
                    entity_type_counts[entity_type] = entity_type_counts.get(entity_type, 0) + 1
            
            analysis["entity_analysis"] = {
                "total_entities": len(entity_details),
                "entity_type_distribution": entity_type_counts,
                "entities": entity_details
            }
        
        # å…³ç³»åˆ†æ
        relationship_details = []
        relationship_type_counts = {}
        
        for source, target, data in graph.nx_graph.edges(data=True):
            if data.get("source_unit_id"):  # åªåˆ†ææŠ½å–å‡ºçš„å…³ç³»
                rel_type = data.get("type", "UNKNOWN")
                
                # è·å–å®ä½“åç§°
                source_unit = graph.get_unit(source)
                target_unit = graph.get_unit(target)
                source_name = source_unit.raw_data.get('entity_name', source) if source_unit else source
                target_name = target_unit.raw_data.get('entity_name', target) if target_unit else target
                
                relationship_details.append({
                    "source_uid": source,
                    "target_uid": target,
                    "source_name": source_name,
                    "target_name": target_name,
                    "relationship_type": rel_type,
                    "description": data.get("description", ""),
                    "strength": data.get("strength", 0.0)
                })
                
                relationship_type_counts[rel_type] = relationship_type_counts.get(rel_type, 0) + 1
        
        analysis["relationship_analysis"] = {
            "total_relationships": len(relationship_details),
            "relationship_type_distribution": relationship_type_counts,
            "relationships": relationship_details
        }
        
        # ç½‘ç»œåˆ†æ
        analysis["network_analysis"] = {
            "graph_density": nx.density(graph.nx_graph),
            "number_of_nodes": graph.nx_graph.number_of_nodes(),
            "number_of_edges": graph.nx_graph.number_of_edges(),
            "is_connected": nx.is_connected(graph.nx_graph.to_undirected()),
            "number_of_connected_components": nx.number_connected_components(graph.nx_graph.to_undirected())
        }
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = output_dir / f"{sample_id}_detailed_analysis.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… è¯¦ç»†åˆ†æå·²ä¿å­˜åˆ°: {analysis_file}")
        
        # ç”Ÿæˆç®€è¦æŠ¥å‘Š
        report_lines = [
            f"=== {sample_id} å®ä½“å…³ç³»æŠ½å–æŠ¥å‘Š ===",
            f"å¤„ç†æ—¶é—´: {analysis['analysis_timestamp']}",
            f"",
            f"ğŸ“Š å®ä½“ç»Ÿè®¡:",
            f"  - æ€»å®ä½“æ•°: {analysis['entity_analysis'].get('total_entities', 0)}",
        ]
        
        for entity_type, count in analysis['entity_analysis'].get('entity_type_distribution', {}).items():
            report_lines.append(f"  - {entity_type}: {count} ä¸ª")
        
        report_lines.extend([
            f"",
            f"ğŸ”— å…³ç³»ç»Ÿè®¡:",
            f"  - æ€»å…³ç³»æ•°: {analysis['relationship_analysis'].get('total_relationships', 0)}",
        ])
        
        for rel_type, count in analysis['relationship_analysis'].get('relationship_type_distribution', {}).items():
            report_lines.append(f"  - {rel_type}: {count} ä¸ª")
        
        report_lines.extend([
            f"",
            f"ğŸ•¸ï¸ ç½‘ç»œç‰¹å¾:",
            f"  - èŠ‚ç‚¹æ•°: {analysis['network_analysis']['number_of_nodes']}",
            f"  - è¾¹æ•°: {analysis['network_analysis']['number_of_edges']}",
            f"  - å›¾å¯†åº¦: {analysis['network_analysis']['graph_density']:.4f}",
            f"  - è¿é€šæ€§: {'æ˜¯' if analysis['network_analysis']['is_connected'] else 'å¦'}",
            f"  - è¿é€šç»„ä»¶æ•°: {analysis['network_analysis']['number_of_connected_components']}",
        ])
        
        report_content = "\n".join(report_lines)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = output_dir / f"{sample_id}_summary_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        print("\n" + report_content)
    
    def _setup_logging(self, output_dir: Path):
        """è®¾ç½®æ—¥å¿—"""
        log_file = output_dir / "extraction.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
    
    def batch_extract_samples(self, 
                            sample_ids: list = None, 
                            use_chunking: bool = True,
                            save_intermediate: bool = False) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªæ ·æœ¬çš„å®ä½“å…³ç³»æŠ½å–
        
        Args:
            sample_ids: è¦å¤„ç†çš„æ ·æœ¬IDåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰æ ·æœ¬
            use_chunking: æ˜¯å¦ä½¿ç”¨æ–‡æœ¬åˆ†å—å¤„ç†
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
        """
        
        if sample_ids is None:
            sample_ids = self.list_available_samples()
        
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡å®ä½“å…³ç³»æŠ½å–ï¼Œå…± {len(sample_ids)} ä¸ªæ ·æœ¬")
        
        batch_results = {
            "processed_samples": [],
            "failed_samples": [],
            "total_entities": 0,
            "total_relationships": 0,
            "total_keywords": 0,
            "start_time": str(datetime.now())
        }
        
        for i, sample_id in enumerate(sample_ids):
            print(f"\n--- å¤„ç†æ ·æœ¬ {i+1}/{len(sample_ids)}: {sample_id} ---")
            
            try:
                result = self.extract_entities_and_relations_for_sample(
                    sample_id=sample_id,
                    use_chunking=use_chunking,
                    save_intermediate=save_intermediate
                )
                
                if result.get("success"):
                    batch_results["processed_samples"].append({
                        "sample_id": sample_id,
                        "entities_count": result.get("entities_count", 0),
                        "relationships_count": result.get("relationships_count", 0),
                        "keywords_count": result.get("keywords_count", 0),
                        "output_directory": str(result.get("output_directory", ""))
                    })
                    
                    batch_results["total_entities"] += result.get("entities_count", 0)
                    batch_results["total_relationships"] += result.get("relationships_count", 0)
                    batch_results["total_keywords"] += result.get("keywords_count", 0)
                    
                    print(f"âœ… æ ·æœ¬ {sample_id} å¤„ç†å®Œæˆ")
                else:
                    batch_results["failed_samples"].append({
                        "sample_id": sample_id,
                        "error": result.get("error", "Unknown error")
                    })
                    print(f"âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥")
                
            except Exception as e:
                print(f"âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
                batch_results["failed_samples"].append({
                    "sample_id": sample_id,
                    "error": str(e)
                })
                logging.error(f"æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥", exc_info=True)
        
        batch_results["end_time"] = str(datetime.now())
        
        # ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ
        batch_summary_file = self.output_base_dir / f"batch_extraction_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(batch_summary_file, 'w', encoding='utf-8') as f:
            json.dump(batch_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸŠ æ‰¹é‡å®ä½“å…³ç³»æŠ½å–å®Œæˆ!")
        print(f"âœ… æˆåŠŸå¤„ç†: {len(batch_results['processed_samples'])} ä¸ªæ ·æœ¬")
        print(f"âŒ å¤±è´¥: {len(batch_results['failed_samples'])} ä¸ªæ ·æœ¬")
        print(f"ğŸ“Š æ€»å®ä½“æ•°: {batch_results['total_entities']}")
        print(f"ğŸ“Š æ€»å…³ç³»æ•°: {batch_results['total_relationships']}")
        print(f"ğŸ“Š æ€»å…³é”®è¯æ•°: {batch_results['total_keywords']}")
        print(f"ğŸ“ æ‰¹é‡æ‘˜è¦å·²ä¿å­˜åˆ°: {batch_summary_file}")
        
        return batch_results

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    import networkx as nx  # æ·»åŠ è¿™ä¸ªå¯¼å…¥
    
    parser = argparse.ArgumentParser(description="LoCoMoæ•°æ®é›†å®ä½“å…³ç³»æŠ½å–å™¨")
    parser.add_argument("--dataset", required=True, help="LoCoMoæ•°æ®é›†è·¯å¾„")
    parser.add_argument("--sample-id", help="è¦å¤„ç†çš„æ ·æœ¬ID")
    parser.add_argument("--list-samples", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ ·æœ¬ID")
    parser.add_argument("--batch", nargs="+", help="æ‰¹é‡å¤„ç†æŒ‡å®šçš„æ ·æœ¬ID")
    parser.add_argument("--batch-all", action="store_true", help="æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬")
    parser.add_argument("--use-chunking", action="store_true", default=True, help="ä½¿ç”¨æ–‡æœ¬åˆ†å—å¤„ç†")
    parser.add_argument("--no-chunking", action="store_true", help="ä¸ä½¿ç”¨æ–‡æœ¬åˆ†å—å¤„ç†")
    parser.add_argument("--save-intermediate", action="store_true", help="ä¿å­˜ä¸­é—´ç»“æœ")
    parser.add_argument("--output", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–å¤„ç†å™¨
        extractor = LoCoMoEntityExtractor(
            dataset_path=args.dataset,
            output_base_dir=args.output
        )
        
        # ç¡®å®šæ˜¯å¦ä½¿ç”¨åˆ†å—
        use_chunking = not args.no_chunking if args.no_chunking else args.use_chunking
        
        # åˆ—å‡ºæ‰€æœ‰æ ·æœ¬
        if args.list_samples:
            samples = extractor.list_available_samples()
            print(f"æ•°æ®é›†ä¸­åŒ…å« {len(samples)} ä¸ªæ ·æœ¬:")
            for i, sample_id in enumerate(samples, 1):
                info = extractor.get_sample_info(sample_id)
                print(f"  {i:2d}. {sample_id} - {info['speakers'][0]} & {info['speakers'][1]} "
                      f"({info['session_count']} sessions, {info['total_messages']} messages)")
            return 0
        
        # å¤„ç†å•ä¸ªæ ·æœ¬
        if args.sample_id:
            start_time = datetime.now()
            print(f"â° å¼€å§‹æ—¶é—´: {start_time}")
            
            result = extractor.extract_entities_and_relations_for_sample(
                sample_id=args.sample_id,
                use_chunking=use_chunking,
                save_intermediate=args.save_intermediate
            )
            
            end_time = datetime.now()
            if result.get("success"):
                print(f"\nğŸ‰ æ ·æœ¬ {args.sample_id} å¤„ç†å®Œæˆ!")
                print(f"â° å®Œæˆæ—¶é—´: {end_time}")
                print(f"â±ï¸  æ€»è€—æ—¶: {end_time - start_time}")
                print(f"ğŸ“ è¾“å‡ºç›®å½•: {result['output_directory']}")
                print(f"ğŸ“Š æŠ½å–ç»Ÿè®¡: {result['entities_count']} å®ä½“, {result['relationships_count']} å…³ç³»")
            else:
                print(f"âŒ æ ·æœ¬ {args.sample_id} å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
        # æ‰¹é‡å¤„ç†æŒ‡å®šæ ·æœ¬
        elif args.batch:
            extractor.batch_extract_samples(
                sample_ids=args.batch,
                use_chunking=use_chunking,
                save_intermediate=args.save_intermediate
            )
            
        # æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬
        elif args.batch_all:
            extractor.batch_extract_samples(
                sample_ids=None,
                use_chunking=use_chunking,
                save_intermediate=args.save_intermediate
            )
            
        else:
            print("è¯·æŒ‡å®šè¦æ‰§è¡Œçš„æ“ä½œ:")
            print("  --list-samples: åˆ—å‡ºæ‰€æœ‰æ ·æœ¬")
            print("  --sample-id SAMPLE_ID: å¤„ç†å•ä¸ªæ ·æœ¬")
            print("  --batch SAMPLE1 SAMPLE2 ...: æ‰¹é‡å¤„ç†æŒ‡å®šæ ·æœ¬")
            print("  --batch-all: æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬")
            return 1
        
    except Exception as e:
        logging.error(f"æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())

# import sys
# import json
# import logging
# from pathlib import Path
# from typing import Dict, Any, Optional
# from datetime import datetime

# # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
# sys.path.append(str(Path(__file__).parent.parent.parent))

# from dev.semantic_graph import SemanticGraph
# from dev.memory_unit import MemoryUnit
# from benchmark.llm_utils.llm_client import LLMClient
# from benchmark.extractor.entity_relation_extractor import EntityRelationExtractor
# from benchmark.extractor.semantic_graph_integrator import SemanticGraphIntegrator
# from benchmark.task_eval.locomo_test_split import load_dataset, ingest_conversation_history

# class SingleSampleProcessor:
#     """å•ä¸ªæ ·æœ¬å¤„ç†å™¨ - ä¸“é—¨å¤„ç†å•ä¸ªsample_idçš„çŸ¥è¯†å›¾è°±æ„å»º"""
    
#     def __init__(self, dataset_path: str, output_base_dir: str = None):
#         self.dataset_path = Path(dataset_path)
#         self.output_base_dir = Path(output_base_dir) if output_base_dir else Path(__file__).parent.parent / "results"
#         self.raw_data = None
        
#         # åŠ è½½æ•°æ®é›†
#         self._load_dataset()
        
#     def _load_dataset(self):
#         """åŠ è½½æ•°æ®é›†"""
#         logging.info(f"åŠ è½½æ•°æ®é›†: {self.dataset_path}")
#         self.raw_data = load_dataset(self.dataset_path)
#         if not self.raw_data:
#             raise ValueError("æ•°æ®é›†åŠ è½½å¤±è´¥")
#         logging.info(f"æˆåŠŸåŠ è½½ {len(self.raw_data)} ä¸ªæ ·æœ¬")
    
#     def list_available_samples(self) -> list:
#         """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„sample_id"""
#         sample_ids = []
#         for sample in self.raw_data:
#             sample_id = sample.get('sample_id')
#             if sample_id:
#                 sample_ids.append(sample_id)
#         return sample_ids
    
#     def get_sample_info(self, sample_id: str) -> Dict[str, Any]:
#         """è·å–æŒ‡å®šæ ·æœ¬çš„åŸºæœ¬ä¿¡æ¯"""
#         sample = self._find_sample(sample_id)
#         if not sample:
#             return {}
        
#         conversation = sample.get('conversation', {})
#         speaker_a = conversation.get('speaker_a', 'Unknown')
#         speaker_b = conversation.get('speaker_b', 'Unknown')
        
#         # ç»Ÿè®¡ä¼šè¯æ•°é‡
#         session_count = sum(1 for key in conversation.keys() 
#                            if key.startswith('session_') and not key.endswith('_date_time'))
        
#         # ç»Ÿè®¡å®é™…å¯¹è¯å†…å®¹
#         total_messages = 0
#         for key, value in conversation.items():
#             if key.startswith('session_') and not key.endswith('_date_time') and isinstance(value, list):
#                 total_messages += len(value)
        
#         return {
#             "sample_id": sample_id,
#             "speakers": [speaker_a, speaker_b],
#             "session_count": session_count,
#             "total_messages": total_messages,
#             "qa_count": len(sample.get('qa', [])),
#             "event_summary_count": len(sample.get('event_summary', {})),
#             "observation_count": len(sample.get('observation', {})),
#             "session_summary_count": len(sample.get('session_summary', {}))
#         }
    
#     def _find_sample(self, sample_id: str) -> Optional[dict]:
#         """æŸ¥æ‰¾æŒ‡å®šçš„æ ·æœ¬"""
#         for sample in self.raw_data:
#             if sample.get('sample_id') == sample_id:
#                 return sample
#         return None
    
#     def process_single_sample(self, 
#                             sample_id: str, 
#                             max_units: int = 50,
#                             save_intermediate: bool = True) -> Dict[str, Any]:
#         """
#         å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±
        
#         Args:
#             sample_id: æ ·æœ¬ID
#             max_units: æœ€å¤§å¤„ç†å•å…ƒæ•°
#             save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
#         """
        
#         print(f"ğŸš€ å¼€å§‹å¤„ç†æ ·æœ¬: {sample_id}")
        
#         # 1. æŸ¥æ‰¾æ ·æœ¬
#         sample = self._find_sample(sample_id)
#         if not sample:
#             raise ValueError(f"æœªæ‰¾åˆ°sample_idä¸º{sample_id}çš„æ ·æœ¬")
        
#         # 2. åˆ›å»ºè¾“å‡ºç›®å½•
#         output_dir = self.output_base_dir / f"{sample_id}_knowledge_graph_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
#         output_dir.mkdir(parents=True, exist_ok=True)
#         self._setup_logging(output_dir)
        
#         # 3. è·å–æ ·æœ¬ä¿¡æ¯
#         sample_info = self.get_sample_info(sample_id)
#         print(f"ğŸ“Š æ ·æœ¬ä¿¡æ¯: {sample_info}")
        
#         # 4. åˆå§‹åŒ–è¯­ä¹‰å›¾
#         print("\n=== åˆå§‹åŒ–è¯­ä¹‰å›¾ ===")
#         graph = SemanticGraph()
        
#         # 5. æ³¨å…¥å¯¹è¯æ•°æ®
#         print("=== æ³¨å…¥å¯¹è¯æ•°æ®åˆ°è¯­ä¹‰å›¾ ===")
#         total_messages = ingest_conversation_history(graph, [sample])
#         print(f"âœ… æˆåŠŸæ³¨å…¥ {total_messages} ä¸ªå¯¹è¯æ¶ˆæ¯")
        
#         # ä¿å­˜åŸå§‹å¯¹è¯å›¾
#         if save_intermediate:
#             original_graph_path = output_dir / "01_original_conversations_graph"
#             graph.save_graph(str(original_graph_path))
#             print(f"ğŸ’¾ åŸå§‹å¯¹è¯å›¾å·²ä¿å­˜åˆ°: {original_graph_path}")
        
#         # 6. æ„å»ºåˆå§‹ç´¢å¼•
#         print("=== æ„å»ºè¯­ä¹‰å›¾ç´¢å¼• ===")
#         graph.build_semantic_map_index()
        
#         # 7. æ˜¾ç¤ºå›¾è°±åŸºæœ¬ä¿¡æ¯
#         print("\n=== è¯­ä¹‰å›¾åŸºæœ¬ä¿¡æ¯ ===")
#         graph.display_graph_summary()
        
#         # 8. å®ä½“å…³ç³»æŠ½å–
#         print(f"\n=== å¼€å§‹å®ä½“å…³ç³»æŠ½å–ï¼ˆæœ€å¤šå¤„ç†{max_units}ä¸ªå•å…ƒï¼‰===")
#         integrator = SemanticGraphIntegrator(graph)
        
#         # å®šä¹‰è¿‡æ»¤å™¨ï¼šåªå¤„ç†å½“å‰æ ·æœ¬çš„å¯¹è¯å†…å®¹
#         def sample_dialog_filter(unit):
#             return (unit.metadata.get('data_source') == 'locomo_dialog' 
#                     and unit.metadata.get('conversation_id') == sample_id
#                     and not unit.metadata.get('entities_extracted', False))
        
#         # æ‰¹é‡æŠ½å–å®ä½“å…³ç³»
#         extraction_results = integrator.batch_extract_entities_from_space(
#             space_name="locomo_dialogs",
#             max_units=max_units,
#             unit_filter=sample_dialog_filter
#         )
        
#         print(f"âœ… å®ä½“å…³ç³»æŠ½å–å®Œæˆ!")
#         print(f"   å¤„ç†å•å…ƒæ•°: {extraction_results.get('processed', 0)}")
#         print(f"   è·³è¿‡å•å…ƒæ•°: {extraction_results.get('skipped', 0)}")
#         print(f"   å¤±è´¥å•å…ƒæ•°: {extraction_results.get('failed', 0)}")
#         print(f"   æ€»æŠ½å–å®ä½“æ•°: {extraction_results.get('total_entities', 0)}")
#         print(f"   æ€»æŠ½å–å…³ç³»æ•°: {extraction_results.get('total_relationships', 0)}")
        
#         # 9. è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
#         print("\n=== ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯ ===")
#         entity_stats = integrator.get_entity_statistics()
#         print(f"å®ä½“ç±»å‹åˆ†å¸ƒ: {entity_stats.get('entity_types', {})}")
#         print(f"å…³ç³»ç±»å‹åˆ†å¸ƒ: {entity_stats.get('relationship_types', {})}")
        
#         # 10. ä¿å­˜æœ€ç»ˆçŸ¥è¯†å›¾è°±
#         print("\n=== ä¿å­˜çŸ¥è¯†å›¾è°± ===")
#         final_graph_path = output_dir / f"{sample_id}_final_knowledge_graph"
#         graph.save_graph(str(final_graph_path))
#         print(f"âœ… çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ°: {final_graph_path}")
        
#         # 11. ä¿å­˜è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
#         final_summary = {
#             "sample_info": sample_info,
#             "processing_info": {
#                 "timestamp": str(datetime.now()),
#                 "max_units_processed": max_units,
#                 "total_messages_ingested": total_messages
#             },
#             "extraction_results": extraction_results,
#             "entity_statistics": entity_stats,
#             "graph_structure": {
#                 "total_memory_units": len(graph.semantic_map.memory_units),
#                 "total_memory_spaces": len(graph.semantic_map.memory_spaces),
#                 "networkx_nodes": graph.nx_graph.number_of_nodes(),
#                 "networkx_edges": graph.nx_graph.number_of_edges(),
#                 "faiss_vectors": graph.semantic_map.faiss_index.ntotal if graph.semantic_map.faiss_index else 0
#             }
#         }
        
#         summary_file = output_dir / f"{sample_id}_summary.json"
#         with open(summary_file, 'w', encoding='utf-8') as f:
#             json.dump(final_summary, f, ensure_ascii=False, indent=2, default=str)
#         print(f"âœ… å¤„ç†æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
        
#         # 12. å¯¼å‡ºå®ä½“å’Œå…³ç³»ç¤ºä¾‹
#         print("\n=== å¯¼å‡ºå®ä½“å’Œå…³ç³»ç¤ºä¾‹ ===")
#         self._export_samples(graph, output_dir, sample_id, num_samples=10)
        
#         # 13. æ˜¾ç¤ºæœ€ç»ˆå›¾è°±æ‘˜è¦
#         print("\n=== æœ€ç»ˆçŸ¥è¯†å›¾è°±æ‘˜è¦ ===")
#         graph.display_graph_summary()
        
#         return {
#             "sample_id": sample_id,
#             "graph": graph,
#             "extraction_results": extraction_results,
#             "entity_statistics": entity_stats,
#             "output_directory": output_dir,
#             "summary": final_summary
#         }
    
#     def _export_samples(self, graph: SemanticGraph, output_dir: Path, sample_id: str, num_samples: int = 10):
#         """å¯¼å‡ºå®ä½“å’Œå…³ç³»ç¤ºä¾‹"""
        
#         # å¯¼å‡ºå®ä½“ç¤ºä¾‹
#         entities_sample = []
#         entity_space = graph.semantic_map.get_memory_space("extracted_entities")
#         if entity_space:
#             entity_uids = list(entity_space.get_memory_uids())[:num_samples]
#             for uid in entity_uids:
#                 unit = graph.get_unit(uid)
#                 if unit:
#                     entities_sample.append({
#                         "uid": uid,
#                         "name": unit.raw_data.get('entity_name', 'Unknown'),
#                         "type": unit.raw_data.get('entity_type', 'Unknown'),
#                         "description": unit.raw_data.get('description', ''),
#                         "confidence": unit.raw_data.get('confidence', 0.0),
#                         "source_unit_id": unit.metadata.get('source_unit_id', '')
#                     })
        
#         # å¯¼å‡ºå…³ç³»ç¤ºä¾‹
#         relationships_sample = []
#         relationship_count = 0
#         for source, target, data in graph.nx_graph.edges(data=True):
#             if relationship_count >= num_samples:
#                 break
#             if data.get("source_unit_id"):  # åªå¯¼å‡ºæŠ½å–å‡ºçš„å…³ç³»
#                 source_unit = graph.get_unit(source)
#                 target_unit = graph.get_unit(target)
                
#                 relationships_sample.append({
#                     "source_uid": source,
#                     "target_uid": target,
#                     "source_name": source_unit.raw_data.get('entity_name', source) if source_unit else source,
#                     "target_name": target_unit.raw_data.get('entity_name', target) if target_unit else target,
#                     "relationship_type": data.get("type", "UNKNOWN"),
#                     "description": data.get("description", ""),
#                     "strength": data.get("strength", 0.0),
#                     "source_unit_id": data.get("source_unit_id", "")
#                 })
#                 relationship_count += 1
        
#         # ä¿å­˜ç¤ºä¾‹
#         samples = {
#             "sample_id": sample_id,
#             "entities_sample": entities_sample,
#             "relationships_sample": relationships_sample,
#             "total_entities": len(entity_space.get_memory_uids()) if entity_space else 0,
#             "total_relationships": graph.nx_graph.number_of_edges()
#         }
        
#         samples_file = output_dir / f"{sample_id}_entities_relationships_samples.json"
#         with open(samples_file, 'w', encoding='utf-8') as f:
#             json.dump(samples, f, ensure_ascii=False, indent=2)
        
#         print(f"âœ… å®ä½“å…³ç³»ç¤ºä¾‹å·²ä¿å­˜åˆ°: {samples_file}")
#         print(f"ğŸ“Š å®ä½“ç¤ºä¾‹æ•°: {len(entities_sample)}")
#         print(f"ğŸ“Š å…³ç³»ç¤ºä¾‹æ•°: {len(relationships_sample)}")
    
#     def _setup_logging(self, output_dir: Path):
#         """è®¾ç½®æ—¥å¿—"""
#         log_file = output_dir / "processing.log"
#         logging.basicConfig(
#             level=logging.INFO,
#             format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
#             handlers=[
#                 logging.FileHandler(log_file),
#                 logging.StreamHandler()
#             ]
#         )
    
#     def batch_process_samples(self, 
#                             sample_ids: list = None, 
#                             max_units_per_sample: int = 50) -> Dict[str, Any]:
#         """
#         æ‰¹é‡å¤„ç†å¤šä¸ªæ ·æœ¬
        
#         Args:
#             sample_ids: è¦å¤„ç†çš„æ ·æœ¬IDåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰æ ·æœ¬
#             max_units_per_sample: æ¯ä¸ªæ ·æœ¬æœ€å¤§å¤„ç†å•å…ƒæ•°
#         """
        
#         if sample_ids is None:
#             sample_ids = self.list_available_samples()
        
#         print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(sample_ids)} ä¸ªæ ·æœ¬")
        
#         batch_results = {
#             "processed_samples": [],
#             "failed_samples": [],
#             "total_entities": 0,
#             "total_relationships": 0,
#             "start_time": str(datetime.now())
#         }
        
#         for i, sample_id in enumerate(sample_ids):
#             print(f"\n--- å¤„ç†æ ·æœ¬ {i+1}/{len(sample_ids)}: {sample_id} ---")
            
#             try:
#                 result = self.process_single_sample(
#                     sample_id=sample_id,
#                     max_units=max_units_per_sample,
#                     save_intermediate=False  # æ‰¹é‡å¤„ç†æ—¶ä¸ä¿å­˜ä¸­é—´ç»“æœ
#                 )
                
#                 batch_results["processed_samples"].append({
#                     "sample_id": sample_id,
#                     "entities": result["extraction_results"].get("total_entities", 0),
#                     "relationships": result["extraction_results"].get("total_relationships", 0),
#                     "output_directory": str(result["output_directory"])
#                 })
                
#                 batch_results["total_entities"] += result["extraction_results"].get("total_entities", 0)
#                 batch_results["total_relationships"] += result["extraction_results"].get("total_relationships", 0)
                
#                 print(f"âœ… æ ·æœ¬ {sample_id} å¤„ç†å®Œæˆ")
                
#             except Exception as e:
#                 print(f"âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}")
#                 batch_results["failed_samples"].append({
#                     "sample_id": sample_id,
#                     "error": str(e)
#                 })
#                 logging.error(f"æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥", exc_info=True)
        
#         batch_results["end_time"] = str(datetime.now())
        
#         # ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ
#         batch_summary_file = self.output_base_dir / f"batch_processing_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
#         with open(batch_summary_file, 'w', encoding='utf-8') as f:
#             json.dump(batch_results, f, ensure_ascii=False, indent=2, default=str)
        
#         print(f"\nğŸŠ æ‰¹é‡å¤„ç†å®Œæˆ!")
#         print(f"âœ… æˆåŠŸå¤„ç†: {len(batch_results['processed_samples'])} ä¸ªæ ·æœ¬")
#         print(f"âŒ å¤±è´¥: {len(batch_results['failed_samples'])} ä¸ªæ ·æœ¬")
#         print(f"ğŸ“Š æ€»å®ä½“æ•°: {batch_results['total_entities']}")
#         print(f"ğŸ“Š æ€»å…³ç³»æ•°: {batch_results['total_relationships']}")
#         print(f"ğŸ“ æ‰¹é‡æ‘˜è¦å·²ä¿å­˜åˆ°: {batch_summary_file}")
        
#         return batch_results

# def main():
#     """ä¸»å‡½æ•°"""
#     import argparse
    
#     parser = argparse.ArgumentParser(description="å•ä¸ªæ ·æœ¬çŸ¥è¯†å›¾è°±å¤„ç†å™¨")
#     parser.add_argument("--dataset", required=True, help="LoCoMoæ•°æ®é›†è·¯å¾„")
#     parser.add_argument("--sample-id", help="è¦å¤„ç†çš„æ ·æœ¬ID")
#     parser.add_argument("--list-samples", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ ·æœ¬ID")
#     parser.add_argument("--batch", nargs="+", help="æ‰¹é‡å¤„ç†æŒ‡å®šçš„æ ·æœ¬ID")
#     parser.add_argument("--batch-all", action="store_true", help="æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬")
#     parser.add_argument("--max-units", type=int, default=50, help="æ¯ä¸ªæ ·æœ¬æœ€å¤§å¤„ç†å•å…ƒæ•°")
#     parser.add_argument("--output", help="è¾“å‡ºç›®å½•")
    
#     args = parser.parse_args()
    
#     try:
#         # åˆå§‹åŒ–å¤„ç†å™¨
#         processor = SingleSampleProcessor(
#             dataset_path=args.dataset,
#             output_base_dir=args.output
#         )
        
#         # åˆ—å‡ºæ‰€æœ‰æ ·æœ¬
#         if args.list_samples:
#             samples = processor.list_available_samples()
#             print(f"æ•°æ®é›†ä¸­åŒ…å« {len(samples)} ä¸ªæ ·æœ¬:")
#             for i, sample_id in enumerate(samples, 1):
#                 info = processor.get_sample_info(sample_id)
#                 print(f"  {i:2d}. {sample_id} - {info['speakers'][0]} & {info['speakers'][1]} "
#                       f"({info['session_count']} sessions, {info['total_messages']} messages)")
#             return 0
        
#         # å¤„ç†å•ä¸ªæ ·æœ¬
#         if args.sample_id:
#             start_time = datetime.now()
#             print(f"â° å¼€å§‹æ—¶é—´: {start_time}")
            
#             result = processor.process_single_sample(
#                 sample_id=args.sample_id,
#                 max_units=args.max_units
#             )
            
#             end_time = datetime.now()
#             print(f"\nğŸ‰ æ ·æœ¬ {args.sample_id} å¤„ç†å®Œæˆ!")
#             print(f"â° å®Œæˆæ—¶é—´: {end_time}")
#             print(f"â±ï¸  æ€»è€—æ—¶: {end_time - start_time}")
#             print(f"ğŸ“ è¾“å‡ºç›®å½•: {result['output_directory']}")
            
#         # æ‰¹é‡å¤„ç†æŒ‡å®šæ ·æœ¬
#         elif args.batch:
#             processor.batch_process_samples(
#                 sample_ids=args.batch,
#                 max_units_per_sample=args.max_units
#             )
            
#         # æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬
#         elif args.batch_all:
#             processor.batch_process_samples(
#                 sample_ids=None,
#                 max_units_per_sample=args.max_units
#             )
            
#         else:
#             print("è¯·æŒ‡å®šè¦æ‰§è¡Œçš„æ“ä½œ:")
#             print("  --list-samples: åˆ—å‡ºæ‰€æœ‰æ ·æœ¬")
#             print("  --sample-id SAMPLE_ID: å¤„ç†å•ä¸ªæ ·æœ¬")
#             print("  --batch SAMPLE1 SAMPLE2 ...: æ‰¹é‡å¤„ç†æŒ‡å®šæ ·æœ¬")
#             print("  --batch-all: æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬")
#             return 1
        
#     except Exception as e:
#         logging.error(f"æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
#         print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
#         return 1
    
#     return 0

# if __name__ == "__main__":
#     exit(main())
