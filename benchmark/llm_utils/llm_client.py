import os
import logging
from typing import List, Dict, Any, Optional
from openai import OpenAI
import re

# æ·»åŠ tiktokenæ”¯æŒ
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktokenæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•ä¼°ç®—æ–¹æ³•ã€‚å»ºè®®å®‰è£…: pip install tiktoken")


class LLMClient:
    """ç®€åŒ–ç‰ˆå¤§æ¨¡å‹å®¢æˆ·ç«¯æ¥å£ï¼Œä¸“æ³¨äºDeepSeekä¼˜åŒ–"""
    
    # æ›´æ–°çš„æ¨¡å‹é…ç½®
    MODEL_CONFIGS = {
        # DeepSeek ç³»åˆ—
        "deepseek-chat": {
            "context_length": 65536,    # 64Kä¸Šä¸‹æ–‡
            "max_output": 8192,             # æœ€å¤§è¾“å‡º8K
            "default_output": 4096,         # é»˜è®¤è¾“å‡º4K
            "encoding": "cl100k_base"
        },
        "deepseek-reasoner": {
            "context_length": 65536,    # 64Kä¸Šä¸‹æ–‡
            "max_output": 65536,        # æœ€å¤§è¾“å‡º64K
            "default_output": 32768,    # é»˜è®¤è¾“å‡º32K
            "encoding": "cl100k_base"
        },
        
        # OpenAI GPT ç³»åˆ—ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        "gpt-3.5-turbo": {
            "context_length": 4096,
            "max_output": 1024,
            "default_output": 512,
            "encoding": "cl100k_base"
        },
        "gpt-4": {
            "context_length": 8192,
            "max_output": 2048,
            "default_output": 1024,
            "encoding": "cl100k_base"
        },
        "gpt-4o-mini": {
            "context_length": 128000,
            "max_output": 4096,
            "default_output": 2048,
            "encoding": "o200k_base"
        },
        "gpt-4o": {
            "context_length": 128000,
            "max_output": 4096,
            "default_output": 2048,
            "encoding": "o200k_base"
        },
    }
    
    def __init__(self, 
                 model_name: str = "deepseek-chat", 
                 api_key: Optional[str] = None,
                 max_context_ratio: float = 0.85):  # ä¸ºè¾“å‡ºé¢„ç•™15%ç©ºé—´
        """
        åˆå§‹åŒ–ç®€åŒ–ç‰ˆLLMå®¢æˆ·ç«¯
        
        Args:
            model_name: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥
            max_context_ratio: ä¸Šä¸‹æ–‡å æ€»é•¿åº¦çš„æ¯”ä¾‹
        """
        self.model_name = model_name
        self.api_key = api_key or self._get_api_key(model_name)
        self.max_context_ratio = max_context_ratio
        
        # è·å–æ¨¡å‹é…ç½®
        self.model_config = self.MODEL_CONFIGS.get(model_name, {
            "context_length": 4096,
            "max_output": 1024,
            "default_output": 512,
            "encoding": "cl100k_base"
        })
        
        self.context_length = self.model_config["context_length"]
        self.max_output_tokens = self.model_config["max_output"]
        self.default_output_tokens = self.model_config["default_output"]
        self.max_context_tokens = int(self.context_length * max_context_ratio)
        
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–tokenizer
        self.tokenizer = self._init_tokenizer()
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = self._init_client()
        
        self.logger.info(f"ğŸ¤– åˆå§‹åŒ–LLMå®¢æˆ·ç«¯: {model_name}")
        self.logger.info(f"ğŸ“ ä¸Šä¸‹æ–‡é•¿åº¦: {self.context_length:,}, å¯ç”¨: {self.max_context_tokens:,}")
        self.logger.info(f"ğŸ“¤ æœ€å¤§è¾“å‡º: {self.max_output_tokens:,}, é»˜è®¤: {self.default_output_tokens:,}")
        self.logger.info(f"ğŸ”§ Tokenè®¡ç®—: {'tiktoken' if self.tokenizer else 'ç®€å•ä¼°ç®—'}")
    
    def _get_api_key(self, model_name: str) -> Optional[str]:
        """è·å–APIå¯†é’¥"""
        if "deepseek" in model_name.lower():
            return os.getenv("DEEPSEEK_API_KEY")
        elif "gpt" in model_name.lower():
            return os.getenv("OPENAI_API_KEY")
        else:
            return os.getenv("LLM_API_KEY")
    
    def _init_tokenizer(self):
        """åˆå§‹åŒ–tokenizer"""
        if not TIKTOKEN_AVAILABLE:
            return None
        
        try:
            encoding_name = self.model_config["encoding"]
            tokenizer = tiktoken.get_encoding(encoding_name)
            self.logger.debug(f"ä½¿ç”¨tiktokenç¼–ç å™¨: {encoding_name}")
            return tokenizer
        except Exception as e:
            self.logger.warning(f"tiktokenåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•ä¼°ç®—")
            return None
    
    def _init_client(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        if "deepseek" in self.model_name.lower():
            return OpenAI(
                api_key=self.api_key,
                base_url="https://api.deepseek.com"
            )
        elif "gpt" in self.model_name.lower():
            return OpenAI(api_key=self.api_key)
        else:
            return OpenAI(api_key=self.api_key)
    
    def count_tokens(self, text: str) -> int:
        """
        è®¡ç®—tokenæ•°é‡
        ä¼˜å…ˆä½¿ç”¨tiktokenï¼Œfallbackåˆ°ç®€åŒ–ä¼°ç®—
        """
        if self.tokenizer:
            try:
                return len(self.tokenizer.encode(text))
            except Exception:
                pass
        
        # ç®€åŒ–çš„tokenä¼°ç®—ï¼šæŒ‰æ‚¨çš„è¦æ±‚
        # 1 ä¸ªè‹±æ–‡å­—ç¬¦ â‰ˆ 0.3 ä¸ª token
        # 1 ä¸ªä¸­æ–‡å­—ç¬¦ â‰ˆ 0.6 ä¸ª token
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        other_chars = len(text) - chinese_chars - english_chars
        
        estimated_tokens = int(
            chinese_chars * 0.6 +      # ä¸­æ–‡å­—ç¬¦
            english_chars * 0.3 +      # è‹±æ–‡å­—ç¬¦
            other_chars * 0.4          # å…¶ä»–å­—ç¬¦ï¼ˆæ•°å­—ã€æ ‡ç‚¹ç­‰ï¼‰
        )
        
        return max(estimated_tokens, 1)  # è‡³å°‘1ä¸ªtoken
    
    def truncate_context(self, context_text: str, max_tokens: Optional[int] = None) -> str:
        """
        ç®€åŒ–çš„ä¸Šä¸‹æ–‡æˆªæ–­
        ä¿ç•™æœ€åçš„å†…å®¹ï¼ˆæœ€æ–°çš„å¯¹è¯é€šå¸¸æœ€é‡è¦ï¼‰
        """
        if max_tokens is None:
            max_tokens = self.max_context_tokens
        
        current_tokens = self.count_tokens(context_text)
        
        if current_tokens <= max_tokens:
            return context_text
        
        # æŒ‰è¡Œæˆªæ–­ï¼Œä»åå¾€å‰ä¿ç•™
        lines = context_text.split('\n')
        selected_lines = []
        current_tokens = 0
        
        for line in reversed(lines):
            line_tokens = self.count_tokens(line + '\n')
            if current_tokens + line_tokens <= max_tokens:
                selected_lines.insert(0, line)
                current_tokens += line_tokens
            else:
                break
        
        result = '\n'.join(selected_lines)
        final_tokens = self.count_tokens(result)
        
        if final_tokens != current_tokens:  # åªåœ¨æˆªæ–­æ—¶è®°å½•
            self.logger.info(f"ğŸ“ ä¸Šä¸‹æ–‡æˆªæ–­: {self.count_tokens(context_text)} -> {final_tokens} tokens")
        
        return result
    
    def get_context_info(self) -> Dict[str, Any]:
        """è·å–LLMä¸Šä¸‹æ–‡é…ç½®ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "context_length": self.context_length,
            "max_output_tokens": self.max_output_tokens,
            "default_output_tokens": self.default_output_tokens,
            "max_context_tokens": self.max_context_tokens,
            "tokenizer_available": self.tokenizer is not None,
            "encoding": self.model_config.get("encoding", "unknown")
        }

    def generate_answer(self, 
                   prompt: str, 
                   max_tokens: Optional[int] = None,
                   temperature: float = 0.1,
                   generate_strategy: str = "default",
                   **kwargs) -> str:
        """
        ç”Ÿæˆç­”æ¡ˆ - ç®€åŒ–ç‰ˆæœ¬ï¼Œç§»é™¤ä¸æ”¯æŒçš„å‚æ•°
        
        Args:
            prompt: å®Œæ•´æç¤ºè¯
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼ˆNoneæ—¶ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            generate_strategy: ç”Ÿæˆç­–ç•¥ ("default", "max")
            **kwargs: å…¶ä»–DeepSeek APIå‚æ•°
            
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        
        # è®¾ç½®è¾“å‡ºtokenæ•°
        if max_tokens is None:
            if generate_strategy == "max":
                max_tokens = self.max_output_tokens
            else:
                # é»˜è®¤ä½¿ç”¨æ¨¡å‹çš„é»˜è®¤è¾“å‡ºtokenæ•°
                max_tokens = self.default_output_tokens
        else:
            max_tokens = min(max_tokens, self.max_output_tokens)  # ä¸è¶…è¿‡æ¨¡å‹é™åˆ¶
        
        # ç¡®ä¿promptä¸è¶…è¿‡ä¸Šä¸‹æ–‡é™åˆ¶
        prompt_tokens = self.count_tokens(prompt)
        max_prompt_tokens = self.context_length - max_tokens - 100  # é¢„ç•™100ç¼“å†²
        
        if prompt_tokens > max_prompt_tokens:
            self.logger.warning(f"âš ï¸ Promptè¿‡é•¿ ({prompt_tokens} > {max_prompt_tokens})ï¼Œæˆªæ–­ä¸­...")
            prompt = self.truncate_context(prompt, max_prompt_tokens)
            prompt_tokens = self.count_tokens(prompt)
            self.logger.info(f"ğŸ“ Promptæˆªæ–­å: {prompt_tokens} tokens")
        
        try:
            # æ„å»ºè¯·æ±‚å‚æ•°ï¼ˆéµå¾ªDeepSeek APIè§„èŒƒï¼‰
            request_params = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": temperature
            }
            
            # æ·»åŠ å…¶ä»–æ”¯æŒçš„å‚æ•°
            supported_params = ['frequency_penalty', 'presence_penalty', 'top_p', 'stop']
            for param in supported_params:
                if param in kwargs:
                    request_params[param] = kwargs[param]
            
            # å¦‚æœæ˜¯DeepSeekæ¨¡å‹ä¸”æ˜ç¡®è¦æ±‚JSONæ ¼å¼ï¼Œæ·»åŠ response_format
            if "deepseek" in self.model_name.lower() and kwargs.get('json_format', False):
                request_params["response_format"] = {"type": "json_object"}
                # ç¡®ä¿promptä¸­åŒ…å«JSONæŒ‡ä»¤
                if "json" not in prompt.lower():
                    prompt += "\n\nè¯·ä»¥JSONæ ¼å¼å›å¤ã€‚"
                    request_params["messages"] = [{"role": "user", "content": prompt}]
            
            self.logger.debug(f"ğŸš€ å‘é€è¯·æ±‚: {prompt_tokens} tokens -> max {max_tokens} tokens")
            
            response = self.client.chat.completions.create(**request_params)
            answer = response.choices[0].message.content.strip()
            
            # è®°å½•tokenä½¿ç”¨
            if hasattr(response, 'usage') and response.usage:
                usage = response.usage
                self.logger.debug(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥={usage.prompt_tokens}, "
                                f"è¾“å‡º={usage.completion_tokens}, "
                                f"æ€»è®¡={usage.total_tokens}")
            else:
                # ä¼°ç®—è¾“å‡ºtokenæ•°
                estimated_output = self.count_tokens(answer)
                self.logger.debug(f"ğŸ“Š Tokenä¼°ç®—: è¾“å…¥â‰ˆ{prompt_tokens}, è¾“å‡ºâ‰ˆ{estimated_output}")
            
            return answer
            
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"

    def get_context_info(self) -> Dict[str, Any]:
        """è·å–LLMä¸Šä¸‹æ–‡é…ç½®ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "context_length": self.context_length,
            "max_output_tokens": self.max_output_tokens,
            "default_output_tokens": self.default_output_tokens,
            "tokenizer_available": self.tokenizer is not None,
            "encoding": self.model_config.get("encoding", "unknown")
        }
    
    def batch_generate(self, 
                      prompts: List[str], 
                      max_tokens: Optional[int] = None,
                      temperature: float = 0.1,
                      **kwargs) -> List[str]:
        """æ‰¹é‡ç”Ÿæˆç­”æ¡ˆ"""
        results = []
        
        self.logger.info(f"ğŸ”„ æ‰¹é‡ç”Ÿæˆå¼€å§‹: {len(prompts)} ä¸ªè¯·æ±‚")
        
        for i, prompt in enumerate(prompts, 1):
            self.logger.debug(f"å¤„ç† {i}/{len(prompts)}")
            answer = self.generate_answer(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                **kwargs
            )
            results.append(answer)
        
        self.logger.info(f"âœ… æ‰¹é‡ç”Ÿæˆå®Œæˆ")
        return results
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "context_length": self.context_length,
            "max_context_tokens": self.max_context_tokens,
            "max_output_tokens": self.max_output_tokens,
            "default_output_tokens": self.default_output_tokens,
            "max_context_ratio": self.max_context_ratio,
            "tokenizer_available": self.tokenizer is not None,
            "encoding": self.model_config["encoding"]
        }
    
    def analyze_text(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬çš„tokenä¿¡æ¯"""
        total_tokens = self.count_tokens(text)
        
        # å­—ç¬¦ç»Ÿè®¡
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        
        return {
            "total_tokens": total_tokens,
            "character_count": len(text),
            "chinese_chars": chinese_chars,
            "english_chars": english_chars,
            "tokens_per_char": total_tokens / len(text) if text else 0,
            "fits_in_context": total_tokens <= self.max_context_tokens,
            "usage_ratio": total_tokens / self.context_length,
            "can_process": total_tokens <= (self.context_length - self.default_output_tokens)
        }


# ä¾¿æ·å‡½æ•°
def create_deepseek_client(model: str = "deepseek-chat", 
                          api_key: Optional[str] = None) -> LLMClient:
    """åˆ›å»ºDeepSeekå®¢æˆ·ç«¯çš„ä¾¿æ·å‡½æ•°"""
    return LLMClient(model_name=model, api_key=api_key)


def estimate_tokens(text: str) -> int:
    """å¿«é€Ÿä¼°ç®—tokenæ•°çš„ç‹¬ç«‹å‡½æ•°"""
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    other_chars = len(text) - chinese_chars - english_chars
    
    return int(chinese_chars * 0.6 + english_chars * 0.3 + other_chars * 0.4)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•å®¢æˆ·ç«¯
    client = LLMClient("deepseek-chat")
    
    # æµ‹è¯•æ–‡æœ¬åˆ†æ
    test_text = "Hello world! ä½ å¥½ä¸–ç•Œï¼è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚"
    analysis = client.analyze_text(test_text)
    print(f"æ–‡æœ¬åˆ†æ: {analysis}")
    
    # æµ‹è¯•ç”Ÿæˆ
    prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½ã€‚"
    answer = client.generate_answer(prompt, max_tokens=200)
    print(f"ç”Ÿæˆç»“æœ: {answer}")
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    info = client.get_model_info()
    print(f"æ¨¡å‹ä¿¡æ¯: {info}")
