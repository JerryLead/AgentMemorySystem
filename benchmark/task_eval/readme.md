# ä»»åŠ¡è¯„ä¼°å·¥å…· (Task Evaluation Tools)

æœ¬ç›®å½•æä¾›äº†å®Œæ•´çš„è¯­ä¹‰å›¾è°±é—®ç­”è¯„ä¼°æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè¯„æµ‹åŸºäºå¯¹è¯è®°å¿†çš„QAç³»ç»Ÿæ€§èƒ½ã€‚æ”¯æŒç‹¬ç«‹æ ·æœ¬è¯„ä¼°ã€è¯­ä¹‰å›¾è°±æ£€ç´¢ã€å¤šç§è¯„åˆ†æŒ‡æ ‡å’Œè¯¦ç»†çš„ç»“æœåˆ†æã€‚

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

- ğŸ§  **è¯­ä¹‰å›¾è°±QAè¯„ä¼°**ï¼šåŸºäºè¯­ä¹‰å›¾è°±çš„æ™ºèƒ½é—®ç­”è¯„ä¼°
- ğŸ” **å¤šç­–ç•¥æ£€ç´¢**ï¼šæ”¯æŒå•è·³ã€å¤šè·³ã€æ—¶é—´ã€å¼€æ”¾åŸŸã€å¯¹æŠ—æ€§æ£€ç´¢ç­–ç•¥
- ğŸ“Š **å¤šç»´åº¦è¯„åˆ†**ï¼šF1ã€ROUGE-Lã€ç²¾ç¡®åŒ¹é…ã€è¯æ®å¬å›ç­‰å¤šç§æŒ‡æ ‡
- ğŸ¯ **ç‹¬ç«‹æ ·æœ¬è¯„ä¼°**ï¼šæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å»ºç«‹è¯­ä¹‰å›¾è°±ï¼Œé¿å…æ•°æ®å¹²æ‰°
- ğŸ“ˆ **è¯¦ç»†åˆ†ææŠ¥å‘Š**ï¼šæŒ‰ç±»åˆ«ã€æŒ‰é—®é¢˜ã€æŒ‰æ ·æœ¬çš„è¯¦ç»†ç»Ÿè®¡åˆ†æ
- âš¡ **å¿«é€Ÿæµ‹è¯•è„šæœ¬**ï¼šæ”¯æŒå•é—®é¢˜å¿«é€ŸéªŒè¯å’Œè°ƒè¯•
- ğŸ”„ **æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒå¤šæ ·æœ¬ã€å¤šæ¨¡å‹çš„æ‰¹é‡è¯„ä¼°
- ğŸ“ **å¤šæ ¼å¼è¾“å‡º**ï¼šJSONã€CSVã€TXTç­‰å¤šç§æ ¼å¼çš„ç»“æœæ–‡ä»¶

## ğŸ“ ç›®å½•ç»“æ„

```
benchmark/task_eval/
â”œâ”€â”€ evaluation.py                      # æ ¸å¿ƒè¯„åˆ†å‡½æ•°åº“
â”œâ”€â”€ semantic_graph_qa_evaluator.py     # è¯­ä¹‰å›¾è°±QAè¯„ä¼°å™¨
â”œâ”€â”€ IndependentSampleEvaluator.py      # ç‹¬ç«‹æ ·æœ¬è¯„ä¼°å™¨
â”œâ”€â”€ run_detailed_evaluation.py         # è¯¦ç»†è¯„ä¼°æ‰§è¡Œè„šæœ¬
â”œâ”€â”€ run_semantic_evaluation.py         # è¯­ä¹‰å›¾è°±è¯„ä¼°è„šæœ¬
â”œâ”€â”€ test_light_script.py              # å¿«é€Ÿæµ‹è¯•è„šæœ¬
â”œâ”€â”€ locomo_test.py                     # LoCoMoå®Œæ•´æµ‹è¯•ï¼ˆå·²æœ‰ï¼‰
â”œâ”€â”€ locomo_test_split.py              # LoCoMoåˆ†ç»„æµ‹è¯•ï¼ˆå·²æœ‰ï¼‰
â”œâ”€â”€ locomo_test_split.py              # LoCoMoåˆ†å‰²æµ‹è¯•ï¼ˆå·²æœ‰ï¼‰
â”œâ”€â”€ readme.md                         # æœ¬æ–‡æ¡£
â””â”€â”€ results/                          # è¯„ä¼°ç»“æœç›®å½•
    â”œâ”€â”€ detailed/                     # è¯¦ç»†è¯„ä¼°ç»“æœ
    â”œâ”€â”€ simplified/                   # ç®€åŒ–è¯„ä¼°ç»“æœ
    â””â”€â”€ semantic/                     # è¯­ä¹‰å›¾è°±è¯„ä¼°ç»“æœ
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿å·²å®Œæˆå®ä½“å…³ç³»æŠ½å–ï¼š

```bash
# è¿è¡Œå®ä½“å…³ç³»æŠ½å–ï¼ˆå¦‚æœå°šæœªå®Œæˆï¼‰
python benchmark/extractor/locomo_entity_extractor.py \
    --dataset benchmark/dataset/locomo/locomo10.json \
    --batch-all \
    --use-chunking \
    --output benchmark/results/full_extraction

# åˆ¶ä½œæ•°æ®é›†
python benchmark/extractor/dataset_maker.py \
    --results-dir benchmark/results/full_extraction \
    --output-dir benchmark/dataset/locomo/extraction
```

### 2. å¿«é€Ÿæµ‹è¯•å•ä¸ªé—®é¢˜

```bash
cd /home/zyh/code/AgentMemorySystem
python benchmark/task_eval/test_light_script.py
```

é€‰æ‹©æµ‹è¯•ç±»å‹ï¼š
- 1: å•é—®é¢˜æµ‹è¯• - å¿«é€ŸéªŒè¯ç³»ç»ŸåŠŸèƒ½
- 2: ç±»åˆ«ç­–ç•¥æµ‹è¯• - æµ‹è¯•ä¸åŒQAç±»åˆ«çš„å¤„ç†ç­–ç•¥

### 3. ç‹¬ç«‹æ ·æœ¬è¯„ä¼°ï¼ˆæ¨èï¼‰

```bash
# è¯„ä¼°å•ä¸ªæ ·æœ¬
python benchmark/task_eval/IndependentSampleEvaluator.py \
    --model deepseek-chat \
    --samples conv-26 \
    --format comprehensive

# è¯„ä¼°å¤šä¸ªæ ·æœ¬
python benchmark/task_eval/IndependentSampleEvaluator.py \
    --model deepseek-chat \
    --samples conv-26 conv-30 conv-41 \
    --format comprehensive

# è¯¦ç»†è¯„ä¼°è„šæœ¬ï¼ˆæ¨èï¼‰
python benchmark/task_eval/run_detailed_evaluation.py \
    --model deepseek-chat \
    --samples conv-26 conv-30 \
    --output_dir benchmark/task_eval/results/detailed
```

### 4. è¯­ä¹‰å›¾è°±è¯„ä¼°

```bash
python benchmark/task_eval/run_semantic_evaluation.py \
    --model deepseek-chat \
    --conversations conv-26 conv-30 \
    --output_dir benchmark/task_eval/results/semantic
```

## ğŸ“š è¯¦ç»†ä½¿ç”¨æŒ‡å—

### æ ¸å¿ƒè¯„åˆ†å‡½æ•° (evaluation.py)

æä¾›æ ‡å‡†çš„è¯„åˆ†å‡½æ•°ï¼Œå…¼å®¹LoCoMoè¯„ä¼°åè®®ï¼š

#### ä¸»è¦å‡½æ•°

| å‡½æ•° | ç”¨é€” | è¯´æ˜ |
|------|------|------|
| `f1_score(prediction, ground_truth)` | è¯çº§åˆ«F1åˆ†æ•° | åŸºäºè¯æ±‡é‡å çš„F1è®¡ç®— |
| `f1(prediction, ground_truth)` | å¤šç­”æ¡ˆF1åˆ†æ•° | å¤„ç†é€—å·åˆ†éš”çš„å¤šç­”æ¡ˆ |
| `rougel_score(prediction, ground_truth)` | ROUGE-Låˆ†æ•° | æœ€é•¿å…¬å…±å­åºåˆ—è¯„åˆ† |
| `exact_match_score(prediction, ground_truth)` | ç²¾ç¡®åŒ¹é… | å®Œå…¨åŒ¹é…è¯„åˆ† |
| `eval_question_answering(qas, eval_key)` | é—®ç­”è¯„ä¼° | æŒ‰ç±»åˆ«ä½¿ç”¨ä¸åŒè¯„åˆ†ç­–ç•¥ |

#### è¯„åˆ†ç­–ç•¥

- **ç±»åˆ«1ï¼ˆå¤šè·³æ¨ç†ï¼‰**: ä½¿ç”¨ `f1()` å¤šç­”æ¡ˆF1åˆ†æ•°
- **ç±»åˆ«2-4ï¼ˆå•è·³/æ—¶é—´/å¼€æ”¾åŸŸï¼‰**: ä½¿ç”¨ `f1_score()` æ ‡å‡†F1åˆ†æ•°
- **ç±»åˆ«5ï¼ˆå¯¹æŠ—æ€§é—®é¢˜ï¼‰**: æ£€æŸ¥æ˜¯å¦æ­£ç¡®æ‹’ç»å›ç­”

#### ä½¿ç”¨ç¤ºä¾‹

```python
from benchmark.task_eval.evaluation import f1_score, f1, eval_question_answering

# å•ä¸ªé—®é¢˜è¯„åˆ†
score = f1_score("Alice is a doctor", "Alice works as a doctor")
print(f"F1 Score: {score:.4f}")

# å¤šç­”æ¡ˆè¯„åˆ†
multi_score = f1("Alice, Bob", "Alice, Bob, Charlie")
print(f"Multi F1 Score: {multi_score:.4f}")

# æ‰¹é‡è¯„ä¼°
qa_data = [
    {"question": "Who is Alice?", "category": 2, "answer": "doctor", "prediction": "Alice is a doctor"},
    {"question": "What does Bob do?", "category": 1, "answer": "teacher, coach", "prediction": "teacher"}
]
scores, lengths, recalls = eval_question_answering(qa_data, eval_key="prediction")
```

### è¯­ä¹‰å›¾è°±QAè¯„ä¼°å™¨ (semantic_graph_qa_evaluator.py)

æ ¸å¿ƒè¯„ä¼°å¼•æ“ï¼Œå®ç°æ™ºèƒ½æ£€ç´¢å’Œç­”æ¡ˆç”Ÿæˆã€‚

#### æ£€ç´¢ç­–ç•¥

| ç±»åˆ« | ç­–ç•¥å | æ£€ç´¢é…ç½® | è¯´æ˜ |
|------|---------|----------|------|
| 1 | multi_hop | top_k=8, æ‰©å±•é‚»å±… | å¤šè·³æ¨ç†ï¼Œæ‰©å±•ç›¸å…³èŠ‚ç‚¹ |
| 2 | single_hop | top_k=5, æ— æ‰©å±• | ç›´æ¥äº‹å®æŸ¥è¯¢ |
| 3 | temporal | top_k=6, æ—¶é—´ä¼˜å…ˆ | æ—¶é—´ç›¸å…³ä¿¡æ¯æ£€ç´¢ |
| 4 | open_domain | top_k=10, å…¨é¢æ£€ç´¢ | å¼€æ”¾åŸŸé—®é¢˜ï¼Œå¹¿æ³›æ£€ç´¢ |
| 5 | adversarial | top_k=3, ä¿å®ˆæ£€ç´¢ | å¯¹æŠ—æ€§é—®é¢˜ï¼Œè°¨æ…å›ç­” |

#### åŸºæœ¬ä½¿ç”¨

```python
from benchmark.task_eval.semantic_graph_qa_evaluator import SemanticGraphQAEvaluator
from benchmark.extractor.dataset_inserter import ConversationSemanticStorage
from benchmark.llm_utils.llm_client import LLMClient

# åˆå§‹åŒ–
storage = ConversationSemanticStorage()
llm_client = LLMClient(model_name="deepseek-chat")
evaluator = SemanticGraphQAEvaluator(storage.semantic_graph, llm_client)

# æ£€ç´¢ä¸Šä¸‹æ–‡
context = evaluator.retrieve_context_for_question(
    question="What LGBTQ support group did Caroline attend?",
    category=2,
    conversation_id="conv-26",
    evidence=["D10:3"]
)

# ç”Ÿæˆç­”æ¡ˆ
answer_result = evaluator.generate_answer(context)
print(f"Generated Answer: {answer_result['generated_answer']}")

# è¯„ä¼°å•ä¸ªå¯¹è¯
qa_data = [{"question": "...", "answer": "...", "category": 2}]
results = evaluator.evaluate_single_conversation("conv-26", qa_data)
```

#### é«˜çº§åŠŸèƒ½

**è‡ªå®šä¹‰æ£€ç´¢ç­–ç•¥**ï¼š
```python
# ä¿®æ”¹æ£€ç´¢é…ç½®
evaluator.retrieval_config[1] = {
    "top_k": 10, 
    "include_relations": True, 
    "expand_neighbors": True
}
```

**æ‰¹é‡è¯„ä¼°**ï¼š
```python
qa_test_data = {
    "conv-26": [qa1, qa2, qa3],
    "conv-30": [qa4, qa5, qa6]
}
full_results = evaluator.run_full_evaluation(qa_test_data, save_results=True)
```

### ç‹¬ç«‹æ ·æœ¬è¯„ä¼°å™¨ (IndependentSampleEvaluator.py)

ç¡®ä¿æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å»ºç«‹è¯­ä¹‰å›¾è°±çš„è¯„ä¼°å™¨ï¼Œé¿å…æ•°æ®æ³„éœ²ã€‚

#### æ ¸å¿ƒç‰¹æ€§

- **ç‹¬ç«‹è¯­ä¹‰å›¾è°±**ï¼šæ¯ä¸ªæ ·æœ¬åˆ›å»ºç‹¬ç«‹çš„å›¾è°±å’Œå­˜å‚¨
- **å®Œæ•´è¯„åˆ†æŒ‡æ ‡**ï¼šF1ã€ROUGE-Lã€ç²¾ç¡®åŒ¹é…ã€å¬å›ç‡ç­‰
- **è¯¦ç»†ç»“æœè®°å½•**ï¼šé—®é¢˜çº§åˆ«ã€ç±»åˆ«çº§åˆ«ã€æ ·æœ¬çº§åˆ«ç»Ÿè®¡
- **ç®€åŒ–è¾“å‡ºæ ¼å¼**ï¼šæ¸…æ™°çš„ç»“æœç»“æ„

#### åŸºæœ¬å‘½ä»¤

```bash
python benchmark/task_eval/IndependentSampleEvaluator.py [OPTIONS]
```

#### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `--model` | str | âŒ | LLMæ¨¡å‹åç§° (é»˜è®¤: deepseek-chat) |
| `--samples` | list | âŒ | è¦è¯„ä¼°çš„æ ·æœ¬IDåˆ—è¡¨ |
| `--output_dir` | str | âŒ | è¾“å‡ºç›®å½•è·¯å¾„ |
| `--format` | str | âŒ | è¾“å‡ºæ ¼å¼ (comprehensive/simple) |
| `--debug` | flag | âŒ | å¯ç”¨è°ƒè¯•æ—¥å¿— |

#### ä½¿ç”¨ç¤ºä¾‹

**1. å•æ ·æœ¬è¯„ä¼°**
```bash
python benchmark/task_eval/IndependentSampleEvaluator.py \
    --model deepseek-chat \
    --samples conv-26 \
    --format comprehensive \
    --output_dir benchmark/task_eval/results/test
```

**2. å¤šæ ·æœ¬æ‰¹é‡è¯„ä¼°**
```bash
python benchmark/task_eval/IndependentSampleEvaluator.py \
    --model deepseek-chat \
    --samples conv-26 conv-30 conv-41 \
    --format comprehensive \
    --output_dir benchmark/task_eval/results/batch
```

**3. ä¸åŒæ¨¡å‹å¯¹æ¯”**
```bash
# DeepSeekè¯„ä¼°
python benchmark/task_eval/IndependentSampleEvaluator.py \
    --model deepseek-chat --samples conv-26 \
    --output_dir results/deepseek

# GPTè¯„ä¼°
python benchmark/task_eval/IndependentSampleEvaluator.py \
    --model gpt-4o-mini --samples conv-26 \
    --output_dir results/gpt4o
```

#### ç¼–ç¨‹æ¥å£

```python
from benchmark.task_eval.IndependentSampleEvaluator import IndependentSampleEvaluator
from benchmark.llm_utils.llm_client import LLMClient

# åˆå§‹åŒ–
llm_client = LLMClient(model_name="deepseek-chat")
evaluator = IndependentSampleEvaluator(llm_client, output_dir="results")

# è¯„ä¼°å•ä¸ªæ ·æœ¬
result = evaluator.evaluate_single_sample("conv-26")
if "error" not in result:
    summary = result["summary"]["overall"]
    print(f"Average F1 Score: {summary['avg_f1_score']:.4f}")
    print(f"Total Questions: {summary['total_questions']}")

# è¯„ä¼°å¤šä¸ªæ ·æœ¬
results = evaluator.evaluate_multiple_samples(
    sample_ids=["conv-26", "conv-30"],
    save_format="comprehensive"
)
```

### è¯¦ç»†è¯„ä¼°è„šæœ¬ (run_detailed_evaluation.py)

é«˜çº§è¯„ä¼°è„šæœ¬ï¼Œæä¾›æ±‡æ€»åˆ†æå’ŒéªŒè¯åŠŸèƒ½ã€‚

#### ä¸»è¦åŠŸèƒ½

- **æ±‡æ€»åˆ†æ**ï¼šè·¨æ ·æœ¬çš„ç»Ÿè®¡åˆ†æ
- **å¤šæ ¼å¼è¾“å‡º**ï¼šJSONã€CSVã€TXTæ ¼å¼ç»“æœ
- **è¯„ä¼°éªŒè¯**ï¼šä½¿ç”¨evaluation.pyè¿›è¡Œç»“æœéªŒè¯
- **è‡ªåŠ¨å‘ç°æ ·æœ¬**ï¼šæ”¯æŒè¯„ä¼°æ‰€æœ‰å¯ç”¨æ ·æœ¬

#### åŸºæœ¬å‘½ä»¤

```bash
python benchmark/task_eval/run_detailed_evaluation.py [OPTIONS]
```

#### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `--model` | str | âŒ | LLMæ¨¡å‹åç§° |
| `--samples` | list | âŒ | æ ·æœ¬IDåˆ—è¡¨ |
| `--output_dir` | str | âŒ | è¾“å‡ºç›®å½• |
| `--all_samples` | flag | âŒ | è¯„ä¼°æ‰€æœ‰å¯ç”¨æ ·æœ¬ |

#### ä½¿ç”¨ç¤ºä¾‹

**1. æŒ‡å®šæ ·æœ¬è¯„ä¼°**
```bash
python benchmark/task_eval/run_detailed_evaluation.py \
    --model deepseek-chat \
    --samples conv-26 conv-30 \
    --output_dir benchmark/task_eval/results/detailed
```

**2. å…¨æ ·æœ¬è¯„ä¼°**
```bash
python benchmark/task_eval/run_detailed_evaluation.py \
    --model deepseek-chat \
    --all_samples \
    --output_dir benchmark/task_eval/results/full
```

**3. å¤šæ¨¡å‹å¯¹æ¯”**
```bash
# DeepSeek Chat
python benchmark/task_eval/run_detailed_evaluation.py \
    --model deepseek-chat --samples conv-26 conv-30

# DeepSeek Reasoner
python benchmark/task_eval/run_detailed_evaluation.py \
    --model deepseek-reasoner --samples conv-26 conv-30

# GPT-4o-mini
python benchmark/task_eval/run_detailed_evaluation.py \
    --model gpt-4o-mini --samples conv-26 conv-30
```

#### è¾“å‡ºæ–‡ä»¶

ç”Ÿæˆçš„æ–‡ä»¶åŒ…æ‹¬ï¼š

```
benchmark/task_eval/results/detailed/
â”œâ”€â”€ evaluation_conv-26_20250708_143025.json       # æ ·æœ¬è¯¦ç»†ç»“æœ
â”œâ”€â”€ evaluation_conv-30_20250708_143156.json       # æ ·æœ¬è¯¦ç»†ç»“æœ
â”œâ”€â”€ cross_sample_summary_20250708_143300.json     # è·¨æ ·æœ¬æ±‡æ€»
â”œâ”€â”€ evaluation_report_20250708_143300.txt         # å¯è¯»æ€§æŠ¥å‘Š
â”œâ”€â”€ all_questions_detailed_20250708_143300.csv    # é—®é¢˜çº§åˆ«CSV
â”œâ”€â”€ sample_metrics_summary_20250708_143300.json   # æ ·æœ¬æŒ‡æ ‡æ±‡æ€»
â”œâ”€â”€ evaluation_dataset_20250708_143300.json       # evaluation.pyå…¼å®¹æ•°æ®
â””â”€â”€ evaluation_validation_20250708_143300.json    # éªŒè¯ç»“æœ
```

### å¿«é€Ÿæµ‹è¯•è„šæœ¬ (test_light_script.py)

ç”¨äºå¿«é€ŸéªŒè¯å’Œè°ƒè¯•çš„è½»é‡çº§æµ‹è¯•å·¥å…·ã€‚

#### åŠŸèƒ½æ¨¡å¼

**1. å•é—®é¢˜æµ‹è¯•**
- æµ‹è¯•å•ä¸ªé¢„è®¾é—®é¢˜çš„å®Œæ•´æµç¨‹
- æ˜¾ç¤ºæ£€ç´¢ä¸Šä¸‹æ–‡ã€ç”Ÿæˆç­”æ¡ˆã€è¯„åˆ†ç»“æœ
- ç”¨äºåŠŸèƒ½éªŒè¯å’Œè°ƒè¯•

**2. ç±»åˆ«ç­–ç•¥æµ‹è¯•**
- æµ‹è¯•ä¸åŒQAç±»åˆ«çš„å¤„ç†ç­–ç•¥
- å¯¹æ¯”å„ç±»åˆ«çš„æ£€ç´¢å’Œç”Ÿæˆæ•ˆæœ
- ç”¨äºç­–ç•¥ä¼˜åŒ–

#### ä½¿ç”¨æ–¹æ³•

```bash
python benchmark/task_eval/test_light_script.py
```

æ ¹æ®æç¤ºé€‰æ‹©æµ‹è¯•æ¨¡å¼ï¼š
- è¾“å…¥ `1`: å•é—®é¢˜æµ‹è¯•
- è¾“å…¥ `2`: ç±»åˆ«ç­–ç•¥æµ‹è¯•

#### ç¤ºä¾‹è¾“å‡º

```
ğŸ§ª Semantic Graph QA Testing
1. å•é—®é¢˜æµ‹è¯•
2. ç±»åˆ«ç­–ç•¥æµ‹è¯•

é€‰æ‹©æµ‹è¯•ç±»å‹ (1 æˆ– 2): 1

ğŸ” æµ‹è¯•é—®é¢˜: What LGBTQ support group did Caroline attend?
ğŸ“ æ ‡å‡†ç­”æ¡ˆ: Connected LGBTQ Activists
ğŸ·ï¸ ç±»åˆ«: 2

1ï¸âƒ£ æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡...
âœ… æ£€ç´¢åˆ° 5 ä¸ªèŠ‚ç‚¹
âœ… æ£€ç´¢åˆ° 0 ä¸ªå…³ç³»
ğŸ“Š è¯æ®å¬å›ç‡: 0.80

ğŸ“‹ æ£€ç´¢åˆ°çš„ä¸»è¦å†…å®¹:
  [1] dialog: Caroline mentioned joining a local LGBTQ support group called "Connected LGBTQ...
  [2] summary: Caroline discussed her involvement with LGBTQ activism...
  [3] event: Caroline attended her first LGBTQ support group meeting...

2ï¸âƒ£ ç”Ÿæˆç­”æ¡ˆ...
ğŸ¤– ç”Ÿæˆç­”æ¡ˆ: Connected LGBTQ Activists

3ï¸âƒ£ è¯„ä¼°ç»“æœ...
ğŸ“Š F1åˆ†æ•°: 1.0000
ğŸ“Š ç²¾ç¡®åŒ¹é…: 1
```

### è¯­ä¹‰å›¾è°±è¯„ä¼°è„šæœ¬ (run_semantic_evaluation.py)

ä¸“é—¨ç”¨äºè¯­ä¹‰å›¾è°±QAè¯„ä¼°çš„å®Œæ•´è„šæœ¬ã€‚

#### ä¸»è¦åŠŸèƒ½

- **å®Œæ•´è¯„ä¼°æµç¨‹**ï¼šä»æ•°æ®åŠ è½½åˆ°ç»“æœä¿å­˜
- **å¤šå¯¹è¯æ”¯æŒ**ï¼šåŒæ—¶è¯„ä¼°å¤šä¸ªå¯¹è¯
- **è¯¦ç»†ç»Ÿè®¡åˆ†æ**ï¼šæŒ‰ç±»åˆ«ã€æŒ‰å¯¹è¯çš„è¯¦ç»†åˆ†æ
- **å¯è§†åŒ–è¾“å‡º**ï¼šäººç±»å¯è¯»çš„è¯„ä¼°æŠ¥å‘Š

#### åŸºæœ¬å‘½ä»¤

```bash
python benchmark/task_eval/run_semantic_evaluation.py [OPTIONS]
```

#### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `--model` | str | âŒ | LLMæ¨¡å‹åç§° |
| `--conversations` | list | âŒ | å¯¹è¯IDåˆ—è¡¨ |
| `--output_dir` | str | âŒ | è¾“å‡ºç›®å½• |
| `--load_existing` | flag | âŒ | åŠ è½½å·²æœ‰è¯­ä¹‰å›¾è°± |
| `--debug` | flag | âŒ | å¯ç”¨è°ƒè¯•æ—¥å¿— |

#### ä½¿ç”¨ç¤ºä¾‹

**1. åŸºæœ¬è¯„ä¼°**
```bash
python benchmark/task_eval/run_semantic_evaluation.py \
    --model deepseek-chat \
    --conversations conv-26 conv-30 \
    --output_dir benchmark/task_eval/results/semantic
```

**2. è°ƒè¯•æ¨¡å¼**
```bash
python benchmark/task_eval/run_semantic_evaluation.py \
    --model deepseek-chat \
    --conversations conv-26 \
    --debug \
    --output_dir benchmark/task_eval/results/debug
```

**3. ä½¿ç”¨å·²æœ‰å›¾è°±**
```bash
python benchmark/task_eval/run_semantic_evaluation.py \
    --model deepseek-chat \
    --conversations conv-26 conv-30 \
    --load_existing \
    --output_dir benchmark/task_eval/results/existing
```

## ğŸ“Š è¾“å‡ºæ–‡ä»¶è¯¦ç»†è¯´æ˜

### è¯„ä¼°ç»“æœæ–‡ä»¶ç»“æ„

#### 1. æ ·æœ¬è¯¦ç»†ç»“æœ (`evaluation_*.json`)

```json
{
  "evaluation_summary": {
    "sample_id": "conv-26",
    "evaluation_timestamp": "2025-01-08T14:30:15.123456",
    "llm_model": "deepseek-chat",
    "overall": {
      "total_questions": 45,
      "avg_f1_score": 0.3245,
      "avg_f1_multi": 0.3156,
      "avg_rouge_l": 0.3892,
      "avg_exact_match": 0.1556,
      "avg_main_score": 0.3245,
      "avg_evidence_recall": 0.7234,
      "avg_context_nodes": 6.2,
      "avg_context_relations": 2.8
    },
    "by_category": {
      "category_1": {
        "category_name": "multi_hop",
        "questions_count": 12,
        "avg_f1_score": 0.2156,
        "avg_main_score": 0.2156,
        "std_main_score": 0.1245
      }
    }
  },
  "questions": [
    {
      "question": "When did Caroline go to the LGBTQ support group?",
      "category": 2,
      "category_name": "single_hop",
      "answer": "7 May 2023",
      "prediction": "Caroline went to the LGBTQ support group yesterday.",
      "scores": {
        "f1_score": 0.2456,
        "f1_multi": 0.2456,
        "rouge_l": 0.3123,
        "rouge_l_multi": 0.3123,
        "exact_match": 0.0,
        "main_score": 0.2456
      },
      "retrieval": {
        "strategy": "single_hop",
        "nodes": [
          {
            "uid": "node_123",
            "data_type": "event",
            "content": "Caroline attended an LGBTQ support group...",
            "score": 0.8756
          }
        ],
        "relations": [],
        "raw_context": "=== CONVERSATION CONTEXT ===\n[1] EVENT: ...",
        "nodes_count": 5,
        "relations_count": 0
      },
      "evidence": ["D1:3"],
      "evidence_recall": 0.8,
      "prompt": "Based on the following conversation context...",
      "context_nodes_count": 5,
      "context_relations_count": 0
    }
  ]
}
```

#### 2. è·¨æ ·æœ¬æ±‡æ€» (`cross_sample_summary_*.json`)

åŒ…å«æ‰€æœ‰æ ·æœ¬çš„æ±‡æ€»ç»Ÿè®¡ï¼Œæ”¯æŒå¤šæ ·æœ¬å¯¹æ¯”åˆ†æã€‚

#### 3. é—®é¢˜çº§åˆ«CSV (`all_questions_detailed_*.csv`)

| Sample_ID | Question | Category | Strategy | Ground_Truth | Prediction | F1_Score | Evidence_Recall | Context_Nodes | Evidence_Count |
|-----------|----------|----------|----------|--------------|------------|----------|-----------------|---------------|----------------|
| conv-26 | When did Caroline... | 2 | single_hop | 7 May 2023 | yesterday | 0.2456 | 0.8000 | 5 | 1 |

#### 4. è¯„ä¼°éªŒè¯ (`evaluation_validation_*.json`)

ä½¿ç”¨evaluation.pyéªŒè¯è¯„ä¼°ç»“æœçš„ä¸€è‡´æ€§ã€‚

### æ–‡ä»¶å‘½åè§„èŒƒ

- `evaluation_{sample_id}_{timestamp}.json`: å•æ ·æœ¬è¯¦ç»†ç»“æœ
- `cross_sample_summary_{timestamp}.json`: è·¨æ ·æœ¬æ±‡æ€»
- `evaluation_report_{timestamp}.txt`: å¯è¯»æ€§æŠ¥å‘Š
- `all_questions_detailed_{timestamp}.csv`: é—®é¢˜çº§åˆ«ç»“æœ
- `evaluation_dataset_{timestamp}.json`: æ ‡å‡†æ ¼å¼æ•°æ®é›†

## ğŸ’¡ è¯„ä¼°ç­–ç•¥è¯´æ˜

### QAç±»åˆ«å’Œç­–ç•¥æ˜ å°„

| ç±»åˆ« | åç§° | ç­–ç•¥ç‰¹ç‚¹ | è¯„åˆ†æ–¹æ³• | é€‚ç”¨åœºæ™¯ |
|------|------|----------|----------|----------|
| 1 | Multi-hop | æ‰©å±•é‚»å±…èŠ‚ç‚¹ï¼Œå…³ç³»æ¨ç† | å¤šç­”æ¡ˆF1 | éœ€è¦å¤šæ­¥æ¨ç†çš„å¤æ‚é—®é¢˜ |
| 2 | Single-hop | ç›´æ¥æ£€ç´¢ï¼Œæ— æ‰©å±• | æ ‡å‡†F1 | ç›´æ¥äº‹å®æŸ¥è¯¢ |
| 3 | Temporal | æ—¶é—´ä¿¡æ¯ä¼˜å…ˆ | æ ‡å‡†F1 | æ—¶é—´ç›¸å…³é—®é¢˜ |
| 4 | Open-domain | å¹¿æ³›æ£€ç´¢ï¼Œå…¨é¢åˆ†æ | æ ‡å‡†F1 | å¼€æ”¾æ€§é—®é¢˜ |
| 5 | Adversarial | ä¿å®ˆæ£€ç´¢ï¼Œæ‹’ç»å›ç­” | æ‹’ç»æ£€æµ‹ | å¯¹æŠ—æ€§é—®é¢˜ï¼Œæµ‹è¯•ç³»ç»Ÿè¾¹ç•Œ |

### è¯„åˆ†æŒ‡æ ‡è¯´æ˜

#### ä¸»è¦æŒ‡æ ‡

- **F1 Score**: è¯çº§åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡è°ƒå’Œå¹³å‡
- **F1 Multi**: å¤„ç†å¤šç­”æ¡ˆçš„F1åˆ†æ•°ï¼ˆé€—å·åˆ†éš”ï¼‰
- **ROUGE-L**: åŸºäºæœ€é•¿å…¬å…±å­åºåˆ—çš„è¯„åˆ†
- **Exact Match**: å®Œå…¨ç²¾ç¡®åŒ¹é…
- **Evidence Recall**: è¯æ®æ£€ç´¢å¬å›ç‡

#### é€‰æ‹©ç­–ç•¥

- **ç±»åˆ«1**: ä½¿ç”¨F1 Multiä½œä¸ºä¸»è¦åˆ†æ•°
- **ç±»åˆ«2-4**: ä½¿ç”¨æ ‡å‡†F1ä½œä¸ºä¸»è¦åˆ†æ•°
- **ç±»åˆ«5**: ä½¿ç”¨æ‹’ç»æ£€æµ‹é€»è¾‘

### æ£€ç´¢ç­–ç•¥é…ç½®

#### å¯è°ƒå‚æ•°

```python
retrieval_config = {
    1: {"top_k": 8, "include_relations": True, "expand_neighbors": True},
    2: {"top_k": 5, "include_relations": False, "expand_neighbors": False},
    3: {"top_k": 6, "include_relations": True, "expand_neighbors": False},
    4: {"top_k": 10, "include_relations": True, "expand_neighbors": True},
    5: {"top_k": 3, "include_relations": False, "expand_neighbors": False}
}
```

#### ä¼˜åŒ–å»ºè®®

- **å¤šè·³æ¨ç†**: å¢åŠ top_kå’Œé‚»å±…æ‰©å±•
- **å•è·³æŸ¥è¯¢**: å‡å°‘æ£€ç´¢æ•°é‡ï¼Œæé«˜ç²¾ç¡®æ€§
- **æ—¶é—´é—®é¢˜**: ä¼˜å…ˆæ£€ç´¢æ—¶é—´ç›¸å…³èŠ‚ç‚¹
- **å¯¹æŠ—æ€§**: å‡å°‘æ£€ç´¢ï¼Œé¿å…è¿‡åº¦æ¨æµ‹

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–å’Œè°ƒè¯•

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### 1. å†…å­˜ä¼˜åŒ–
```bash
# é™åˆ¶æ ·æœ¬æ•°é‡
python benchmark/task_eval/IndependentSampleEvaluator.py \
    --samples conv-26 conv-30 \
    --format simple

# åˆ†æ‰¹å¤„ç†
for sample in conv-26 conv-30 conv-41; do
    python benchmark/task_eval/IndependentSampleEvaluator.py \
        --samples $sample --output_dir results/batch_$sample
done
```

#### 2. é€Ÿåº¦ä¼˜åŒ–
```bash
# ä½¿ç”¨è¾ƒå°çš„æ£€ç´¢æ•°é‡
# åœ¨ä»£ç ä¸­ä¿®æ”¹retrieval_configçš„top_kå€¼

# ç¦ç”¨å¤æ‚ç‰¹æ€§
python benchmark/task_eval/IndependentSampleEvaluator.py \
    --samples conv-26 --format simple
```

#### 3. æ¨¡å‹é€‰æ‹©
- **å¿«é€Ÿæµ‹è¯•**: deepseek-chat (è¾ƒå¿«)
- **é«˜è´¨é‡**: deepseek-reasoner (è¾ƒæ…¢ä½†æ¨ç†èƒ½åŠ›å¼º)
- **å¯¹æ¯”åŸºå‡†**: gpt-4o-mini

### è°ƒè¯•å’Œæ•…éšœæ’é™¤

#### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—
```bash
python benchmark/task_eval/IndependentSampleEvaluator.py \
    --samples conv-26 --debug
```

#### 2. å¿«é€ŸåŠŸèƒ½éªŒè¯
```bash
python benchmark/task_eval/test_light_script.py
```

#### 3. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
```bash
# éªŒè¯å®ä½“æŠ½å–ç»“æœ
ls benchmark/results/full_extraction/

# éªŒè¯æ•°æ®é›†åˆ¶ä½œç»“æœ
ls benchmark/dataset/locomo/extraction/

# æ£€æŸ¥QAæ•°æ®æ ¼å¼
python -c "
import json
with open('benchmark/dataset/locomo/locomo10.json') as f:
    data = json.load(f)
    sample = data[0]
    print('Sample keys:', sample.keys())
    if 'qa' in sample:
        print('QA keys:', sample['qa'][0].keys())
"
```

#### 4. å¸¸è§é—®é¢˜è§£å†³

**é—®é¢˜1: æ£€ç´¢ç»“æœä¸ºç©º**
```bash
# æ£€æŸ¥è¯­ä¹‰å›¾è°±æ˜¯å¦æ­£ç¡®æ„å»º
python -c "
from benchmark.extractor.dataset_inserter import ConversationSemanticStorage
storage = ConversationSemanticStorage()
storage.store_conversation('conv-26')
print(f'èŠ‚ç‚¹æ•°: {len(storage.semantic_graph.semantic_map.memory_units)}')
"
```

**é—®é¢˜2: LLMç”Ÿæˆå¤±è´¥**
```bash
# æµ‹è¯•LLMè¿æ¥
python -c "
from benchmark.llm_utils.llm_client import LLMClient
client = LLMClient('deepseek-chat')
result = client.generate_answer('Test question')
print(f'Result: {result}')
"
```

**é—®é¢˜3: è¯„åˆ†å¼‚å¸¸**
```bash
# éªŒè¯è¯„åˆ†å‡½æ•°
python -c "
from benchmark.task_eval.evaluation import f1_score, f1
print('F1 Score:', f1_score('test answer', 'test answer'))
print('F1 Multi:', f1('test, answer', 'test, answer'))
"
```

## ğŸ“ˆ å®Œæ•´å·¥ä½œæµç¨‹ç¤ºä¾‹

### ä»é›¶å¼€å§‹çš„å®Œæ•´è¯„ä¼°æµç¨‹

```bash
# 1. è®¾ç½®ç¯å¢ƒ
cd /home/zyh/code/AgentMemorySystem
export DEEPSEEK_API_KEY="your-api-key"

# 2. æ•°æ®å‡†å¤‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
python benchmark/extractor/locomo_entity_extractor.py \
    --dataset benchmark/dataset/locomo/locomo10.json \
    --batch-all --use-chunking \
    --output benchmark/results/full_extraction

python benchmark/extractor/dataset_maker.py \
    --results-dir benchmark/results/full_extraction \
    --output-dir benchmark/dataset/locomo/extraction

# 3. å¿«é€ŸåŠŸèƒ½æµ‹è¯•
python benchmark/task_eval/test_light_script.py

# 4. å•æ ·æœ¬è¯¦ç»†è¯„ä¼°
python benchmark/task_eval/IndependentSampleEvaluator.py \
    --model deepseek-chat \
    --samples conv-26 \
    --format comprehensive \
    --output_dir benchmark/task_eval/results/single

# 5. å¤šæ ·æœ¬æ‰¹é‡è¯„ä¼°
python benchmark/task_eval/run_detailed_evaluation.py \
    --model deepseek-chat \
    --samples conv-26 conv-30 conv-41 \
    --output_dir benchmark/task_eval/results/batch

# 6. å…¨æ ·æœ¬è¯„ä¼°ï¼ˆå¦‚æœéœ€è¦ï¼‰
python benchmark/task_eval/run_detailed_evaluation.py \
    --model deepseek-chat \
    --all_samples \
    --output_dir benchmark/task_eval/results/full

# 7. ç»“æœåˆ†æ
cat benchmark/task_eval/results/batch/evaluation_report_*.txt
```

### å¤šæ¨¡å‹å¯¹æ¯”è¯„ä¼°

```bash
# DeepSeek Chat
python benchmark/task_eval/run_detailed_evaluation.py \
    --model deepseek-chat \
    --samples conv-26 conv-30 \
    --output_dir benchmark/task_eval/results/deepseek_chat

# DeepSeek Reasoner
python benchmark/task_eval/run_detailed_evaluation.py \
    --model deepseek-reasoner \
    --samples conv-26 conv-30 \
    --output_dir benchmark/task_eval/results/deepseek_reasoner

# GPT-4o-mini
python benchmark/task_eval/run_detailed_evaluation.py \
    --model gpt-4o-mini \
    --samples conv-26 conv-30 \
    --output_dir benchmark/task_eval/results/gpt4o_mini

# ç»“æœå¯¹æ¯”
echo "=== Model Comparison ==="
for model in deepseek_chat deepseek_reasoner gpt4o_mini; do
    echo "--- $model ---"
    grep "å¹³å‡åˆ†æ•°" benchmark/task_eval/results/$model/evaluation_report_*.txt
done
```

### å®šåˆ¶åŒ–è¯„ä¼°æµç¨‹

```bash
# é’ˆå¯¹ç‰¹å®šç±»åˆ«çš„è¯„ä¼°
python -c "
import json
from benchmark.task_eval.IndependentSampleEvaluator import IndependentSampleEvaluator
from benchmark.llm_utils.llm_client import LLMClient

# è¿‡æ»¤ç‰¹å®šç±»åˆ«çš„é—®é¢˜
# åœ¨è¿™é‡Œå¯ä»¥æ·»åŠ è‡ªå®šä¹‰çš„è¿‡æ»¤å’Œè¯„ä¼°é€»è¾‘
"

# é’ˆå¯¹ç‰¹å®šç­–ç•¥çš„ä¼˜åŒ–
# ä¿®æ”¹retrieval_configåé‡æ–°è¯„ä¼°
```

## ğŸ“‹ è¾“å‡ºåˆ†æå’Œè§£è¯»

### å…³é”®æŒ‡æ ‡è§£è¯»

#### æ€»ä½“æ€§èƒ½æŒ‡æ ‡
- **avg_main_score**: ä¸»è¦è¯„åˆ†ï¼ˆæ ¹æ®ç±»åˆ«é€‰æ‹©æœ€é€‚åˆçš„è¯„åˆ†æ–¹æ³•ï¼‰
- **avg_f1_score**: å¹³å‡F1åˆ†æ•°
- **avg_exact_match**: ç²¾ç¡®åŒ¹é…ç‡
- **avg_evidence_recall**: è¯æ®å¬å›ç‡

#### æŒ‰ç±»åˆ«æ€§èƒ½
- **category_X_avg_score**: å„ç±»åˆ«å¹³å‡åˆ†æ•°
- **category_X_count**: å„ç±»åˆ«é—®é¢˜æ•°é‡
- **category_X_std_score**: å„ç±»åˆ«åˆ†æ•°æ ‡å‡†å·®

#### æ£€ç´¢è´¨é‡æŒ‡æ ‡
- **avg_context_nodes**: å¹³å‡æ£€ç´¢èŠ‚ç‚¹æ•°
- **avg_context_relations**: å¹³å‡æ£€ç´¢å…³ç³»æ•°
- **retrieval_strategy**: ä½¿ç”¨çš„æ£€ç´¢ç­–ç•¥

### ç»“æœè§£è¯»å»ºè®®

#### è‰¯å¥½æ€§èƒ½æŒ‡æ ‡
- **F1 Score > 0.4**: è¾ƒå¥½çš„ç­”æ¡ˆè´¨é‡
- **Evidence Recall > 0.7**: è‰¯å¥½çš„æ£€ç´¢è´¨é‡
- **Exact Match > 0.2**: å¯æ¥å—çš„ç²¾ç¡®æ€§

#### é—®é¢˜è¯Šæ–­
- **F1 Score < 0.2**: å¯èƒ½æ˜¯æ£€ç´¢è´¨é‡å·®æˆ–LLMç”Ÿæˆé—®é¢˜
- **Evidence Recall < 0.5**: æ£€ç´¢ç­–ç•¥éœ€è¦ä¼˜åŒ–
- **High Std Score**: æ€§èƒ½ä¸ç¨³å®šï¼Œéœ€è¦å‚æ•°è°ƒä¼˜

## ğŸ”— ç›¸å…³å·¥å…·å’Œä¾èµ–

### æ ¸å¿ƒä¾èµ–
- **å®ä½“å…³ç³»æŠ½å–**: [`benchmark/extractor/`](../extractor/) - æä¾›è¯­ä¹‰å›¾è°±æ„å»º
- **LLMå·¥å…·**: [`benchmark/llm_utils/`](../llm_utils/) - æä¾›æ¨¡å‹æ¥å£
- **è¯­ä¹‰å›¾è°±**: [`dev/semantic_graph.py`](../../dev/semantic_graph.py) - æ ¸å¿ƒå›¾è°±ç»“æ„

### æ•°æ®ä¾èµ–
- **åŸå§‹æ•°æ®**: `benchmark/dataset/locomo/locomo10.json`
- **æŠ½å–ç»“æœ**: `benchmark/dataset/locomo/extraction/`
- **å®ä½“å…³ç³»**: `benchmark/results/full_extraction/`

### ç¯å¢ƒè¦æ±‚
- Python 3.8+
- DeepSeek API Key æˆ– OpenAI API Key
- å……è¶³çš„å†…å­˜ï¼ˆæ¨è16GB+ç”¨äºå¤§è§„æ¨¡è¯„ä¼°ï¼‰

## ğŸ“„ ç‰ˆæœ¬å†å²

- **v1.0**: åŸºç¡€è¯­ä¹‰å›¾è°±QAè¯„ä¼°
- **v1.1**: æ·»åŠ ç‹¬ç«‹æ ·æœ¬è¯„ä¼°
- **v1.2**: å¤šç­–ç•¥æ£€ç´¢å’Œè¯„åˆ†
- **v1.3**: è¯¦ç»†åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ
- **v1.4**: å¿«é€Ÿæµ‹è¯•å’Œè°ƒè¯•å·¥å…·
- **v1.5**: å¤šæ¨¡å‹æ”¯æŒå’Œå¯¹æ¯”è¯„ä¼°
- **v1.6**: å®Œæ•´å·¥ä½œæµç¨‹å’Œæ€§èƒ½ä¼˜åŒ–

## ğŸ¤ è´¡çŒ®æŒ‡å—

### å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
git clone <repository-url>
cd AgentMemorySystem
pip install -r requirements.txt
```

### æ·»åŠ æ–°çš„è¯„ä¼°ç­–ç•¥
1. åœ¨ `SemanticGraphQAEvaluator` ä¸­æ·»åŠ æ–°çš„ `category_strategies`
2. æ›´æ–° `retrieval_config` é…ç½®
3. åœ¨ `_build_prompt` ä¸­æ·»åŠ å¯¹åº”çš„æç¤ºè¯æ¨¡æ¿
4. æ›´æ–°è¯„åˆ†é€»è¾‘

### æµ‹è¯•æ–°åŠŸèƒ½
```bash
python benchmark/task_eval/test_light_script.py
```

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š
- æäº¤Issue: [GitHub Issues]
- é‚®ç®±: zhangyuhan25@otcaix.iscas.ac.cn

---

*æœ€åæ›´æ–°: 2025å¹´7æœˆ8æ—¥*