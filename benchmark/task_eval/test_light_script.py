import logging
import json
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from benchmark.task_eval.semantic_graph_qa_evaluator import SemanticGraphQAEvaluator
from benchmark.extractor.dataset_inserter import ConversationSemanticStorage
from benchmark.llm_utils.llm_client import LLMClient


def quick_test_single_question():
    """å¿«é€Ÿæµ‹è¯•å•ä¸ªé—®é¢˜"""
    
    # åˆå§‹åŒ–
    storage = ConversationSemanticStorage()
    llm_client = LLMClient(model_name="deepseek-chat")
    evaluator = SemanticGraphQAEvaluator(storage.semantic_graph, llm_client)
    
    # æµ‹è¯•é—®é¢˜
    test_question = {
        "question": "What LGBTQ support group did Caroline attend?",
        "answer": "Connected LGBTQ Activists",
        "category": 2,
        "evidence": ["D10:3"]
    }
    
    conversation_id = "conv-26"
    
    print(f"ğŸ” æµ‹è¯•é—®é¢˜: {test_question['question']}")
    print(f"ğŸ“ æ ‡å‡†ç­”æ¡ˆ: {test_question['answer']}")
    print(f"ğŸ·ï¸ ç±»åˆ«: {test_question['category']}")
    
    # æ£€ç´¢ä¸Šä¸‹æ–‡
    print("\n1ï¸âƒ£ æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡...")
    context = evaluator.retrieve_context_for_question(
        question=test_question["question"],
        category=test_question["category"],
        conversation_id=conversation_id,
        evidence=test_question.get("evidence", [])
    )
    
    print(f"âœ… æ£€ç´¢åˆ° {len(context['retrieved_nodes'])} ä¸ªèŠ‚ç‚¹")
    print(f"âœ… æ£€ç´¢åˆ° {len(context['retrieved_relations'])} ä¸ªå…³ç³»")
    print(f"ğŸ“Š è¯æ®å¬å›ç‡: {context.get('evidence_recall', 0):.2f}")
    
    # æ˜¾ç¤ºéƒ¨åˆ†æ£€ç´¢å†…å®¹
    print("\nğŸ“‹ æ£€ç´¢åˆ°çš„ä¸»è¦å†…å®¹:")
    for i, node in enumerate(context["retrieved_nodes"][:3]):
        print(f"  [{i+1}] {node['data_type']}: {node['content'][:100]}...")
    
    # ç”Ÿæˆç­”æ¡ˆ
    print("\n2ï¸âƒ£ ç”Ÿæˆç­”æ¡ˆ...")
    answer_result = evaluator.generate_answer(context)
    
    generated_answer = answer_result["generated_answer"]
    print(f"ğŸ¤– ç”Ÿæˆç­”æ¡ˆ: {generated_answer}")
    
    # ç®€å•è¯„ä¼°
    print("\n3ï¸âƒ£ è¯„ä¼°ç»“æœ...")
    from benchmark.task_eval.evaluation import f1_score, exact_match_score
    
    f1 = f1_score(generated_answer, test_question["answer"])
    em = exact_match_score(generated_answer, test_question["answer"])
    
    print(f"ğŸ“Š F1åˆ†æ•°: {f1:.4f}")
    print(f"ğŸ“Š ç²¾ç¡®åŒ¹é…: {em}")
    
    # æ˜¾ç¤ºæç¤ºè¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    print(f"\nğŸ”§ ä½¿ç”¨çš„æç¤ºè¯é¢„è§ˆ:")
    print(answer_result.get("prompt", "")[:300] + "...")


def test_category_strategies():
    """æµ‹è¯•ä¸åŒç±»åˆ«çš„ç­–ç•¥"""
    
    storage = ConversationSemanticStorage()
    llm_client = LLMClient(model_name="deepseek-chat")
    evaluator = SemanticGraphQAEvaluator(storage.semantic_graph, llm_client)
    
    # æµ‹è¯•æ¯ä¸ªç±»åˆ«çš„ä¸€ä¸ªé—®é¢˜
    test_questions = [
        {
            "question": "How does Caroline's LGBTQ activism connect to her adoption plans?",
            "answer": "Caroline chose an adoption agency that supports LGBTQ+ community",
            "category": 1,  # å¤šè·³æ¨ç†
            "evidence": []
        },
        {
            "question": "What is the name of Caroline's LGBTQ activist group?",
            "answer": "Connected LGBTQ Activists",
            "category": 2,  # å•è·³äº‹å®
            "evidence": ["D10:3"]
        },
        {
            "question": "When did Caroline join the LGBTQ activist group?",
            "answer": "last Tuesday",
            "category": 3,  # æ—¶é—´ç›¸å…³
            "evidence": ["D10:3"]
        }
    ]
    
    conversation_id = "conv-26"
    
    for i, test_q in enumerate(test_questions):
        print(f"\n{'='*50}")
        print(f"ğŸ§ª æµ‹è¯•ç±»åˆ« {test_q['category']} - {evaluator.category_strategies[test_q['category']]}")
        print(f"â“ é—®é¢˜: {test_q['question']}")
        
        # æ£€ç´¢å’Œç”Ÿæˆ
        context = evaluator.retrieve_context_for_question(
            question=test_q["question"],
            category=test_q["category"],
            conversation_id=conversation_id,
            evidence=test_q.get("evidence", [])
        )
        
        answer_result = evaluator.generate_answer(context)
        
        print(f"ğŸ¯ æ ‡å‡†ç­”æ¡ˆ: {test_q['answer']}")
        print(f"ğŸ¤– ç”Ÿæˆç­”æ¡ˆ: {answer_result['generated_answer']}")
        
        # è¯„ä¼°
        from benchmark.task_eval.evaluation import f1_score
        f1 = f1_score(answer_result['generated_answer'], test_q['answer'])
        print(f"ğŸ“Š F1åˆ†æ•°: {f1:.4f}")
        print(f"ğŸ“Š æ£€ç´¢èŠ‚ç‚¹æ•°: {len(context['retrieved_nodes'])}")
        print(f"ğŸ“Š ç­–ç•¥: {context['strategy']}")


if __name__ == "__main__":
    print("ğŸ§ª Semantic Graph QA Testing")
    print("1. å•é—®é¢˜æµ‹è¯•")
    print("2. ç±»åˆ«ç­–ç•¥æµ‹è¯•")
    
    choice = input("\né€‰æ‹©æµ‹è¯•ç±»å‹ (1 æˆ– 2): ").strip()
    
    if choice == "1":
        quick_test_single_question()
    elif choice == "2":
        test_category_strategies()
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")