# ä¿®æ”¹ IndependentSampleEvaluator.py 

import logging
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import numpy as np
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from benchmark.task_eval.semantic_graph_qa_evaluator import SemanticGraphQAEvaluator
from benchmark.extractor.dataset_inserter import ConversationSemanticStorage
from benchmark.llm_utils.llm_client import LLMClient
from benchmark.task_eval.evaluation import calculate_f1_score,calculate_f1_score_multi, calculate_rouge_score, calculate_bleu_score, exact_match_score


class IndependentSampleEvaluator:
    """
    ç‹¬ç«‹æ ·æœ¬è¯„ä¼°å™¨ - ç®€åŒ–è¾“å‡ºæ ¼å¼ï¼ŒåŒ…å«å¤šç§è¯„åˆ†æŒ‡æ ‡
    """
    
    def __init__(self, 
                 llm_client: LLMClient,
                 output_dir: str = "benchmark/task_eval/results"):
        """
        åˆå§‹åŒ–ç‹¬ç«‹æ ·æœ¬è¯„ä¼°å™¨
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            output_dir: è¾“å‡ºç›®å½•
        """
        self.llm_client = llm_client
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger(__name__)
        
    def evaluate_single_sample(self, 
                              sample_id: str,
                              raw_dataset_file: str = "benchmark/dataset/locomo/locomo10.json",
                              extracted_dataset_file: str = "benchmark/dataset/locomo/extraction/locomo_extracted_full_dataset.json") -> Dict[str, Any]:
        """ç‹¬ç«‹è¯„ä¼°å•ä¸ªæ ·æœ¬ - è¿”å›ç®€åŒ–ç»“æœ"""
        self.logger.info(f"ğŸ”„ å¼€å§‹ç‹¬ç«‹è¯„ä¼°æ ·æœ¬: {sample_id}")
        
        try:
            # æ­¥éª¤1-3: åˆ›å»ºå­˜å‚¨ã€å­˜å‚¨æ•°æ®ã€æå–QA
            storage = ConversationSemanticStorage()
            storage_stats = storage.store_conversation(
                sample_id=sample_id,
                raw_dataset_file=raw_dataset_file,
                extracted_dataset_file=extracted_dataset_file,
                include_raw=True,
                include_extracted=True
            )
            
            qa_test_data = storage.get_qa_test_data([sample_id], raw_dataset_file)
            if not qa_test_data or sample_id not in qa_test_data:
                raise ValueError(f"æ²¡æœ‰æ‰¾åˆ° {sample_id} çš„QAæµ‹è¯•æ•°æ®")
            
            # æ­¥éª¤4: åˆ›å»ºè¯„ä¼°å™¨å¹¶è¯„ä¼°
            evaluator = SemanticGraphQAEvaluator(
                semantic_graph=storage.semantic_graph,
                llm_client=self.llm_client,
                output_dir=self.output_dir
            )
            
            # æ­¥éª¤5: é€ä¸ªé—®é¢˜è¯„ä¼°å¹¶æ”¶é›†è¯¦ç»†ç»“æœ
            detailed_results = []
            all_scores = []
            
            for qa in qa_test_data[sample_id]:
                result = self._evaluate_single_question(qa, evaluator, sample_id)
                detailed_results.append(result)
                all_scores.append(result["scores"])
            
            # æ­¥éª¤6: è®¡ç®—æ±‡æ€»ç»Ÿè®¡
            summary_stats = self._calculate_summary_stats(detailed_results, sample_id)
            
            # æ¸…ç†èµ„æº
            del storage
            del evaluator
            
            return {
                "sample_id": sample_id,
                "summary": summary_stats,
                "detailed_results": detailed_results
            }
            
        except Exception as e:
            self.logger.error(f"âŒ è¯„ä¼° {sample_id} å¤±è´¥: {e}")
            return {
                "sample_id": sample_id,
                "error": str(e),
                "summary": None,
                "detailed_results": []
            }
    
    def _evaluate_single_question(self, 
                            qa: Dict[str, Any], 
                            evaluator: SemanticGraphQAEvaluator, 
                            sample_id: str) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªé—®é¢˜å¹¶è¿”å›å®Œæ•´ç»“æœ - ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜"""
        
        # ç¡®å®šæ ‡å‡†ç­”æ¡ˆ
        category = qa.get("category", 2)
        if category == 5:  # å¯¹æŠ—æ€§é—®é¢˜
            standard_answer = qa.get("adversarial_answer", "")
        else:
            standard_answer = qa.get("answer", "")
        
        # ç¡®ä¿ç­”æ¡ˆæ˜¯å­—ç¬¦ä¸²ç±»å‹
        standard_answer = str(standard_answer) if standard_answer is not None else ""
        
        # æ£€ç´¢ä¸Šä¸‹æ–‡
        context = evaluator.retrieve_context_for_question(
            question=qa["question"],
            category=category,
            conversation_id=sample_id,
            evidence=qa.get("evidence", [])
        )
        
        # ç”Ÿæˆç­”æ¡ˆ
        answer_result = evaluator.generate_answer(context)
        prediction = answer_result.get("generated_answer", "")
        
        # ç¡®ä¿é¢„æµ‹ä¹Ÿæ˜¯å­—ç¬¦ä¸²ç±»å‹
        prediction = str(prediction) if prediction is not None else ""
        
        # è®¡ç®—å¤šç§è¯„åˆ†
        scores = self._calculate_all_scores(prediction, standard_answer, category)
        
        # æ•´ç†æ£€ç´¢å†…å®¹
        retrieval_info = self._format_retrieval_info(context)
        
        return {
            "question": qa["question"],
            "category": category,
            "category_name": evaluator.category_strategies.get(category, "unknown"),
            "answer": standard_answer,
            "prediction": prediction,
            "scores": scores,
            "retrieval": retrieval_info,
            "evidence": qa.get("evidence", []),
            "evidence_recall": context.get("evidence_recall", 0.0),
            "prompt": answer_result.get("prompt", ""),
            "context_nodes_count": len(context.get("retrieved_nodes", [])),
            "context_relations_count": len(context.get("retrieved_relations", []))
        }

    # ä¿®å¤ _calculate_all_scores æ–¹æ³•ï¼Œæ·»åŠ ç±»å‹æ£€æŸ¥å’Œè½¬æ¢

    def _calculate_all_scores(self, prediction: str, answer: str, category: int) -> Dict[str, float]:
        """è®¡ç®—æ‰€æœ‰è¯„åˆ†æŒ‡æ ‡ - ä¿®å¤ç±»å‹é”™è¯¯"""
        scores = {}
        
        try:
            # 1. ç¡®ä¿è¾“å…¥éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹
            prediction = str(prediction) if prediction is not None else ""
            answer = str(answer) if answer is not None else ""
            
            # 2. F1åˆ†æ•°ï¼ˆè¯çº§åˆ«ï¼‰
            scores["f1_score"] = calculate_f1_score(prediction, answer)
            
            # 3. å¤šç­”æ¡ˆF1åˆ†æ•°ï¼ˆå¤„ç†é€—å·åˆ†éš”çš„ç­”æ¡ˆï¼‰
            scores["f1_multi"] = calculate_f1_score_multi(prediction, answer)
            
            # 4. ROUGE-Låˆ†æ•°
            scores["rouge_l"] = calculate_rouge_score(prediction, answer)
            
            # 5. ROUGE-Lå¤šç­”æ¡ˆåˆ†æ•°
            scores["rouge_l_multi"] = calculate_bleu_score(prediction, [answer])
            
            # 6. ç²¾ç¡®åŒ¹é…
            scores["exact_match"] = float(exact_match_score(prediction, answer))
            
            # 7. æ ¹æ®ç±»åˆ«é€‰æ‹©ä¸»è¦åˆ†æ•°
            if category == 5:  # å¯¹æŠ—æ€§é—®é¢˜
                pred_lower = prediction.lower()
                ans_lower = answer.lower()
                
                # æ£€æŸ¥æ˜¯å¦æ­£ç¡®æ‹’ç»å›ç­”
                rejection_phrases = ['no information available', 'not mentioned', 'not provided', 'unknown']
                pred_is_rejection = any(phrase in pred_lower for phrase in rejection_phrases)
                ans_is_rejection = any(phrase in ans_lower for phrase in rejection_phrases)
                
                if pred_is_rejection and ans_is_rejection:
                    scores["main_score"] = 1.0
                elif not pred_is_rejection and not ans_is_rejection:
                    scores["main_score"] = scores["f1_score"]
                else:
                    scores["main_score"] = 0.0
                    
            elif category == 1:  # å¤šè·³æ¨ç†
                scores["main_score"] = scores["f1_multi"]
            else:  # å…¶ä»–ç±»åˆ«
                scores["main_score"] = scores["f1_score"]
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—è¯„åˆ†å¤±è´¥: {e}")
            self.logger.error(f"é¢„æµ‹ç±»å‹: {type(prediction)}, å€¼: {prediction}")
            self.logger.error(f"ç­”æ¡ˆç±»å‹: {type(answer)}, å€¼: {answer}")
            
            # è¿”å›é»˜è®¤åˆ†æ•°
            scores = {
                "f1_score": 0.0,
                "f1_multi": 0.0,
                "rouge_l": 0.0,
                "rouge_l_multi": 0.0,
                "exact_match": 0.0,
                "main_score": 0.0
            }
        
        return scores
    

    def _format_retrieval_info(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¼å¼åŒ–æ£€ç´¢ä¿¡æ¯"""
        
        # èŠ‚ç‚¹ä¿¡æ¯
        nodes_info = []
        for node in context.get("retrieved_nodes", []):
            nodes_info.append({
                "uid": node.get("uid", ""),
                "data_type": node.get("data_type", "unknown"),
                "content": node.get("content", "")[:150],  # é™åˆ¶é•¿åº¦
                "score": node.get("score", 0.0)
            })
        
        # å…³ç³»ä¿¡æ¯
        relations_info = []
        for rel in context.get("retrieved_relations", []):
            relations_info.append({
                "source": rel.get("source", ""),
                "target": rel.get("target", ""),
                "type": rel.get("type", "UNKNOWN"),
                "properties": rel.get("properties", {})
            })
        
        # åŸå§‹ä¸Šä¸‹æ–‡å†…å®¹ï¼ˆæˆªæ–­ï¼‰
        raw_context = context.get("context_text", "")
        if len(raw_context) > 500:
            raw_context = raw_context[:500] + "...[TRUNCATED]"
        
        return {
            "strategy": context.get("strategy", "unknown"),
            "nodes": nodes_info,
            "relations": relations_info,
            "raw_context": raw_context,
            "nodes_count": len(nodes_info),
            "relations_count": len(relations_info)
        }
    
    def _calculate_summary_stats(self, detailed_results: List[Dict], sample_id: str) -> Dict[str, Any]:
        """è®¡ç®—æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
        
        if not detailed_results:
            return {"error": "No results to summarize"}
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        by_category = {}
        all_scores = []
        
        for result in detailed_results:
            category = result["category"]
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(result)
            all_scores.append(result["scores"])
        
        # æ€»ä½“ç»Ÿè®¡
        overall_stats = {
            "total_questions": len(detailed_results),
            "avg_f1_score": float(np.mean([s["f1_score"] for s in all_scores])),
            "avg_f1_multi": float(np.mean([s["f1_multi"] for s in all_scores])),
            "avg_rouge_l": float(np.mean([s["rouge_l"] for s in all_scores])),
            "avg_exact_match": float(np.mean([s["exact_match"] for s in all_scores])),
            "avg_main_score": float(np.mean([s["main_score"] for s in all_scores])),
            "avg_evidence_recall": float(np.mean([r["evidence_recall"] for r in detailed_results])),
            "avg_context_nodes": float(np.mean([r["context_nodes_count"] for r in detailed_results])),
            "avg_context_relations": float(np.mean([r["context_relations_count"] for r in detailed_results]))
        }
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        category_stats = {}
        for category, results in by_category.items():
            cat_scores = [r["scores"] for r in results]
            category_name = results[0]["category_name"]
            
            category_stats[f"category_{category}"] = {
                "category_name": category_name,
                "questions_count": len(results),
                "avg_f1_score": float(np.mean([s["f1_score"] for s in cat_scores])),
                "avg_f1_multi": float(np.mean([s["f1_multi"] for s in cat_scores])),
                "avg_rouge_l": float(np.mean([s["rouge_l"] for s in cat_scores])),
                "avg_exact_match": float(np.mean([s["exact_match"] for s in cat_scores])),
                "avg_main_score": float(np.mean([s["main_score"] for s in cat_scores])),
                "std_main_score": float(np.std([s["main_score"] for s in cat_scores]))
            }
        
        return {
            "sample_id": sample_id,
            "evaluation_timestamp": datetime.now().isoformat(),
            "llm_model": self.llm_client.model_name,
            "overall": overall_stats,
            "by_category": category_stats
        }
    
    def save_sample_results(self, sample_result: Dict[str, Any], output_format: str = "comprehensive") -> str:
        """ä¿å­˜æ ·æœ¬ç»“æœåˆ°æ–‡ä»¶"""
        
        sample_id = sample_result["sample_id"]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if output_format == "comprehensive":
            # å®Œæ•´æ ¼å¼ï¼šåŒ…å«æ‰€æœ‰ä¿¡æ¯
            output_file = self.output_dir / f"evaluation_{sample_id}_{timestamp}.json"
            
            # æ„å»ºè¾“å‡ºç»“æ„
            output_data = {
                "evaluation_summary": sample_result.get("summary", {}),
                "questions": sample_result.get("detailed_results", [])
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
                
        elif output_format == "simple":
            # ç®€åŒ–æ ¼å¼ï¼šåªåŒ…å«æ ¸å¿ƒä¿¡æ¯
            output_file = self.output_dir / f"simple_{sample_id}_{timestamp}.json"
            
            simple_results = []
            for result in sample_result.get("detailed_results", []):
                simple_results.append({
                    "question": result["question"],
                    "category": result["category"],
                    "answer": result["answer"],
                    "prediction": result["prediction"],
                    "score": result["scores"]["main_score"],
                    "retrieve": {
                        "nodes_count": result["retrieval"]["nodes_count"],
                        "relations_count": result["retrieval"]["relations_count"],
                        "strategy": result["retrieval"]["strategy"]
                    },
                    "evidence": result["evidence"]
                })
            
            output_data = {
                "summary": sample_result.get("summary", {}),
                "results": simple_results
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ’¾ {sample_id} ç»“æœå·²ä¿å­˜: {output_file}")
        return str(output_file)
    
    def evaluate_multiple_samples(self, 
                                 sample_ids: List[str],
                                 raw_dataset_file: str = "benchmark/dataset/locomo/locomo10.json",
                                 extracted_dataset_file: str = "benchmark/dataset/locomo/extraction/locomo_extracted_full_dataset.json",
                                 save_format: str = "comprehensive") -> Dict[str, Any]:
        """è¯„ä¼°å¤šä¸ªæ ·æœ¬"""
        
        self.logger.info(f"ğŸš€ å¼€å§‹ç‹¬ç«‹è¯„ä¼° {len(sample_ids)} ä¸ªæ ·æœ¬")
        
        all_results = []
        failed_samples = []
        
        for sample_id in sample_ids:
            self.logger.info(f"\n{'='*50}")
            self.logger.info(f"è¯„ä¼°æ ·æœ¬ {sample_id} ({sample_ids.index(sample_id)+1}/{len(sample_ids)})")
            self.logger.info(f"{'='*50}")
            
            # è¯„ä¼°å•ä¸ªæ ·æœ¬
            sample_result = self.evaluate_single_sample(
                sample_id=sample_id,
                raw_dataset_file=raw_dataset_file,
                extracted_dataset_file=extracted_dataset_file
            )
            
            if "error" not in sample_result:
                all_results.append(sample_result)
                
                # ä¿å­˜å•ä¸ªæ ·æœ¬ç»“æœ
                self.save_sample_results(sample_result, save_format)
                
                # æ˜¾ç¤ºç®€è¦ç»“æœ
                if sample_result.get("summary"):
                    summary = sample_result["summary"]["overall"]
                    self.logger.info(f"âœ… {sample_id} å®Œæˆ:")
                    self.logger.info(f"   é—®é¢˜æ•°: {summary['total_questions']}")
                    self.logger.info(f"   ä¸»è¦åˆ†æ•°: {summary['avg_main_score']:.4f}")
                    self.logger.info(f"   F1åˆ†æ•°: {summary['avg_f1_score']:.4f}")
                    self.logger.info(f"   ç²¾ç¡®åŒ¹é…: {summary['avg_exact_match']:.4f}")
                    
            else:
                failed_samples.append(sample_id)
                self.logger.error(f"âŒ {sample_id} è¯„ä¼°å¤±è´¥")
        
        # è®¡ç®—è·¨æ ·æœ¬æ±‡æ€»
        if all_results:
            aggregated_summary = self._calculate_cross_sample_summary(all_results)
            
            # ä¿å­˜æ±‡æ€»ç»“æœ
            self._save_aggregated_results(aggregated_summary, sample_ids, failed_samples, save_format)
            
            return {
                "aggregated_summary": aggregated_summary,
                "individual_results": all_results,
                "failed_samples": failed_samples
            }
        else:
            self.logger.error("âŒ æ²¡æœ‰æˆåŠŸçš„è¯„ä¼°ç»“æœ")
            return {"error": "No successful evaluations", "failed_samples": failed_samples}
    
    def _calculate_cross_sample_summary(self, all_results: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—è·¨æ ·æœ¬æ±‡æ€»ç»Ÿè®¡"""
        
        # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„æ€»ä½“æŒ‡æ ‡
        overall_metrics = []
        category_metrics = {i: [] for i in range(1, 6)}
        
        for sample_result in all_results:
            if sample_result.get("summary") and sample_result["summary"].get("overall"):
                overall_metrics.append(sample_result["summary"]["overall"])
                
                # æŒ‰ç±»åˆ«æ”¶é›†
                by_category = sample_result["summary"].get("by_category", {})
                for cat_key, cat_data in by_category.items():
                    category_num = int(cat_key.split("_")[1])
                    category_metrics[category_num].append(cat_data["avg_main_score"])
        
        # è®¡ç®—è·¨æ ·æœ¬ç»Ÿè®¡
        cross_sample_stats = {
            "evaluation_info": {
                "timestamp": datetime.now().isoformat(),
                "llm_model": self.llm_client.model_name,
                "total_samples": len(all_results),
                "evaluation_mode": "independent_samples"
            },
            "cross_sample_overall": {},
            "cross_sample_by_category": {}
        }
        
        # æ€»ä½“è·¨æ ·æœ¬ç»Ÿè®¡
        if overall_metrics:
            cross_sample_stats["cross_sample_overall"] = {
                "avg_f1_score": float(np.mean([m["avg_f1_score"] for m in overall_metrics])),
                "std_f1_score": float(np.std([m["avg_f1_score"] for m in overall_metrics])),
                "avg_f1_multi": float(np.mean([m["avg_f1_multi"] for m in overall_metrics])),
                "std_f1_multi": float(np.std([m["avg_f1_multi"] for m in overall_metrics])),
                "avg_rouge_l": float(np.mean([m["avg_rouge_l"] for m in overall_metrics])),
                "std_rouge_l": float(np.std([m["avg_rouge_l"] for m in overall_metrics])),
                "avg_exact_match": float(np.mean([m["avg_exact_match"] for m in overall_metrics])),
                "std_exact_match": float(np.std([m["avg_exact_match"] for m in overall_metrics])),
                "avg_main_score": float(np.mean([m["avg_main_score"] for m in overall_metrics])),
                "std_main_score": float(np.std([m["avg_main_score"] for m in overall_metrics])),
                "total_questions": sum([m["total_questions"] for m in overall_metrics])
            }
        
        # æŒ‰ç±»åˆ«è·¨æ ·æœ¬ç»Ÿè®¡
        strategy_names = {1: "multi_hop", 2: "single_hop", 3: "temporal", 4: "open_domain", 5: "adversarial"}
        for category, scores in category_metrics.items():
            if scores:
                cross_sample_stats["cross_sample_by_category"][f"category_{category}"] = {
                    "category_name": strategy_names.get(category, "unknown"),
                    "samples_count": len(scores),
                    "avg_score": float(np.mean(scores)),
                    "std_score": float(np.std(scores)),
                    "min_score": float(np.min(scores)),
                    "max_score": float(np.max(scores))
                }
        
        return cross_sample_stats
    
    def _save_aggregated_results(self, 
                               aggregated_summary: Dict[str, Any],
                               sample_ids: List[str],
                               failed_samples: List[str],
                               save_format: str):
        """ä¿å­˜è·¨æ ·æœ¬æ±‡æ€»ç»“æœ"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. JSONæ ¼å¼æ±‡æ€»
        summary_file = self.output_dir / f"cross_sample_summary_{timestamp}.json"
        
        summary_data = {
            **aggregated_summary,
            "sample_info": {
                "evaluated_samples": sample_ids,
                "failed_samples": failed_samples,
                "success_rate": len(sample_ids) - len(failed_samples) / len(sample_ids) if sample_ids else 0
            }
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        # 2. å¯è¯»æ€§æŠ¥å‘Š
        report_file = self.output_dir / f"evaluation_report_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=== Independent Sample QA Evaluation Report ===\n\n")
            
            # åŸºæœ¬ä¿¡æ¯
            info = aggregated_summary["evaluation_info"]
            f.write(f"Timestamp: {info['timestamp']}\n")
            f.write(f"LLM Model: {info['llm_model']}\n")
            f.write(f"Total Samples: {info['total_samples']}\n")
            f.write(f"Failed Samples: {len(failed_samples)}\n")
            f.write(f"Sample IDs: {', '.join(sample_ids)}\n\n")
            
            # æ€»ä½“ç»“æœ
            if "cross_sample_overall" in aggregated_summary:
                overall = aggregated_summary["cross_sample_overall"]
                f.write("=== Cross-Sample Overall Results ===\n")
                f.write(f"Average F1 Score: {overall['avg_f1_score']:.4f} Â± {overall['std_f1_score']:.4f}\n")
                f.write(f"Average F1 Multi: {overall['avg_f1_multi']:.4f} Â± {overall['std_f1_multi']:.4f}\n")
                f.write(f"Average ROUGE-L: {overall['avg_rouge_l']:.4f} Â± {overall['std_rouge_l']:.4f}\n")
                f.write(f"Average Exact Match: {overall['avg_exact_match']:.4f} Â± {overall['std_exact_match']:.4f}\n")
                f.write(f"Average Main Score: {overall['avg_main_score']:.4f} Â± {overall['std_main_score']:.4f}\n")
                f.write(f"Total Questions Evaluated: {overall['total_questions']}\n\n")
            
            # æŒ‰ç±»åˆ«ç»“æœ
            if "cross_sample_by_category" in aggregated_summary:
                f.write("=== Results by Category ===\n")
                for cat_key, cat_data in aggregated_summary["cross_sample_by_category"].items():
                    category_num = cat_key.split("_")[1]
                    f.write(f"Category {category_num} ({cat_data['category_name']}):\n")
                    f.write(f"  Samples: {cat_data['samples_count']}\n")
                    f.write(f"  Average: {cat_data['avg_score']:.4f} Â± {cat_data['std_score']:.4f}\n")
                    f.write(f"  Range: [{cat_data['min_score']:.4f}, {cat_data['max_score']:.4f}]\n\n")
        
        self.logger.info(f"ğŸ“Š è·¨æ ·æœ¬æ±‡æ€»ç»“æœå·²ä¿å­˜:")
        self.logger.info(f"  - JSON: {summary_file}")
        self.logger.info(f"  - æŠ¥å‘Š: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Independent Sample QA Evaluation - Simplified Output")
    parser.add_argument("--model", default="deepseek-chat", help="LLM model name")
    parser.add_argument("--samples", nargs="+", default=["conv-26", "conv-30"], 
                       help="Sample IDs to evaluate")
    parser.add_argument("--output_dir", default="benchmark/task_eval/results/simplified", 
                       help="Output directory")
    parser.add_argument("--format", choices=["comprehensive", "simple"], default="comprehensive",
                       help="Output format")
    parser.add_argument("--debug", action="store_true", help="Enable debug logging")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_level = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(level=log_level, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
    llm_client = LLMClient(model_name=args.model)
    
    # åˆ›å»ºç‹¬ç«‹è¯„ä¼°å™¨
    evaluator = IndependentSampleEvaluator(
        llm_client=llm_client,
        output_dir=args.output_dir
    )
    
    # æ‰§è¡Œè¯„ä¼°
    results = evaluator.evaluate_multiple_samples(
        sample_ids=args.samples,
        save_format=args.format
    )
    
    if "aggregated_summary" in results:
        summary = results["aggregated_summary"]["cross_sample_overall"]
        print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸè¯„ä¼°: {len(results['individual_results'])}/{len(args.samples)} ä¸ªæ ·æœ¬")
        print(f"ğŸ“ˆ è·¨æ ·æœ¬å¹³å‡åˆ†æ•°: {summary['avg_main_score']:.4f} Â± {summary['std_main_score']:.4f}")
        print(f"ğŸ“ˆ è·¨æ ·æœ¬F1åˆ†æ•°: {summary['avg_f1_score']:.4f} Â± {summary['std_f1_score']:.4f}")
        print(f"ğŸ“ˆ è·¨æ ·æœ¬ç²¾ç¡®åŒ¹é…: {summary['avg_exact_match']:.4f} Â± {summary['std_exact_match']:.4f}")


if __name__ == "__main__":
    main()