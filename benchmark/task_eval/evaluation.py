import regex
import json
import string
import unicodedata
from typing import List, Dict, Any, Optional
import numpy as np
from collections import Counter
import os
import asyncio
import time
import logging

# è¯„ä¼°ç›¸å…³åº“
from bert_score import score
from nltk.stem import PorterStemmer
from rouge import Rouge
import nltk
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer
from scipy.spatial.distance import cosine
from sentence_transformers import SentenceTransformer
from pydantic import BaseModel, Field

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
from llm_utils.llm_client import LLMClient

# å°è¯•ä¸‹è½½NLTKèµ„æº
try:
    nltk.download("wordnet", quiet=True)
    nltk.download("punkt", quiet=True)
except Exception as e:
    logging.warning(f"Failed to download NLTK resources: {e}")

# åˆå§‹åŒ–è¯å¹²æå–å™¨
ps = PorterStemmer()

# é•¿åº¦é˜ˆå€¼
LENGTH_THRESHOLD = 5

# å…¨å±€è¯­ä¹‰æ¨¡å‹
_sentence_model = None

def get_sentence_model():
    """è·å–å…¨å±€è¯­ä¹‰æ¨¡å‹"""
    global _sentence_model
    if _sentence_model is None:
        try:
            # å¯ä»¥æ ¹æ®éœ€è¦æ›´æ”¹æ¨¡å‹
            model_name = "all-MiniLM-L6-v2"  # æˆ– "Qwen/Qwen3-Embedding-0.6B"
            _sentence_model = SentenceTransformer(model_name)
            logging.info(f"SentenceTransformer model {model_name} loaded successfully.")
        except Exception as e:
            logging.error(f"Failed to load SentenceTransformer model: {e}")
            _sentence_model = None
    return _sentence_model

# ================================
# LLMè¯„ä¼°ç›¸å…³ç±»å’Œå‡½æ•°
# ================================

class LLMGrade(BaseModel):
    llm_judgment: str = Field(description="CORRECT or WRONG")
    llm_reasoning: str = Field(description="Explain why the answer is correct or incorrect.")

async def llm_grader_async(llm_client: LLMClient, question: str, gold_answer: str, response: str) -> bool:
    """
    LLM-as-a-Judgeè¯„ä¼°å™¨ï¼ˆé€‚é…LLMClientï¼‰
    
    Args:
        llm_client: LLMClientå®ä¾‹
        question: é—®é¢˜
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
        
    Returns:
        bool: æ˜¯å¦æ­£ç¡®
    """
    accuracy_prompt = f"""
    Your task is to label an answer to a question as 'CORRECT' or 'WRONG'. You will be given the following data:
        (1) a question (posed by one user to another user),
        (2) a 'gold' (ground truth) answer,
        (3) a generated answer
    which you will score as CORRECT/WRONG.

    You are an expert grader that determines if answers to questions match a gold standard answer.
    Be generous with your grading - focus on whether the core meaning and facts are correct.

    The point of the question is to ask about something one user should know about the other user based on their prior conversations.
    The gold answer will usually be a concise and short answer that includes the referenced topic, for example:
    Question: Do you remember what I got the last time I went to Hawaii?
    Gold answer: A shell necklace
    The generated answer might be much longer, but you should be generous with your grading - as long as it touches on the same topic as the gold answer, it should be counted as CORRECT.

    For time related questions, the gold answer will be a specific date, month, year, etc. The generated answer might be much longer or use relative time references (like "last Tuesday" or "next month"), but you should be generous with your grading - as long as it refers to the same date or time period as the gold answer, it should be counted as CORRECT. Even if the format differs (e.g., "May 7th" vs "7 May"), consider it CORRECT if it's the same date.

    Now it's time for the real question:
    Question: {question}
    Gold answer: {gold_answer}
    Generated answer: {response}

    First, provide a short (one sentence) explanation of your reasoning, then finish with CORRECT or WRONG.
    Do NOT include both CORRECT and WRONG in your response, or it will break the evaluation script.

    Just return the label CORRECT or WRONG in a json format with the key as "label".
    """

    try:
        # ä½¿ç”¨LLMClientçš„generate_answeræ–¹æ³•
        llm_response = llm_client.generate_answer(
            prompt=accuracy_prompt,
            temperature=0,
            max_tokens=100,  # è¶³å¤ŸçŸ­çš„å›å¤
            json_format=True  # å¦‚æœæ”¯æŒJSONæ ¼å¼
        )
        
        # å°è¯•æå–JSON
        try:
            if '{' in llm_response and '}' in llm_response:
                start = llm_response.find('{')
                end = llm_response.rfind('}') + 1
                json_str = llm_response[start:end]
                result = json.loads(json_str)
                label = result.get("label", "").strip().lower()
                return label == "correct"
            else:
                # å›é€€åˆ°ç®€å•æ–‡æœ¬åŒ¹é…
                return "correct" in llm_response.lower()
        except json.JSONDecodeError:
            # JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬åŒ¹é…
            logging.warning(f"JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬åŒ¹é…: {llm_response}")
            return "correct" in llm_response.lower()
            
    except Exception as e:
        logging.error(f"LLM grader failed: {e}")
        return False

def llm_grader(llm_client: LLMClient, question: str, gold_answer: str, response: str) -> bool:
    """
    åŒæ­¥ç‰ˆæœ¬çš„LLMè¯„ä¼°å™¨ï¼ˆé€‚é…LLMClientï¼‰
    
    Args:
        llm_client: LLMClientå®ä¾‹
        question: é—®é¢˜
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
        
    Returns:
        bool: æ˜¯å¦æ­£ç¡®
    """
    accuracy_prompt = f"""
    You are an expert grader that determines if answers to questions match a gold standard answer.
    Be generous with your grading - focus on whether the core meaning and facts are correct.

    Your task is to evaluate if the generated answer is CORRECT or WRONG compared to the gold answer.

    Question: {question}
    Gold answer: {gold_answer}
    Generated answer: {response}

    Consider the answer CORRECT if:
    - It contains the same key information as the gold answer
    - The meaning is equivalent even if wording differs
    - For time questions, the time period matches even if format differs
    - For factual questions, the core facts are accurate

    Return only a JSON with "label" key containing either "CORRECT" or "WRONG".
    Example: {{"label": "CORRECT"}} or {{"label": "WRONG"}}
    """

    try:
        # ä½¿ç”¨LLMClientçš„generate_answeræ–¹æ³•
        llm_response = llm_client.generate_answer(
            prompt=accuracy_prompt,
            temperature=0.0,  # ç¡®ä¿ä¸€è‡´æ€§
            max_tokens=50,    # è¶³å¤Ÿè¿”å›JSON
        )
        
        # å°è¯•æå–JSON
        try:
            if '{' in llm_response and '}' in llm_response:
                start = llm_response.find('{')
                end = llm_response.rfind('}') + 1
                json_str = llm_response[start:end]
                result = json.loads(json_str)
                label = result.get("label", "").strip().upper()
                return label == "CORRECT"
            else:
                # å›é€€åˆ°å…³é”®è¯åŒ¹é…
                llm_response_upper = llm_response.upper()
                if "CORRECT" in llm_response_upper and "WRONG" not in llm_response_upper:
                    return True
                elif "WRONG" in llm_response_upper and "CORRECT" not in llm_response_upper:
                    return False
                else:
                    # å¦‚æœåŒæ—¶åŒ…å«æˆ–éƒ½ä¸åŒ…å«ï¼Œä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…
                    return any(word in llm_response.lower() for word in ["correct", "right", "accurate", "yes"])
                    
        except json.JSONDecodeError as e:
            # JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬åŒ¹é…
            logging.warning(f"JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬åŒ¹é…: {llm_response}, é”™è¯¯: {e}")
            llm_response_upper = llm_response.upper()
            
            # æ›´å¥å£®çš„æ–‡æœ¬åŒ¹é…
            if "CORRECT" in llm_response_upper:
                return True
            elif "WRONG" in llm_response_upper:
                return False
            else:
                # æœ€åçš„å›é€€ç­–ç•¥
                positive_words = ["correct", "right", "accurate", "yes", "true", "match"]
                negative_words = ["wrong", "incorrect", "false", "no", "mismatch"]
                
                response_lower = llm_response.lower()
                positive_count = sum(1 for word in positive_words if word in response_lower)
                negative_count = sum(1 for word in negative_words if word in response_lower)
                
                return positive_count > negative_count
                
    except Exception as e:
        logging.error(f"LLM grader sync failed: {e}")
        return False

def llm_grader_batch(llm_client: LLMClient, 
                    questions: List[str], 
                    gold_answers: List[str], 
                    responses: List[str]) -> List[bool]:
    """
    æ‰¹é‡LLMè¯„ä¼°å™¨ï¼ˆé€‚é…LLMClientï¼‰
    
    Args:
        llm_client: LLMClientå®ä¾‹
        questions: é—®é¢˜åˆ—è¡¨
        gold_answers: æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨
        responses: ç”Ÿæˆç­”æ¡ˆåˆ—è¡¨
        
    Returns:
        List[bool]: è¯„ä¼°ç»“æœåˆ—è¡¨
    """
    if not (len(questions) == len(gold_answers) == len(responses)):
        raise ValueError("è¾“å…¥åˆ—è¡¨é•¿åº¦ä¸ä¸€è‡´")
    
    results = []
    
    for i, (question, gold_answer, response) in enumerate(zip(questions, gold_answers, responses)):
        logging.debug(f"æ‰¹é‡è¯„ä¼° {i+1}/{len(questions)}")
        result = llm_grader(llm_client, question, gold_answer, response)
        results.append(result)
    
    return results

def calculate_llm_judgment(llm_client: LLMClient, 
                        question: str, 
                        gold_answer: str, 
                        response: str,
                        num_runs: int = 1) -> Dict[str, Any]:
    """
    è®¡ç®—LLMåˆ¤æ–­åˆ†æ•°ï¼ˆé€‚é…LLMClientï¼‰
    
    Args:
        llm_client: LLMClientå®ä¾‹
        question: é—®é¢˜
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
        num_runs: è¿è¡Œæ¬¡æ•°ï¼ˆç”¨äºä¸€è‡´æ€§æ£€æŸ¥ï¼‰
        
    Returns:
        åŒ…å«LLMåˆ¤æ–­ç»“æœçš„å­—å…¸
    """
    judgments = []
    
    for i in range(num_runs):
        try:
            judgment = llm_grader(llm_client, question, gold_answer, response)
            judgments.append(judgment)
            logging.debug(f"LLMåˆ¤æ–­ {i+1}/{num_runs}: {judgment}")
        except Exception as e:
            logging.error(f"LLM judgment {i+1} failed: {e}")
            judgments.append(False)
    
    if not judgments:
        return {
            "judgments": [],
            "accuracy": 0.0,
            "num_runs": num_runs,
            "consistency": False,
            "error": "æ‰€æœ‰åˆ¤æ–­éƒ½å¤±è´¥äº†"
        }
    
    accuracy = sum(judgments) / len(judgments)
    consistency = len(set(judgments)) == 1  # æ‰€æœ‰åˆ¤æ–­æ˜¯å¦ä¸€è‡´
    
    return {
        "judgments": judgments,
        "accuracy": accuracy,
        "num_runs": num_runs,
        "consistency": consistency,
        "confidence": "high" if consistency else "low"
    }

# æ›´æ–°ç»¼åˆè¯„ä¼°å‡½æ•°ä»¥æ”¯æŒLLMClient
def evaluate_answer_comprehensive(question: str,
                                gold_answer: str,
                                predicted_answer: str,
                                context: str = "",
                                llm_client: Optional[LLMClient] = None,
                                include_llm_judgment: bool = False,
                                evaluation_options: Optional[List[str]] = None,
                                llm_runs: int = 1) -> Dict[str, Any]:
    """
    ç»¼åˆç­”æ¡ˆè¯„ä¼°æ¥å£ï¼ˆé€‚é…LLMClientï¼‰
    
    Args:
        question: é—®é¢˜
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        predicted_answer: é¢„æµ‹ç­”æ¡ˆ
        context: ä¸Šä¸‹æ–‡
        llm_client: LLMClientå®ä¾‹ï¼ˆå¯é€‰ï¼‰
        include_llm_judgment: æ˜¯å¦åŒ…å«LLMåˆ¤æ–­
        evaluation_options: è¯„ä¼°é€‰é¡¹
        llm_runs: LLMè¯„ä¼°è¿è¡Œæ¬¡æ•°
        
    Returns:
        ç»¼åˆè¯„ä¼°ç»“æœ
    """
    if evaluation_options is None:
        evaluation_options = ["lexical", "semantic"]
    
    # åŸºç¡€æŒ‡æ ‡
    result = calculate_comprehensive_metrics(
        gold_answer, predicted_answer, context, evaluation_options
    )
    
    # LLMåˆ¤æ–­ï¼ˆå¦‚æœå¯ç”¨ä¸”æä¾›äº†å®¢æˆ·ç«¯ï¼‰
    if include_llm_judgment and llm_client is not None:
        try:
            llm_result = calculate_llm_judgment(
                llm_client, question, gold_answer, predicted_answer, llm_runs
            )
            result["llm_judgment"] = llm_result
        except Exception as e:
            logging.error(f"LLMåˆ¤æ–­å¤±è´¥: {e}")
            result["llm_judgment"] = {
                "error": str(e),
                "accuracy": 0.0,
                "num_runs": llm_runs,
                "consistency": False
            }
    
    # è½¬æ¢numpyç±»å‹
    result = convert_numpy_types(result)
    
    return result

# æ·»åŠ æµ‹è¯•å‡½æ•°
def test_llm_grader(llm_client: LLMClient):
    """
    æµ‹è¯•LLMè¯„ä¼°å™¨


    """
    test_cases = [
        {
            "question": "What is Caroline's relationship status?",
            "gold_answer": "single",
            "response": "Caroline is single",
            "expected": True
        },
        {
            "question": "What did they eat for dinner?",
            "gold_answer": "pizza",
            "response": "They had Chinese food",
            "expected": False
        },
        {
            "question": "When did they meet?",
            "gold_answer": "May 7, 2023",
            "response": "They met on 7 May 2023",
            "expected": True
        }
    ]
    
    print("ğŸ§ª æµ‹è¯•LLMè¯„ä¼°å™¨...")
    
    for i, case in enumerate(test_cases, 1):
        print(f"\næµ‹è¯•æ¡ˆä¾‹ {i}:")
        print(f"é—®é¢˜: {case['question']}")
        print(f"æ ‡å‡†ç­”æ¡ˆ: {case['gold_answer']}")
        print(f"ç”Ÿæˆç­”æ¡ˆ: {case['response']}")
        print(f"æœŸæœ›ç»“æœ: {case['expected']}")
        
        result = llm_grader(
            llm_client, 
            case['question'], 
            case['gold_answer'], 
            case['response']
        )
        
        print(f"å®é™…ç»“æœ: {result}")
        print(f"åŒ¹é…æœŸæœ›: {'âœ…' if result == case['expected'] else 'âŒ'}")
    
    print("\nğŸ¯ æ‰¹é‡æµ‹è¯•...")
    questions = [case['question'] for case in test_cases]
    gold_answers = [case['gold_answer'] for case in test_cases]
    responses = [case['response'] for case in test_cases]
    
    batch_results = llm_grader_batch(llm_client, questions, gold_answers, responses)
    print(f"æ‰¹é‡ç»“æœ: {batch_results}")

# async def llm_grader(llm_client:LLMClient, question: str, gold_answer: str, response: str) -> bool:
#     """
#     LLM-as-a-Judgeè¯„ä¼°å™¨ï¼ˆLoCoMoé£æ ¼ï¼‰
    
#     Args:
#         llm_client: LLMå®¢æˆ·ç«¯ï¼ˆOpenAIæˆ–å…¶ä»–ï¼‰
#         question: é—®é¢˜
#         gold_answer: æ ‡å‡†ç­”æ¡ˆ
#         response: ç”Ÿæˆçš„ç­”æ¡ˆ
        
#     Returns:
#         bool: æ˜¯å¦æ­£ç¡®
#     """
#     system_prompt = """
#     You are an expert grader that determines if answers to questions match a gold standard answer
#     """

#     accuracy_prompt = f"""
#     Your task is to label an answer to a question as 'CORRECT' or 'WRONG'. You will be given the following data:
#         (1) a question (posed by one user to another user),
#         (2) a 'gold' (ground truth) answer,
#         (3) a generated answer
#     which you will score as CORRECT/WRONG.

#     The point of the question is to ask about something one user should know about the other user based on their prior conversations.
#     The gold answer will usually be a concise and short answer that includes the referenced topic, for example:
#     Question: Do you remember what I got the last time I went to Hawaii?
#     Gold answer: A shell necklace
#     The generated answer might be much longer, but you should be generous with your grading - as long as it touches on the same topic as the gold answer, it should be counted as CORRECT.

#     For time related questions, the gold answer will be a specific date, month, year, etc. The generated answer might be much longer or use relative time references (like "last Tuesday" or "next month"), but you should be generous with your grading - as long as it refers to the same date or time period as the gold answer, it should be counted as CORRECT. Even if the format differs (e.g., "May 7th" vs "7 May"), consider it CORRECT if it's the same date.

#     Now it's time for the real question:
#     Question: {question}
#     Gold answer: {gold_answer}
#     Generated answer: {response}

#     First, provide a short (one sentence) explanation of your reasoning, then finish with CORRECT or WRONG.
#     Do NOT include both CORRECT and WRONG in your response, or it will break the evaluation script.

#     Just return the label CORRECT or WRONG in a json format with the key as "label".
#     """

#     try:
#         response = await llm_client.chat.completions.create(
#             model="gpt-4o-mini",  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ¨¡å‹
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": accuracy_prompt},
#             ],
#             temperature=0,
#         )
#         message_content = response.choices[0].message.content
#         label = json.loads(message_content)["label"]
#         parsed = LLMGrade(llm_judgment=label, llm_reasoning="")
        
#         return parsed.llm_judgment.strip().lower() == "correct"
#     except Exception as e:
#         logging.error(f"LLM grader failed: {e}")
#         return False

# def llm_grader(llm_client:LLMClient, question: str, gold_answer: str, response: str) -> bool:
#     """
#     åŒæ­¥ç‰ˆæœ¬çš„LLMè¯„ä¼°å™¨ï¼ˆç”¨äºéå¼‚æ­¥ç¯å¢ƒï¼‰
#     """
#     system_prompt = """
#     You are an expert grader that determines if answers to questions match a gold standard answer.
#     Be generous with your grading - focus on whether the core meaning and facts are correct.
#     """

#     accuracy_prompt = f"""
#     Question: {question}
#     Gold answer: {gold_answer}
#     Generated answer: {response}

#     Rate this as CORRECT or WRONG. Return only a JSON with "label" key.
#     """

#     try:
#         # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„LLMå®¢æˆ·ç«¯è°ƒæ•´
#         if hasattr(llm_client, 'generate_answer'):
#             # é€‚é…æˆ‘ä»¬çš„LLMClient
#             full_prompt = f"{system_prompt}\n\n{accuracy_prompt}"
#             llm_response = llm_client.generate_answer(full_prompt, temperature=0, max_tokens=50)
            
#             # å°è¯•æå–JSON
#             try:
#                 if '{' in llm_response and '}' in llm_response:
#                     start = llm_response.find('{')
#                     end = llm_response.rfind('}') + 1
#                     json_str = llm_response[start:end]
#                     result = json.loads(json_str)
#                     return result.get("label", "").strip().lower() == "correct"
#                 else:
#                     # å›é€€åˆ°ç®€å•æ–‡æœ¬åŒ¹é…
#                     return "correct" in llm_response.lower()
#             except:
#                 return "correct" in llm_response.lower()
#         else:
#             logging.warning("LLM client not compatible, skipping LLM evaluation")
#             return False
#     except Exception as e:
#         logging.error(f"Sync LLM grader failed: {e}")
#         return False

# ================================
# ç°æœ‰å‡½æ•°ä¿æŒä¸å˜ï¼ˆå·²ç»ä¿®å¤è¿‡çš„ï¼‰
# ================================

class SimpleTokenizer(object):
    """ç®€å•çš„åˆ†è¯å™¨ç±»"""
    ALPHA_NUM = r'[\p{L}\p{N}\p{M}]+'
    NON_WS = r'[^\p{Z}\p{C}]'

    def __init__(self):
        self._regexp = regex.compile(
            '(%s)|(%s)' % (self.ALPHA_NUM, self.NON_WS),
            flags=regex.IGNORECASE + regex.UNICODE + regex.MULTILINE
        )

    def tokenize(self, text, uncased=False):
        matches = [m for m in self._regexp.finditer(text)]
        if uncased:
            tokens = [m.group().lower() for m in matches]
        else:
            tokens = [m.group() for m in matches]
        return tokens

def _normalize(text):
    """æ–‡æœ¬æ ‡å‡†åŒ–"""
    return unicodedata.normalize('NFD', text)

def normalize_answer(s):
    """ç­”æ¡ˆæ ‡å‡†åŒ–å‡½æ•°ï¼ˆå·²ä¿®å¤ï¼‰"""
    if s is None:
        s = ""
    elif not isinstance(s, str):
        s = str(s)
    
    s = s.replace(',', "")
    
    def remove_articles(text):
        return regex.sub(r'\b(a|an|the|and)\b', ' ', text)

    def white_space_fix(text):
        return ' '.join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))

def exact_match_score(gold_answer: str, response: str) -> bool:
    """
    ç²¾ç¡®åŒ¹é…å¾—åˆ†ï¼ˆç»Ÿä¸€å‚æ•°é¡ºåºï¼‰
    Args:
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
    Returns:
        bool: æ˜¯å¦å®Œå…¨åŒ¹é…
    """
    response = str(response) if response is not None else ""
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    
    response = normalize_answer(response)
    gold_answer = normalize_answer(gold_answer)
    return set(response.split()) == set(gold_answer.split())

def calculate_f1_score(gold_answer: str, response: str) -> float:
    """
    F1å¾—åˆ†ï¼ˆç»Ÿä¸€å‚æ•°é¡ºåºï¼‰
    Args:
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
    Returns:
        float: F1å¾—åˆ†
    """
    response = str(response) if response is not None else ""
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    
    response_tokens = [ps.stem(w) for w in normalize_answer(response).split()]
    gold_answer_tokens = [ps.stem(w) for w in normalize_answer(gold_answer).split()]
    
    common = Counter(response_tokens) & Counter(gold_answer_tokens)
    num_same = sum(common.values())
    
    if num_same == 0:
        return 0
    
    precision = 1.0 * num_same / len(response_tokens)
    recall = 1.0 * num_same / len(gold_answer_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    
    return f1

def calculate_f1_score_multi(gold_answer: str, response: str) -> float:
    """
    å¤šç­”æ¡ˆF1å¾—åˆ†ï¼ˆç»Ÿä¸€å‚æ•°é¡ºåºï¼‰
    Args:
        gold_answer: æ ‡å‡†ç­”æ¡ˆï¼ˆå¯ä»¥æ˜¯å¤šä¸ªç­”æ¡ˆï¼Œç”¨é€—å·åˆ†éš”ï¼‰
        response: ç”Ÿæˆçš„ç­”æ¡ˆï¼ˆå¯ä»¥æ˜¯å¤šä¸ªç­”æ¡ˆï¼Œç”¨é€—å·åˆ†éš”ï¼‰
    Returns:
        float: å¤šç­”æ¡ˆF1å¾—åˆ†
    """
    response = str(response) if response is not None else ""
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    
    responses = [r.strip() for r in response.split(',')]
    gold_answers = [g.strip() for g in gold_answer.split(',')]
    
    return np.mean([max([calculate_f1_score(ga, resp) for resp in responses]) for ga in gold_answers])

# ================================
# æ›´æ–°ç›¸å…³çš„å‘åå…¼å®¹å‡½æ•°
# ================================

# def ems(prediction, ground_truths):
#     """ä¿æŒå‘åå…¼å®¹çš„å¤šç­”æ¡ˆç²¾ç¡®åŒ¹é…å‡½æ•°"""
#     prediction = str(prediction) if prediction is not None else ""
    
#     safe_ground_truths = []
#     for gt in ground_truths:
#         if gt is not None:
#             safe_ground_truths.append(str(gt))
#         else:
#             safe_ground_truths.append("")
    
#     return max([exact_match_score(gt, prediction) for gt in safe_ground_truths])

# ================================
# æ›´æ–° eval_question_answering å‡½æ•°ä¸­çš„è°ƒç”¨
# ================================

def eval_question_answering(qas, eval_key='prediction', metric='f1'):
    """é—®ç­”ä»»åŠ¡è¯„ä¼°ï¼ˆæ›´æ–°å‡½æ•°è°ƒç”¨ï¼‰"""
    all_ems = []
    all_recall = []
    
    for i, line in enumerate(qas):
        if type(line[eval_key]) == list:
            answer = str(line['answer']) if line['answer'] is not None else ""
        else:
            answer = str(line['answer']) if line['answer'] is not None else ""
            
        if line['category'] == 3:
            answer = answer.split(';')[0].strip()
        
        output = str(line[eval_key]) if line[eval_key] is not None else ""
        
        if line['category'] in [2, 3, 4]:
            all_ems.append(calculate_f1_score(answer, output))  # æ›´æ–°å‚æ•°é¡ºåº
        elif line['category'] in [1]:
            all_ems.append(calculate_f1_score_multi(answer, output))  # æ›´æ–°å‚æ•°é¡ºåºå’Œå‡½æ•°å
        elif line['category'] in [5]:
            output_lower = output.lower()
            if 'no information available' in output_lower or 'not mentioned' in output_lower:
                all_ems.append(1)
            else:
                all_ems.append(0)
        else:
            raise ValueError(f"æœªçŸ¥çš„é—®é¢˜ç±»åˆ«: {line['category']}")
        
        assert i+1 == len(all_ems)

        if eval_key + '_context' in line and len(line['evidence']) > 0:
            if line[eval_key + '_context'][0].startswith('S'):
                sessions = [e[1:] for e in line[eval_key + '_context']]
                recall_acc = float(sum([ev.split(':')[0][1:] in sessions for ev in line["evidence"]]))/len(line['evidence'])
            else:
                recall_acc = float(sum([ev in line[eval_key + '_context'] for ev in line["evidence"]]))/len(line['evidence'])
            all_recall.append(recall_acc)
        else:
            all_recall.append(1)

    print("{} ä¸ªQAæ ·æœ¬å·²è¯„ä¼°; {} ä¸ªå‡†ç¡®ç‡å€¼".format(len(qas), len(all_ems)))
    return all_ems, 0.0, all_recall

def eval_dialogue_system(infile):
    """å¯¹è¯ç³»ç»Ÿè¯„ä¼°ï¼ˆæ›´æ–°å‡½æ•°è°ƒç”¨ï¼‰"""
    lines = open(infile, 'r').readlines()[1:]

    f1_scores = []
    rl_scores = []
    answer_lengths = []
    
    for line in lines:
        line = json.loads(line)
        answer = line['answer']
        output = line['output'][0]

        f1_scores.append(calculate_f1_score(answer, output))  # æ›´æ–°å‚æ•°é¡ºåº
        rl_scores.append(calculate_bleu_score(answer, output))  # æ›´æ–°å‚æ•°é¡ºåº
        answer_lengths.append(len(output.split()))

    F1 = round(np.mean(f1_scores), 4)
    RL = round(np.mean(rl_scores), 4)
    lens = round(np.mean(answer_lengths), 4)

    return F1, RL, lens

def calculate_comprehensive_metrics(gold_answer: str, 
                                  response: str, 
                                  context: str = "", 
                                  options: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    è®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡ï¼ˆæ›´æ–°å‡½æ•°è°ƒç”¨ï¼‰
    
    Args:
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
        context: ä¸Šä¸‹æ–‡
        options: è¯„ä¼°é€‰é¡¹ ["lexical", "semantic"]
        
    Returns:
        åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    if options is None:
        options = ["lexical", "semantic"]

    gold_answer = str(gold_answer) if gold_answer is not None else ""
    response = str(response) if response is not None else ""

    metrics = {
        "context_tokens": len(nltk.word_tokenize(context)) if context else 0,
        "response_tokens": len(nltk.word_tokenize(response)),
        "gold_tokens": len(nltk.word_tokenize(gold_answer))
    }

    if "lexical" in options:
        metrics["lexical"] = {}
        
        # åŸºç¡€æŒ‡æ ‡ - æ›´æ–°å‚æ•°é¡ºåº
        metrics["lexical"]["exact_match"] = float(exact_match_score(gold_answer, response))
        metrics["lexical"]["token_f1"] = calculate_f1_score(gold_answer, response)
        
        # ROUGEæŒ‡æ ‡
        rouge_scores = calculate_rouge_score(gold_answer, response)
        metrics["lexical"].update(rouge_scores)
        
        # BLEUæŒ‡æ ‡
        bleu_scores = calculate_bleu_score(gold_answer, response)
        metrics["lexical"].update(bleu_scores)
        
        # METEORæŒ‡æ ‡
        metrics["lexical"]["meteor"] = calculate_meteor_score(gold_answer, response)

    if "semantic" in options:
        metrics["semantic"] = {}
        
        # è¯­ä¹‰ç›¸ä¼¼åº¦
        metrics["semantic"]["similarity"] = calculate_semantic_similarity(gold_answer, response)
        
        # BERT F1
        metrics["semantic"]["bert_f1"] = calculate_bert_f1_score(gold_answer, response)

    return metrics


# ================================
# æ–°å¢å’Œæ”¹è¿›çš„è¯„åˆ†å‡½æ•°ï¼ˆå‚è€ƒlocomo_evalï¼‰
# ================================

def calculate_rouge_score(gold_answer: str, response: str) -> Dict[str, float]:
    """
    è®¡ç®—ROUGEåˆ†æ•°ï¼ˆä½¿ç”¨rouge_scoreåº“ï¼Œæ›´å‡†ç¡®ï¼‰
    
    Args:
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
        
    Returns:
        DictåŒ…å«rouge1_f, rouge2_f, rougeL_f
    """
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    response = str(response) if response is not None else ""
    
    metrics = {"rouge1_f": 0.0, "rouge2_f": 0.0, "rougeL_f": 0.0}
    
    try:
        scorer = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
        rouge_scores = scorer.score(gold_answer, response)
        metrics["rouge1_f"] = rouge_scores["rouge1"].fmeasure
        metrics["rouge2_f"] = rouge_scores["rouge2"].fmeasure
        metrics["rougeL_f"] = rouge_scores["rougeL"].fmeasure
    except Exception as e:
        logging.error(f"Failed to calculate ROUGE scores: {e}")
    
    return metrics

def calculate_bleu_score(gold_answer: str, response: str) -> Dict[str, float]:
    """
    è®¡ç®—BLEUåˆ†æ•°
    
    Args:
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
        
    Returns:
        DictåŒ…å«bleu1, bleu2, bleu3, bleu4
    """
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    response = str(response) if response is not None else ""
    
    metrics = {"bleu1": 0.0, "bleu2": 0.0, "bleu3": 0.0, "bleu4": 0.0}

    try:
        gold_tokens = nltk.word_tokenize(gold_answer.lower())
        response_tokens = nltk.word_tokenize(response.lower())
        
        smoothing = SmoothingFunction().method1
        weights = [(1, 0, 0, 0), (0.5, 0.5, 0, 0), (0.33, 0.33, 0.33, 0), (0.25, 0.25, 0.25, 0.25)]

        for i, weight in enumerate(weights, 1):
            metrics[f"bleu{i}"] = sentence_bleu(
                [gold_tokens], response_tokens, weights=weight, smoothing_function=smoothing
            )
    except Exception as e:
        logging.error(f"Failed to calculate BLEU scores: {e}")

    return metrics

def calculate_meteor_score(gold_answer: str, response: str) -> float:
    """
    è®¡ç®—METEORåˆ†æ•°
    
    Args:
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
        
    Returns:
        METEORåˆ†æ•°
    """
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    response = str(response) if response is not None else ""
    
    try:
        gold_tokens = nltk.word_tokenize(gold_answer.lower())
        response_tokens = nltk.word_tokenize(response.lower())
        return meteor_score([gold_tokens], response_tokens)
    except Exception as e:
        logging.error(f"Failed to calculate METEOR score: {e}")
        return 0.0

def calculate_semantic_similarity(gold_answer: str, response: str) -> float:
    """
    è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
    
    Args:
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
        
    Returns:
        è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
    """
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    response = str(response) if response is not None else ""
    
    try:
        sentence_model = get_sentence_model()
        if sentence_model is None:
            return 0.0
            
        gold_embedding = sentence_model.encode([gold_answer], show_progress_bar=False)[0]
        response_embedding = sentence_model.encode([response], show_progress_bar=False)[0]
        return 1 - cosine(gold_embedding, response_embedding)
    except Exception as e:
        logging.error(f"Failed to calculate semantic similarity: {e}")
        return 0.0

def calculate_bert_f1_score(gold_answer: str, response: str) -> float:
    """
    è®¡ç®—BERT F1åˆ†æ•°
    
    Args:
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
        
    Returns:
        BERT F1åˆ†æ•°
    """
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    response = str(response) if response is not None else ""
    
    try:
        _, _, f1 = score([response], [gold_answer], lang="en", rescale_with_baseline=True, verbose=False)
        return f1.item() if f1 is not None else 0.0
    except Exception as e:
        logging.error(f"Failed to calculate BERT F1 score: {e}")
        return 0.0

# def calculate_token_f1_score(gold_answer: str, response: str) -> float:
#     """
#     è®¡ç®—åŸºäºtokençš„F1åˆ†æ•°ï¼ˆä¸åŸf1_scoreç›¸åŒä½†åç§°æ›´æ˜ç¡®ï¼‰
    
#     Args:
#         gold_answer: æ ‡å‡†ç­”æ¡ˆ
#         response: ç”Ÿæˆçš„ç­”æ¡ˆ
        
#     Returns:
#         Token-level F1åˆ†æ•°
#     """
#     return calculate_f1_score(response, gold_answer)

# ================================
# ç»¼åˆè¯„ä¼°å‡½æ•°
# ================================

def calculate_comprehensive_metrics(gold_answer: str, 
                                  response: str, 
                                  context: str = "", 
                                  options: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    è®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡ï¼ˆå‚è€ƒlocomo_evalçš„calculate_nlp_metricsï¼‰
    
    Args:
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
        context: ä¸Šä¸‹æ–‡
        options: è¯„ä¼°é€‰é¡¹ ["lexical", "semantic"]
        
    Returns:
        åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    if options is None:
        options = ["lexical", "semantic"]

    gold_answer = str(gold_answer) if gold_answer is not None else ""
    response = str(response) if response is not None else ""

    metrics = {
        "context_tokens": len(nltk.word_tokenize(context)) if context else 0,
        "response_tokens": len(nltk.word_tokenize(response)),
        "gold_tokens": len(nltk.word_tokenize(gold_answer))
    }

    if "lexical" in options:
        metrics["lexical"] = {}
        
        # åŸºç¡€æŒ‡æ ‡
        metrics["lexical"]["exact_match"] = float(exact_match_score(response, gold_answer))
        metrics["lexical"]["token_f1"] = calculate_f1_score(gold_answer, response)
        
        # ROUGEæŒ‡æ ‡
        rouge_scores = calculate_rouge_score(gold_answer, response)
        metrics["lexical"].update(rouge_scores)
        
        # BLEUæŒ‡æ ‡
        bleu_scores = calculate_bleu_score(gold_answer, response)
        metrics["lexical"].update(bleu_scores)
        
        # METEORæŒ‡æ ‡
        metrics["lexical"]["meteor"] = calculate_meteor_score(gold_answer, response)

    if "semantic" in options:
        metrics["semantic"] = {}
        
        # è¯­ä¹‰ç›¸ä¼¼åº¦
        metrics["semantic"]["similarity"] = calculate_semantic_similarity(gold_answer, response)
        
        # BERT F1
        metrics["semantic"]["bert_f1"] = calculate_bert_f1_score(gold_answer, response)

    return metrics

def calculate_llm_judgment(llm_client, 
                         question: str, 
                         gold_answer: str, 
                         response: str,
                         num_runs: int = 1) -> Dict[str, Any]:
    """
    è®¡ç®—LLMåˆ¤æ–­åˆ†æ•°
    
    Args:
        llm_client: LLMå®¢æˆ·ç«¯
        question: é—®é¢˜
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
        num_runs: è¿è¡Œæ¬¡æ•°
        
    Returns:
        åŒ…å«LLMåˆ¤æ–­ç»“æœçš„å­—å…¸
    """
    judgments = []
    
    for i in range(num_runs):
        try:
            judgment = llm_grader(llm_client, question, gold_answer, response)
            judgments.append(judgment)
        except Exception as e:
            logging.error(f"LLM judgment {i+1} failed: {e}")
            judgments.append(False)
    
    return {
        "judgments": judgments,
        "accuracy": sum(judgments) / len(judgments) if judgments else 0.0,
        "num_runs": num_runs,
        "consistency": len(set(judgments)) == 1  # æ˜¯å¦æ‰€æœ‰åˆ¤æ–­ä¸€è‡´
    }

# ================================
# å·¥å…·å‡½æ•°
# ================================

def convert_numpy_types(obj):
    """
    è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼ˆç”¨äºJSONåºåˆ—åŒ–ï¼‰
    """
    if isinstance(obj, np.number):
        return float(obj)
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(i) for i in obj]
    else:
        return obj

def ems(prediction, ground_truths):
    """ä¿æŒå‘åå…¼å®¹çš„å¤šç­”æ¡ˆç²¾ç¡®åŒ¹é…å‡½æ•°"""
    prediction = str(prediction) if prediction is not None else ""
    
    safe_ground_truths = []
    for gt in ground_truths:
        if gt is not None:
            safe_ground_truths.append(str(gt))
        else:
            safe_ground_truths.append("")
    
    return max([exact_match_score(prediction, gt) for gt in safe_ground_truths])

# ================================
# åŸæœ‰æ–‡ä»¶çº§è¯„ä¼°å‡½æ•°ä¿æŒä¸å˜
# ================================

def has_answer(answers, text, tokenizer=SimpleTokenizer()) -> bool:
    """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦åŒ…å«ç­”æ¡ˆå­—ç¬¦ä¸²"""
    text = _normalize(text)
    text = tokenizer.tokenize(text, uncased=True)

    for answer in answers:
        answer = _normalize(answer)
        answer = tokenizer.tokenize(answer, uncased=True)
        for i in range(0, len(text) - len(answer) + 1):
            if answer == text[i: i + len(answer)]:
                return True
    return False

def check_answer(example, tokenizer) -> List[bool]:
    """åœ¨æ‰€æœ‰é¡¶éƒ¨æ–‡æ¡£ä¸­æœç´¢ç­”æ¡ˆ"""
    answers = example['answers']
    ctxs = example['ctxs']

    hits = []
    for _, doc in enumerate(ctxs):
        text = doc['text']
        if text is None:
            hits.append(False)
            continue
        hits.append(has_answer(answers, text, tokenizer))

    return hits

def eval_recall(infile):
    """è¯„ä¼°å¬å›ç‡"""
    tokenizer = SimpleTokenizer()
    lines = open(infile, 'r').readlines()[1:]

    has_answer_count = 0
    answer_lengths = []
    
    for line in lines:
        line = json.loads(line)
        answer = line['answer']
        output = ' || '.join(line['output'])

        if has_answer(answer, output, tokenizer):
            has_answer_count += 1

        answer_lengths.append(len(output.split()))

    recall = round(has_answer_count/len(lines), 4)
    lens = round(np.mean(answer_lengths), 4)

    return recall, lens

def eval_question_answering(qas, eval_key='prediction', metric='f1'):
    """é—®ç­”ä»»åŠ¡è¯„ä¼°ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
    all_ems = []
    all_recall = []
    
    for i, line in enumerate(qas):
        if type(line[eval_key]) == list:
            answer = str(line['answer']) if line['answer'] is not None else ""
        else:
            answer = str(line['answer']) if line['answer'] is not None else ""
            
        if line['category'] == 3:
            answer = answer.split(';')[0].strip()
        
        output = str(line[eval_key]) if line[eval_key] is not None else ""
        
        if line['category'] in [2, 3, 4]:
            all_ems.append(calculate_f1_score(output, answer))
        elif line['category'] in [1]:
            all_ems.append(calculate_f1_score(output, answer))
        elif line['category'] in [5]:
            output_lower = output.lower()
            if 'no information available' in output_lower or 'not mentioned' in output_lower:
                all_ems.append(1)
            else:
                all_ems.append(0)
        else:
            raise ValueError(f"æœªçŸ¥çš„é—®é¢˜ç±»åˆ«: {line['category']}")
        
        assert i+1 == len(all_ems)

        if eval_key + '_context' in line and len(line['evidence']) > 0:
            if line[eval_key + '_context'][0].startswith('S'):
                sessions = [e[1:] for e in line[eval_key + '_context']]
                recall_acc = float(sum([ev.split(':')[0][1:] in sessions for ev in line["evidence"]]))/len(line['evidence'])
            else:
                recall_acc = float(sum([ev in line[eval_key + '_context'] for ev in line["evidence"]]))/len(line['evidence'])
            all_recall.append(recall_acc)
        else:
            all_recall.append(1)

    print("{} ä¸ªQAæ ·æœ¬å·²è¯„ä¼°; {} ä¸ªå‡†ç¡®ç‡å€¼".format(len(qas), len(all_ems)))
    return all_ems, 0.0, all_recall

def eval_fact_checking(infile):
    """äº‹å®æ£€æŸ¥ä»»åŠ¡è¯„ä¼°"""
    tokenizer = SimpleTokenizer()
    lines = open(infile, 'r').readlines()[1:]

    exact_match_count = 0
    answer_lengths = []
    
    for line in lines:
        line = json.loads(line)
        answer = line['answer']
        output = line['output'][0]

        if answer == ["refutes"]:
            answer = ["refutes", "no", "false"]
        if answer == ["supports"]:
            answer = ["supports", "yes", "true"]

        if has_answer(answer, output, tokenizer):
            exact_match_count += 1
        
        answer_lengths.append(len(output.split()))

    em = round(exact_match_count/len(lines), 4)
    lens = round(np.mean(answer_lengths), 4)

    return em, lens

def eval_dialogue_system(infile):
    """å¯¹è¯ç³»ç»Ÿè¯„ä¼°"""
    lines = open(infile, 'r').readlines()[1:]

    f1_scores = []
    rl_scores = []
    answer_lengths = []
    
    for line in lines:
        line = json.loads(line)
        answer = line['answer']
        output = line['output'][0]

        f1_scores.append(calculate_f1_score(output, answer))
        rl_scores.append(calculate_bleu_score(output, [answer]))
        answer_lengths.append(len(output.split()))

    F1 = round(np.mean(f1_scores), 4)
    RL = round(np.mean(rl_scores), 4)
    lens = round(np.mean(answer_lengths), 4)

    return F1, RL, lens

# ================================
# æ–°å¢ï¼šç®€åŒ–çš„è¯„ä¼°æ¥å£
# ================================

def evaluate_answer_comprehensive(question: str,
                                gold_answer: str,
                                predicted_answer: str,
                                context: str = "",
                                llm_client=None,
                                include_llm_judgment: bool = False,
                                evaluation_options: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    ç»¼åˆç­”æ¡ˆè¯„ä¼°æ¥å£
    
    Args:
        question: é—®é¢˜
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        predicted_answer: é¢„æµ‹ç­”æ¡ˆ
        context: ä¸Šä¸‹æ–‡
        llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        include_llm_judgment: æ˜¯å¦åŒ…å«LLMåˆ¤æ–­
        evaluation_options: è¯„ä¼°é€‰é¡¹
        
    Returns:
        ç»¼åˆè¯„ä¼°ç»“æœ
    """
    if evaluation_options is None:
        evaluation_options = ["lexical", "semantic"]
    
    # åŸºç¡€æŒ‡æ ‡
    result = calculate_comprehensive_metrics(
        gold_answer, predicted_answer, context, evaluation_options
    )
    
    # LLMåˆ¤æ–­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if include_llm_judgment and llm_client is not None:
        llm_result = calculate_llm_judgment(
            llm_client, question, gold_answer, predicted_answer
        )
        result["llm_judgment"] = llm_result
    
    # è½¬æ¢numpyç±»å‹
    result = convert_numpy_types(result)
    
    return result

# ================================
# ä½¿ç”¨ç¤ºä¾‹
# ================================

def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    question = "What is Caroline's relationship status?"
    gold_answer = "single"
    predicted_answer = "Based on the conversation, Caroline appears to be single."
    
    # åŸºç¡€è¯„ä¼°
    basic_result = evaluate_answer_comprehensive(
        question=question,
        gold_answer=gold_answer,
        predicted_answer=predicted_answer,
        evaluation_options=["lexical", "semantic"]
    )
    
    print("Basic evaluation result:")
    print(json.dumps(basic_result, indent=2))
    
    # ä½¿ç”¨LLMè¯„ä¼°ï¼ˆéœ€è¦LLMå®¢æˆ·ç«¯ï¼‰
    # llm_result = evaluate_answer_comprehensive(
    #     question=question,
    #     gold_answer=gold_answer,
    #     predicted_answer=predicted_answer,
    #     llm_client=your_llm_client,
    #     include_llm_judgment=True
    # )

# ä½¿ç”¨ç¤ºä¾‹
def example_llm_grader_usage():
    """LLMè¯„ä¼°å™¨ä½¿ç”¨ç¤ºä¾‹"""
    
    # åˆ›å»ºLLMClient
    llm_client = LLMClient("deepseek-chat")  # æˆ–è€…ä½¿ç”¨å…¶ä»–æ¨¡å‹
    
    # å•ä¸ªè¯„ä¼°
    question = "What is Caroline's job?"
    gold_answer = "psychologist"
    predicted_answer = "She works as a therapist and counselor."
    
    # åŸºç¡€LLMè¯„ä¼°
    is_correct = llm_grader(llm_client, question, gold_answer, predicted_answer)
    print(f"LLMè¯„ä¼°ç»“æœ: {is_correct}")
    
    # å¤šæ¬¡è¿è¡Œçš„ä¸€è‡´æ€§æ£€æŸ¥
    llm_judgment = calculate_llm_judgment(
        llm_client, question, gold_answer, predicted_answer, num_runs=3
    )
    print(f"LLMåˆ¤æ–­è¯¦æƒ…: {llm_judgment}")
    
    # ç»¼åˆè¯„ä¼°ï¼ˆåŒ…å«LLMåˆ¤æ–­ï¼‰
    comprehensive_result = evaluate_answer_comprehensive(
        question=question,
        gold_answer=gold_answer,
        predicted_answer=predicted_answer,
        llm_client=llm_client,
        include_llm_judgment=True,
        llm_runs=2
    )
    print(f"ç»¼åˆè¯„ä¼°ç»“æœ: {json.dumps(comprehensive_result, indent=2)}")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    print("ğŸ” æµ‹è¯•LLMè¯„ä¼°å™¨...")
    client = LLMClient("deepseek-chat")
    test_llm_grader(client)
    
    # è¿è¡Œç¤ºä¾‹
    example_llm_grader_usage()
    example_usage()
