#!/usr/bin/env python3
"""
æµ‹è¯•conv-26æ•°æ®é›†åŠ è½½æƒ…å†µï¼ŒéªŒè¯datetimeå­—æ®µå¡«å……
"""

import json
import logging
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from dev import SemanticGraph, MemoryUnit
from benchmark.llm_utils.llm_client import LLMClient
from benchmark.task_eval.QA_evaluator import NewEvalExperiment

def test_conv26_datetime_loading():
    """æµ‹è¯•conv-26æ•°æ®é›†çš„datetimeåŠ è½½æƒ…å†µ"""
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•conv-26æ•°æ®é›†datetimeå­—æ®µåŠ è½½")
    print("=" * 60)
    
    # åˆå§‹åŒ–å®éªŒæ¡†æ¶
    try:
        llm_client = LLMClient(model_name="deepseek-chat")
        experiment = NewEvalExperiment(
            llm_client=llm_client,
            output_dir="test_results/datetime_test"
        )
        print("âœ… å®éªŒæ¡†æ¶åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ å®éªŒæ¡†æ¶åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    conv_id = "conv-26"
    fused_dir = "benchmark/dataset/locomo/fused_llm"
    
    print(f"\nğŸ“ åŠ è½½æ•°æ®é›†: {conv_id}")
    print(f"ğŸ“ Fusedç›®å½•: {fused_dir}")
    
    try:
        # è°ƒç”¨åŠ è½½å‡½æ•°
        graph, qa_pairs = experiment.load_single_conversation_from_fused(conv_id, fused_dir)
        
        if not qa_pairs:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°QAæ•°æ®")
            return
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(qa_pairs)} ä¸ªQAé—®é¢˜")
        
        # æ£€æŸ¥å›¾è°±ä¸­çš„æ•°æ®
        print(f"\nğŸ” æ£€æŸ¥è¯­ä¹‰å›¾è°±æ•°æ®...")
        
        # è·å–æ‰€æœ‰memory space
        ms_names = [f"conversation_{conv_id}"]
        all_units = graph.get_units_in_memory_space(ms_names)
        
        print(f"ğŸ“Š æ€»è®¡åŠ è½½äº† {len(all_units)} ä¸ªå†…å­˜å•å…ƒ")
        
        # åˆ†ææ•°æ®ç±»å‹åˆ†å¸ƒ
        type_stats = {}
        datetime_stats = {
            "æœ‰æ—¶é—´": 0,
            "æ— æ—¶é—´": 0,
            "æ—¶é—´æ ·ä¾‹": []
        }
        
        for unit in all_units:
            data_type = unit.raw_data.get("data_type", "unknown")
            datetime_val = unit.raw_data.get("datetime", "")
            
            # ç»Ÿè®¡æ•°æ®ç±»å‹
            type_stats[data_type] = type_stats.get(data_type, 0) + 1
            
            # ç»Ÿè®¡æ—¶é—´å­—æ®µ
            if datetime_val and datetime_val.strip():
                datetime_stats["æœ‰æ—¶é—´"] += 1
                if len(datetime_stats["æ—¶é—´æ ·ä¾‹"]) < 5:  # åªæ”¶é›†å‰5ä¸ªæ ·ä¾‹
                    datetime_stats["æ—¶é—´æ ·ä¾‹"].append({
                        "uid": unit.uid,
                        "data_type": data_type,
                        "datetime": datetime_val,
                        "session": unit.raw_data.get("session", ""),
                        "speaker": unit.raw_data.get("speaker", "")
                    })
            else:
                datetime_stats["æ— æ—¶é—´"] += 1
        
        print(f"\nğŸ“Š æ•°æ®ç±»å‹åˆ†å¸ƒ:")
        for data_type, count in sorted(type_stats.items()):
            print(f"   - {data_type}: {count} ä¸ª")
        
        print(f"\nâ° æ—¶é—´å­—æ®µç»Ÿè®¡:")
        print(f"   - æœ‰æ—¶é—´ä¿¡æ¯: {datetime_stats['æœ‰æ—¶é—´']} ä¸ª")
        print(f"   - æ— æ—¶é—´ä¿¡æ¯: {datetime_stats['æ— æ—¶é—´']} ä¸ª")
        print(f"   - æ—¶é—´å¡«å……ç‡: {datetime_stats['æœ‰æ—¶é—´'] / len(all_units) * 100:.1f}%")
        
        print(f"\nğŸ“ æ—¶é—´ä¿¡æ¯æ ·ä¾‹:")
        for i, sample in enumerate(datetime_stats["æ—¶é—´æ ·ä¾‹"], 1):
            print(f"   [{i}] {sample['data_type']} - {sample['session']}")
            print(f"       æ—¶é—´: {sample['datetime']}")
            print(f"       è¯´è¯äºº: {sample['speaker']}")
            print(f"       UID: {sample['uid']}")
            print()
        
        # è¯¦ç»†æ£€æŸ¥å¯¹è¯æ•°æ®
        print(f"\nğŸ” è¯¦ç»†æ£€æŸ¥å¯¹è¯æ•°æ®...")
        dialogue_units = [u for u in all_units if u.raw_data.get("data_type") == "dialogue"]
        
        print(f"ğŸ“‹ å¯¹è¯å•å…ƒ: {len(dialogue_units)} ä¸ª")
        
        # æŒ‰sessionåˆ†ç»„æ£€æŸ¥
        session_groups = {}
        for unit in dialogue_units:
            session = unit.raw_data.get("session", "unknown")
            if session not in session_groups:
                session_groups[session] = []
            session_groups[session].append(unit)
        
        print(f"ğŸ“Š Sessionåˆ†å¸ƒ:")
        for session, units in sorted(session_groups.items()):
            if units:
                first_unit = units[0]
                datetime_val = first_unit.raw_data.get("datetime", "")
                print(f"   - {session}: {len(units)} ä¸ªå¯¹è¯, æ—¶é—´: {datetime_val if datetime_val else 'âŒ æ— æ—¶é—´'}")
        
        # æ„å»ºæµ‹è¯•ç»“æœ
        test_result = {
            "æµ‹è¯•ä¿¡æ¯": {
                "æµ‹è¯•æ—¶é—´": datetime.now().isoformat(),
                "å¯¹è¯ID": conv_id,
                "Fusedç›®å½•": fused_dir,
                "QAé—®é¢˜æ•°é‡": len(qa_pairs),
                "æ€»å†…å­˜å•å…ƒæ•°": len(all_units)
            },
            "æ•°æ®ç±»å‹ç»Ÿè®¡": type_stats,
            "æ—¶é—´å­—æ®µç»Ÿè®¡": {
                "æœ‰æ—¶é—´ä¿¡æ¯": datetime_stats["æœ‰æ—¶é—´"],
                "æ— æ—¶é—´ä¿¡æ¯": datetime_stats["æ— æ—¶é—´"],
                "æ—¶é—´å¡«å……ç‡": f"{datetime_stats['æœ‰æ—¶é—´'] / len(all_units) * 100:.1f}%"
            },
            "æ—¶é—´ä¿¡æ¯æ ·ä¾‹": datetime_stats["æ—¶é—´æ ·ä¾‹"],
            "Sessionåˆ†æ": {},
            "å¯¹è¯å•å…ƒè¯¦æƒ…": []
        }
        
        # è¯¦ç»†çš„sessionåˆ†æ
        for session, units in sorted(session_groups.items()):
            if units:
                first_unit = units[0]
                test_result["Sessionåˆ†æ"][session] = {
                    "å¯¹è¯æ•°é‡": len(units),
                    "æ—¶é—´ä¿¡æ¯": first_unit.raw_data.get("datetime", ""),
                    "æ˜¯å¦æœ‰æ—¶é—´": bool(first_unit.raw_data.get("datetime", "").strip()),
                    "è¯´è¯äººåˆ—è¡¨": list(set([u.raw_data.get("speaker", "") for u in units if u.raw_data.get("speaker")]))
                }
        
        # è¯¦ç»†çš„å¯¹è¯å•å…ƒä¿¡æ¯ï¼ˆå‰10ä¸ªï¼‰
        for i, unit in enumerate(dialogue_units[:10]):
            test_result["å¯¹è¯å•å…ƒè¯¦æƒ…"].append({
                "ç´¢å¼•": i + 1,
                "UID": unit.uid,
                "å¯¹è¯ID": unit.raw_data.get("dialogue_id", ""),
                "Session": unit.raw_data.get("session", ""),
                "è¯´è¯äºº": unit.raw_data.get("speaker", ""),
                "æ—¶é—´ä¿¡æ¯": unit.raw_data.get("datetime", ""),
                "æ–‡æœ¬å†…å®¹": unit.raw_data.get("text_content", "")[:100] + "..." if len(unit.raw_data.get("text_content", "")) > 100 else unit.raw_data.get("text_content", ""),
                "æ˜¯å¦æœ‰æ—¶é—´": bool(unit.raw_data.get("datetime", "").strip())
            })
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        output_dir = Path("test_results/datetime_test")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        result_file = output_dir / f"{conv_id}_datetime_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(test_result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜: {result_file}")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶å¤§å°: {result_file.stat().st_size / 1024:.1f} KB")
        
        # è¾“å‡ºå…³é”®å‘ç°
        print(f"\nğŸ¯ å…³é”®å‘ç°:")
        
        if datetime_stats["æœ‰æ—¶é—´"] > 0:
            print(f"âœ… æ—¶é—´å­—æ®µä¿®å¤æˆåŠŸï¼")
            print(f"   - {datetime_stats['æœ‰æ—¶é—´']}/{len(all_units)} ä¸ªå•å…ƒæœ‰æ—¶é—´ä¿¡æ¯")
            print(f"   - æ—¶é—´å¡«å……ç‡: {datetime_stats['æœ‰æ—¶é—´'] / len(all_units) * 100:.1f}%")
        else:
            print(f"âŒ æ—¶é—´å­—æ®µä»ç„¶ä¸ºç©ºï¼")
            print(f"   - æ‰€æœ‰ {len(all_units)} ä¸ªå•å…ƒéƒ½æ²¡æœ‰æ—¶é—´ä¿¡æ¯")
        
        # æ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯å¯¹è¯å•å…ƒæœ‰æ—¶é—´
        dialogue_with_time = len([u for u in dialogue_units if u.raw_data.get("datetime", "").strip()])
        if dialogue_with_time > 0:
            print(f"âœ… å¯¹è¯å•å…ƒæ—¶é—´å¡«å……: {dialogue_with_time}/{len(dialogue_units)} ä¸ª")
        else:
            print(f"âŒ å¯¹è¯å•å…ƒå®Œå…¨æ²¡æœ‰æ—¶é—´ä¿¡æ¯")
        
        return test_result
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None

def inspect_raw_fused_data():
    """æ£€æŸ¥åŸå§‹fusedæ•°æ®çš„ç»“æ„"""
    
    print(f"\n" + "=" * 60)
    print("ğŸ” æ£€æŸ¥åŸå§‹fusedæ•°æ®ç»“æ„")
    print("=" * 60)
    
    fused_file = Path("benchmark/dataset/locomo/fused_llm/conv-26_fused.json")
    
    if not fused_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {fused_file}")
        return
    
    try:
        with open(fused_file, 'r', encoding='utf-8') as f:
            fused_data = json.load(f)
        
        print(f"âœ… æˆåŠŸåŠ è½½åŸå§‹æ•°æ®")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {fused_file.stat().st_size / 1024 / 1024:.1f} MB")
        
        # æ£€æŸ¥é¡¶å±‚ç»“æ„
        print(f"\nğŸ“Š é¡¶å±‚æ•°æ®ç»“æ„:")
        for key in fused_data.keys():
            if isinstance(fused_data[key], dict):
                print(f"   - {key}: å­—å…¸ (åŒ…å« {len(fused_data[key])} ä¸ªé”®)")
            elif isinstance(fused_data[key], list):
                print(f"   - {key}: åˆ—è¡¨ (åŒ…å« {len(fused_data[key])} ä¸ªå…ƒç´ )")
            else:
                print(f"   - {key}: {type(fused_data[key]).__name__}")
        
        # æ£€æŸ¥conversationsç»“æ„
        conversations = fused_data.get("conversations", {})
        print(f"\nğŸ“‹ conversationsç»“æ„:")
        for key in conversations.keys():
            if isinstance(conversations[key], dict):
                print(f"   - {key}: å­—å…¸ (åŒ…å« {len(conversations[key])} ä¸ªé”®)")
            elif isinstance(conversations[key], list):
                print(f"   - {key}: åˆ—è¡¨ (åŒ…å« {len(conversations[key])} ä¸ªå…ƒç´ )")
            else:
                print(f"   - {key}: {type(conversations[key]).__name__}")
        
        # æ£€æŸ¥raw_conversationä¸­çš„sessionæ—¶é—´
        raw_conversation = conversations.get("raw_conversation", {})
        print(f"\nâ° raw_conversationä¸­çš„æ—¶é—´ä¿¡æ¯:")
        
        session_times = {}
        session_dialogues = {}
        
        for key, value in raw_conversation.items():
            if key.endswith("_date_time"):
                session_key = key.replace("_date_time", "")
                session_times[session_key] = value
                print(f"   - {key}: {value}")
            elif key.startswith("session_") and isinstance(value, list):
                session_key = key
                session_dialogues[session_key] = len(value)
                print(f"   - {key}: {len(value)} ä¸ªå¯¹è¯")
        
        print(f"\nğŸ“Š Sessionæ—¶é—´æ˜ å°„éªŒè¯:")
        for session_key in sorted(session_times.keys()):
            time_val = session_times.get(session_key, "")
            dialogue_count = session_dialogues.get(session_key, 0)
            print(f"   - {session_key}: æ—¶é—´='{time_val}', å¯¹è¯æ•°={dialogue_count}")
        
        # æ£€æŸ¥å‡ ä¸ªsessionçš„å¯¹è¯æ ·ä¾‹
        print(f"\nğŸ“ å¯¹è¯æ ·ä¾‹æ£€æŸ¥ (å‰3ä¸ªsession):")
        count = 0
        for session_key, dialogues in raw_conversation.items():
            if session_key.startswith("session_") and isinstance(dialogues, list) and count < 3:
                session_time = session_times.get(session_key, "æ— æ—¶é—´")
                print(f"\n   Session: {session_key} (æ—¶é—´: {session_time})")
                
                for i, dialogue in enumerate(dialogues[:2]):  # æ¯ä¸ªsessionæ˜¾ç¤ºå‰2ä¸ªå¯¹è¯
                    dia_id = dialogue.get("dia_id", f"d{i}")
                    speaker = dialogue.get("speaker", "")
                    text = dialogue.get("text", "")[:50] + "..." if len(dialogue.get("text", "")) > 50 else dialogue.get("text", "")
                    print(f"     [{i+1}] {dia_id} - {speaker}: {text}")
                
                count += 1
        
        return {
            "æ–‡ä»¶è·¯å¾„": str(fused_file),
            "æ–‡ä»¶å¤§å°MB": fused_file.stat().st_size / 1024 / 1024,
            "é¡¶å±‚é”®": list(fused_data.keys()),
            "conversationsé”®": list(conversations.keys()),
            "sessionæ—¶é—´æ˜ å°„": session_times,
            "sessionå¯¹è¯æ•°": session_dialogues
        }
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥åŸå§‹æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹conv-26æ•°æ®é›†datetimeå­—æ®µæµ‹è¯•")
    
    # 1. æ£€æŸ¥åŸå§‹æ•°æ®ç»“æ„
    raw_info = inspect_raw_fused_data()
    
    # 2. æµ‹è¯•åŠ è½½è¿‡ç¨‹
    test_result = test_conv26_datetime_loading()
    
    if test_result:
        print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° test_results/datetime_test/ ç›®å½•")
    else:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼")
    
    print(f"\n{'=' * 60}")
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
    if raw_info:
        print(f"   - åŸå§‹æ•°æ®æ£€æŸ¥: âœ… æˆåŠŸ")
        print(f"   - Sessionæ—¶é—´æ˜ å°„: {len(raw_info.get('sessionæ—¶é—´æ˜ å°„', {}))} ä¸ª")
    else:
        print(f"   - åŸå§‹æ•°æ®æ£€æŸ¥: âŒ å¤±è´¥")
    
    if test_result:
        print(f"   - æ•°æ®åŠ è½½æµ‹è¯•: âœ… æˆåŠŸ")
        time_stats = test_result.get("æ—¶é—´å­—æ®µç»Ÿè®¡", {})
        print(f"   - æ—¶é—´å¡«å……ç‡: {time_stats.get('æ—¶é—´å¡«å……ç‡', '0%')}")
    else:
        print(f"   - æ•°æ®åŠ è½½æµ‹è¯•: âŒ å¤±è´¥")
    
    print("=" * 60)