import json
import logging
import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
import numpy as np
from sentence_transformers import SentenceTransformer, util
import torch

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥devæ¨¡å—
sys.path.append(str(Path(__file__).parent.parent.parent))
from dev import SemanticGraph, MemoryUnit
from benchmark.task_eval.evaluation import eval_question_answering, f1_score, exact_match_score
from benchmark.llm_utils import LLMClient, PromptBuilder, AnswerExtractor

# --- é…ç½® ---
LOGGING_LEVEL = logging.INFO
DATASET_DIR = Path(__file__).parent.parent / "dataset" / "locomo"
DATASET_PATH = DATASET_DIR / "locomo10.json"
RESULTS_DIR = Path(__file__).parent.parent / "results"
TOP_K_RESULTS = 5
EVALUATION_MODEL = 'all-MiniLM-L6-v2'

# LLMé…ç½®
LLM_MODELS = [
    "deepseek-chat",
    # "gpt-3.5-turbo",
    # "gpt-4",
    # å¯ä»¥æ·»åŠ æ›´å¤šæ¨¡å‹
]

# åˆ›å»ºç»“æœç›®å½•
RESULTS_DIR.mkdir(exist_ok=True)

# --- æ—¥å¿—è®¾ç½® ---
logging.basicConfig(
    level=LOGGING_LEVEL, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(RESULTS_DIR / f'locomo_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)

# ... ä¿ç•™ä¹‹å‰çš„æ•°æ®åŠ è½½å’Œå¤„ç†å‡½æ•° ...

def llm_enhanced_evaluation(graph: SemanticGraph, 
                           qa_pairs: List[Dict], 
                           eval_model: SentenceTransformer,
                           llm_models: List[str]) -> Dict[str, Any]:
    """ä½¿ç”¨LLMå¢å¼ºçš„è¯„ä¼°å‡½æ•°"""
    
    # åˆå§‹åŒ–å·¥å…·
    prompt_builder = PromptBuilder()
    answer_extractor = AnswerExtractor()
    
    results = {
        'total_questions': len(qa_pairs),
        'models': {},
        'category_results': {},
        'detailed_results': []
    }
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºç»“æœå­˜å‚¨
    for model in llm_models:
        results['models'][model] = {
            'all_scores': [],
            'f1_scores': [],
            'similarity_scores': [],
            'exact_match_scores': [],
            'category_results': {}
        }
    
    for qa_idx, qa in enumerate(qa_pairs):
        question = qa.get('question')
        golden_answer = qa.get('answer')
        category = qa.get('category', 0)
        evidence = qa.get('evidence', [])
        
        if not question or not golden_answer:
            continue
        
        logging.info(f"å¤„ç†é—®é¢˜ {qa_idx + 1}/{len(qa_pairs)}: {question[:50]}...")
        
        # 1. æ£€ç´¢ç›¸å…³è®°å¿†
        retrieved_units = graph.search_similarity_in_graph(
            query_text=question, k=TOP_K_RESULTS
        )
        
        if not retrieved_units:
            logging.warning(f"é—®é¢˜ {qa_idx + 1} æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³è®°å¿†")
            continue
        
        # 2. æ„å»ºæç¤ºè¯
        prompt = prompt_builder.build_prompt(
            question=question,
            context_units=retrieved_units,
            category=category,
            evidence=evidence
        )
        
        question_result = {
            'question_id': qa_idx,
            'question': question,
            'golden_answer': golden_answer,
            'category': category,
            'evidence': evidence,
            'retrieved_count': len(retrieved_units),
            'context_summary': _summarize_context(retrieved_units),
            'prompt': prompt,
            'model_results': {}
        }
        
        # 3. å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°
        for model_name in llm_models:
            logging.info(f"  ä½¿ç”¨æ¨¡å‹ {model_name} ç”Ÿæˆç­”æ¡ˆ...")
            
            try:
                # åˆ›å»ºLLMå®¢æˆ·ç«¯
                llm_client = LLMClient(model_name=model_name)
                
                # ç”Ÿæˆç­”æ¡ˆ
                llm_response = llm_client.generate_answer(prompt)
                
                # æå–ç­”æ¡ˆ
                predicted_answer = answer_extractor.extract_answer(llm_response, category)
                
                # è¯„ä¼°ç­”æ¡ˆè´¨é‡
                scores = _evaluate_answer(
                    predicted_answer, golden_answer, category, eval_model
                )
                
                # è®°å½•ç»“æœ
                model_result = {
                    'llm_response': llm_response,
                    'predicted_answer': predicted_answer,
                    'scores': scores
                }
                
                question_result['model_results'][model_name] = model_result
                
                # æ›´æ–°ç»Ÿè®¡
                model_stats = results['models'][model_name]
                model_stats['all_scores'].append(scores['combined_score'])
                model_stats['f1_scores'].append(scores['f1_score'])
                model_stats['similarity_scores'].append(scores['similarity_score'])
                model_stats['exact_match_scores'].append(scores['exact_match'])
                
                # æŒ‰ç±»åˆ«ç»Ÿè®¡
                if category not in model_stats['category_results']:
                    model_stats['category_results'][category] = {
                        'count': 0,
                        'scores': [],
                        'f1_scores': [],
                        'similarity_scores': [],
                        'exact_match_scores': []
                    }
                
                cat_stats = model_stats['category_results'][category]
                cat_stats['count'] += 1
                cat_stats['scores'].append(scores['combined_score'])
                cat_stats['f1_scores'].append(scores['f1_score'])
                cat_stats['similarity_scores'].append(scores['similarity_score'])
                cat_stats['exact_match_scores'].append(scores['exact_match'])
                
                logging.info(f"    {model_name}: F1={scores['f1_score']:.3f}, "
                           f"ç›¸ä¼¼åº¦={scores['similarity_score']:.3f}, "
                           f"ç²¾ç¡®åŒ¹é…={scores['exact_match']}")
                
            except Exception as e:
                logging.error(f"æ¨¡å‹ {model_name} å¤„ç†é—®é¢˜ {qa_idx + 1} å¤±è´¥: {e}")
                question_result['model_results'][model_name] = {
                    'error': str(e),
                    'predicted_answer': "ç”Ÿæˆå¤±è´¥",
                    'scores': {'f1_score': 0, 'similarity_score': 0, 'exact_match': 0, 'combined_score': 0}
                }
        
        results['detailed_results'].append(question_result)
        
        # æ‰“å°å½“å‰é—®é¢˜çš„ç»“æœæ‘˜è¦
        print(f"\n{'='*60}")
        print(f"é—®é¢˜ {qa_idx + 1}/{len(qa_pairs)} (ç±»åˆ« {category})")
        print(f"â“ é—®é¢˜: {question}")
        print(f"âœ… æ ‡å‡†ç­”æ¡ˆ: {golden_answer}")
        print(f"ğŸ” æ£€ç´¢åˆ° {len(retrieved_units)} ä¸ªç›¸å…³è®°å¿†")
        
        for model_name in llm_models:
            if model_name in question_result['model_results']:
                result = question_result['model_results'][model_name]
                if 'error' not in result:
                    scores = result['scores']
                    print(f"ğŸ¤– {model_name}: {result['predicted_answer']}")
                    print(f"   ğŸ“Š F1={scores['f1_score']:.3f}, ç›¸ä¼¼åº¦={scores['similarity_score']:.3f}, EM={scores['exact_match']}")
    
    return results

def _summarize_context(retrieved_units: List[Tuple]) -> str:
    """æ€»ç»“æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡"""
    if not retrieved_units:
        return "æ— ä¸Šä¸‹æ–‡"
    
    sources = []
    for unit, score in retrieved_units:
        data_source = unit.metadata.get('data_source', 'unknown')
        sources.append(f"{data_source}({score:.3f})")
    
    return ", ".join(sources[:3])

def _evaluate_answer(predicted_answer: str, 
                    golden_answer: str, 
                    category: int,
                    eval_model: SentenceTransformer) -> Dict[str, float]:
    """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""
    
    # F1å¾—åˆ†
    f1_score_result = f1_score(predicted_answer, golden_answer)
    
    # ç²¾ç¡®åŒ¹é…
    exact_match = 1.0 if exact_match_score(predicted_answer, golden_answer) else 0.0
    
    # è¯­ä¹‰ç›¸ä¼¼åº¦
    try:
        embedding1 = eval_model.encode(predicted_answer, convert_to_tensor=True)
        embedding2 = eval_model.encode(golden_answer, convert_to_tensor=True)
        similarity_score = util.cos_sim(embedding1, embedding2).item()
    except:
        similarity_score = 0.0
    
    # ç»¼åˆå¾—åˆ†
    combined_score = (f1_score_result + similarity_score + exact_match) / 3
    
    return {
        'f1_score': f1_score_result,
        'similarity_score': similarity_score,
        'exact_match': exact_match,
        'combined_score': combined_score
    }

def calculate_llm_metrics(results: Dict[str, Any]) -> Dict[str, Any]:
    """è®¡ç®—LLMè¯„ä¼°æŒ‡æ ‡"""
    metrics = {}
    
    for model_name, model_results in results['models'].items():
        model_metrics = {}
        
        # æ•´ä½“æŒ‡æ ‡
        if model_results['all_scores']:
            model_metrics['avg_combined_score'] = np.mean(model_results['all_scores'])
            model_metrics['std_combined_score'] = np.std(model_results['all_scores'])
            model_metrics['avg_f1_score'] = np.mean(model_results['f1_scores'])
            model_metrics['avg_similarity_score'] = np.mean(model_results['similarity_scores'])
            model_metrics['exact_match_rate'] = np.mean(model_results['exact_match_scores'])
        
        # æŒ‰ç±»åˆ«æŒ‡æ ‡
        category_metrics = {}
        for category, cat_results in model_results['category_results'].items():
            if cat_results['scores']:
                category_metrics[f'category_{category}'] = {
                    'count': cat_results['count'],
                    'avg_score': np.mean(cat_results['scores']),
                    'avg_f1': np.mean(cat_results['f1_scores']),
                    'avg_similarity': np.mean(cat_results['similarity_scores']),
                    'exact_match_rate': np.mean(cat_results['exact_match_scores'])
                }
        
        model_metrics['categories'] = category_metrics
        metrics[model_name] = model_metrics
    
    return metrics

def print_llm_evaluation_summary(results: Dict[str, Any], metrics: Dict[str, Any]):
    """æ‰“å°LLMè¯„ä¼°æ€»ç»“"""
    print(f"\n{'='*80}")
    print("ğŸ“Š LoCoMo LLM å¢å¼ºè¯„ä¼°æ€»ç»“")
    print(f"{'='*80}")
    
    print(f"æ€»é—®é¢˜æ•°: {results['total_questions']}")
    print(f"æµ‹è¯•æ¨¡å‹: {', '.join(results['models'].keys())}")
    
    # æ¨¡å‹å¯¹æ¯”
    print(f"\nğŸ† æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
    for model_name, model_metrics in metrics.items():
        print(f"\n{model_name}:")
        print(f"  ç»¼åˆå¾—åˆ†: {model_metrics.get('avg_combined_score', 0):.4f} Â± {model_metrics.get('std_combined_score', 0):.4f}")
        print(f"  F1å¾—åˆ†: {model_metrics.get('avg_f1_score', 0):.4f}")
        print(f"  è¯­ä¹‰ç›¸ä¼¼åº¦: {model_metrics.get('avg_similarity_score', 0):.4f}")
        print(f"  ç²¾ç¡®åŒ¹é…ç‡: {model_metrics.get('exact_match_rate', 0):.4f}")
        
        # æŒ‰ç±»åˆ«æ˜¾ç¤º
        categories = model_metrics.get('categories', {})
        if categories:
            print(f"  æŒ‰ç±»åˆ«ç»Ÿè®¡:")
            for cat_key, cat_data in categories.items():
                print(f"    {cat_key}: {cat_data['count']} é—®é¢˜, "
                      f"å¾—åˆ† {cat_data['avg_score']:.3f}, "
                      f"F1 {cat_data['avg_f1']:.3f}, "
                      f"EM {cat_data['exact_match_rate']:.3f}")

def save_llm_results(results: Dict[str, Any], metrics: Dict[str, Any], timestamp: str):
    """ä¿å­˜LLMè¯„ä¼°ç»“æœ"""
    result_file = RESULTS_DIR / f'locomo_llm_evaluation_{timestamp}.json'
    
    output_data = {
        'timestamp': timestamp,
        'config': {
            'dataset_path': str(DATASET_PATH),
            'top_k_results': TOP_K_RESULTS,
            'evaluation_model': EVALUATION_MODEL,
            'llm_models': LLM_MODELS
        },
        'results': results,
        'metrics': metrics
    }
    
    # è½¬æ¢numpyç±»å‹
    def convert_numpy(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return obj
    
    def deep_convert(data):
        if isinstance(data, dict):
            return {k: deep_convert(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [deep_convert(item) for item in data]
        else:
            return convert_numpy(data)
    
    output_data = deep_convert(output_data)
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    logging.info(f"LLMè¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_file}")

# åœ¨ç°æœ‰çš„å¯¼å…¥éƒ¨åˆ†åæ·»åŠ è¿™äº›å‡½æ•°ï¼š

def load_dataset(file_path: Path) -> List[Dict[str, Any]]:
    """åŠ è½½LoCoMoæ•°æ®é›†æ–‡ä»¶ã€‚"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            logging.info(f"æˆåŠŸåŠ è½½æ•°æ®é›†: {file_path}")
            logging.info(f"æ•°æ®é›†åŒ…å« {len(data)} ä¸ªæ ·æœ¬")
            return data
    except FileNotFoundError:
        logging.error(f"æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return []
    except json.JSONDecodeError as e:
        logging.error(f"JSONè§£æé”™è¯¯: {e}")
        return []

def parse_locomo_data(data: List[Dict[str, Any]]) -> Tuple[List[Dict], List[Dict]]:
    """
    è§£æLoCoMoæ•°æ®é›†ï¼Œåˆ†ç¦»å¯¹è¯å†å²å’ŒQAå¯¹
    
    Returns:
        Tuple[å¯¹è¯å†å², QAå¯¹åˆ—è¡¨]
    """
    conversations = []
    qa_pairs = []
    
    for sample in data:
        sample_id = sample.get('sample_id', 'unknown')
        
        # å¤„ç†å¯¹è¯å†å² - ä»conversationå­—æ®µæå–
        if 'conversation' in sample:
            conversation_data = sample['conversation']
            dialog_messages = []
            
            # æå–æ‰€æœ‰sessionçš„å¯¹è¯
            for key, value in conversation_data.items():
                if key.startswith('session_') and not key.endswith('_date_time') and isinstance(value, list):
                    session_time = conversation_data.get(f"{key}_date_time", "")
                    for msg_idx, message in enumerate(value):
                        if isinstance(message, dict):
                            dialog_messages.append({
                                'speaker': message.get('speaker', 'unknown'),
                                'content': message.get('message', message.get('text', '')),
                                'timestamp': session_time,
                                'session': key,
                                'message_index': msg_idx
                            })
            
            if dialog_messages:
                conversations.append({
                    'dialog_id': sample_id,
                    'messages': dialog_messages,
                    'speaker_a': conversation_data.get('speaker_a', 'Speaker A'),
                    'speaker_b': conversation_data.get('speaker_b', 'Speaker B')
                })
        
        # å¤„ç†QAå¯¹
        if 'qa' in sample:
            for qa_idx, qa in enumerate(sample['qa']):
                if isinstance(qa, dict) and qa.get('question') and qa.get('answer'):
                    qa_pairs.append({
                        'sample_id': sample_id,
                        'qa_index': qa_idx,
                        'question': qa.get('question'),
                        'answer': qa.get('answer'),
                        'evidence': qa.get('evidence', []),
                        'category': qa.get('category', 0),
                        'adversarial_answer': qa.get('adversarial_answer', None)
                    })
    
    logging.info(f"è§£æå¾—åˆ° {len(conversations)} ä¸ªå¯¹è¯ï¼Œ{len(qa_pairs)} ä¸ªQAå¯¹")
    return conversations, qa_pairs

def extract_all_speakers(data: List[Dict]) -> List[str]:
    """ä»æ•°æ®é›†ä¸­æå–æ‰€æœ‰è¯´è¯äºº"""
    speakers = set()
    
    for sample in data:
        if 'conversation' in sample:
            conversation_data = sample['conversation']
            
            # æå– speaker_a å’Œ speaker_b
            speaker_a = conversation_data.get('speaker_a')
            speaker_b = conversation_data.get('speaker_b')
            if speaker_a:
                speakers.add(speaker_a.lower())
            if speaker_b:
                speakers.add(speaker_b.lower())
            
            # ä»å…·ä½“å¯¹è¯æ¶ˆæ¯ä¸­æå–è¯´è¯äºº
            for key, value in conversation_data.items():
                if key.startswith('session_') and not key.endswith('_date_time') and isinstance(value, list):
                    for message in value:
                        if isinstance(message, dict):
                            speaker = message.get('speaker')
                            if speaker:
                                speakers.add(speaker.lower())
    
    return sorted(list(speakers))

def setup_locomo_memory_spaces(graph: SemanticGraph, data: List[Dict]):
    """ä¸ºLoCoMoæ•°æ®é›†åˆ›å»ºä¸“é—¨çš„å†…å­˜ç©ºé—´"""
    
    # æŒ‰æ•°æ®ç±»å‹åˆ›å»ºç©ºé—´
    dialog_space = graph.create_memory_space_in_map("locomo_dialogs")
    qa_space = graph.create_memory_space_in_map("locomo_qa_pairs")
    
    # æŒ‰QAç±»åˆ«åˆ›å»ºç»†åˆ†ç©ºé—´
    for category in [1, 2, 3, 4, 5]:
        category_space = graph.create_memory_space_in_map(f"locomo_qa_category_{category}")
    
    # åŠ¨æ€æå–è¯´è¯äººå¹¶åˆ›å»ºç©ºé—´
    speakers = extract_all_speakers(data)
    logging.info(f"å‘ç° {len(speakers)} ä¸ªè¯´è¯äºº: {speakers}")
    
    for speaker in speakers:
        speaker_space = graph.create_memory_space_in_map(f"speaker_{speaker}")
    
    logging.info("LoCoMoå†…å­˜ç©ºé—´å·²åˆ›å»º")

def ingest_conversation_history(graph: SemanticGraph, data: List[Dict]) -> int:
    """å°†LoCoMoæ•°æ®é›†æ³¨å…¥åˆ°SemanticGraphä¸­"""
    logging.info("å¼€å§‹æ³¨å…¥LoCoMoå¯¹è¯å†å²...")
    total_messages = 0
    
    # è®¾ç½®å†…å­˜ç©ºé—´ï¼ˆä¼ å…¥æ•°æ®ä»¥åŠ¨æ€æå–è¯´è¯äººï¼‰
    setup_locomo_memory_spaces(graph, data)
    
    for sample in data:
        sample_id = sample.get('sample_id', f'sample_{total_messages}')
        
        # å¤„ç†å¯¹è¯å†å²
        if 'conversation' in sample:
            conversation_data = sample['conversation']
            speaker_a = conversation_data.get('speaker_a', 'Speaker A')
            speaker_b = conversation_data.get('speaker_b', 'Speaker B')
            
            # å¤„ç†session summariesä½œä¸ºé«˜çº§è®°å¿†å•å…ƒ
            if 'session_summary' in sample:
                for session_key, summary in sample['session_summary'].items():
                    if isinstance(summary, str) and summary.strip():
                        unit_id = f"{sample_id}_{session_key}_summary"
                        
                        summary_unit = MemoryUnit(
                            uid=unit_id,
                            raw_data={
                                "text_content": summary,
                                "content_type": "session_summary",
                                "session": session_key,
                                "sample_id": sample_id,
                                "speakers": f"{speaker_a} & {speaker_b}"
                            },
                            metadata={
                                "data_source": "locomo_summary",
                                "sample_id": sample_id,
                                "session": session_key,
                                "content_type": "summary"
                            }
                        )
                        
                        graph.add_unit(summary_unit)
                        graph.add_unit_to_space_in_map(unit_id, "locomo_dialogs")
                        total_messages += 1
            
            # å¤„ç†å…·ä½“çš„å¯¹è¯æ¶ˆæ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            for key, value in conversation_data.items():
                if key.startswith('session_') and not key.endswith('_date_time') and isinstance(value, list):
                    session_time = conversation_data.get(f"{key}_date_time", "")
                    
                    for msg_idx, message in enumerate(value):
                        if isinstance(message, dict) and message.get('message'):
                            unit_id = f"{sample_id}_{key}_msg_{msg_idx}"
                            speaker = message.get('speaker', 'unknown')
                            content = message.get('message', '')
                            
                            if content.strip():
                                unit = MemoryUnit(
                                    uid=unit_id,
                                    raw_data={
                                        "text_content": f"{speaker}: {content}",
                                        "speaker": speaker,
                                        "message_content": content,
                                        "session": key,
                                        "sample_id": sample_id
                                    },
                                    metadata={
                                        "timestamp": session_time,
                                        "conversation_id": sample_id,
                                        "session": key,
                                        "message_index": msg_idx,
                                        "data_source": "locomo_dialog",
                                        "speaker": speaker
                                    }
                                )
                                
                                graph.add_unit(unit)
                                graph.add_unit_to_space_in_map(unit_id, "locomo_dialogs")
                                
                                # æŒ‰è¯´è¯äººåˆ†ç±»
                                speaker_space = f"speaker_{speaker.lower().replace(' ', '_')}"
                                graph.add_unit_to_space_in_map(unit_id, speaker_space)
                                
                                total_messages += 1
        
        # å¤„ç†QAå¯¹ä½œä¸ºç‹¬ç«‹çš„çŸ¥è¯†ç‚¹ï¼ˆå¯é€‰ï¼Œç”¨äºå‚è€ƒï¼‰
        if 'qa' in sample:
            for qa_idx, qa in enumerate(sample['qa']):
                if isinstance(qa, dict) and qa.get('question') and qa.get('answer'):
                    qa_unit_id = f"{sample_id}_qa_{qa_idx}"
                    
                    qa_unit = MemoryUnit(
                        uid=qa_unit_id,
                        raw_data={
                            "text_content": f"Q: {qa['question']} A: {qa['answer']}",
                            "question": qa['question'],
                            "answer": qa['answer'],
                            "evidence": qa.get('evidence', []),
                            "category": qa.get('category', 0)
                        },
                        metadata={
                            "data_source": "locomo_qa",
                            "sample_id": sample_id,
                            "qa_category": qa.get('category', 0),
                            "evidence_sources": qa.get('evidence', [])
                        }
                    )
                    
                    graph.add_unit(qa_unit)
                    graph.add_unit_to_space_in_map(qa_unit_id, "locomo_qa_pairs")
                    
                    # æŒ‰ç±»åˆ«åˆ†ç±»
                    category = qa.get('category', 0)
                    if category > 0:
                        graph.add_unit_to_space_in_map(qa_unit_id, f"locomo_qa_category_{category}")
    
    logging.info(f"æ•°æ®æ³¨å…¥å®Œæˆã€‚å…±æ·»åŠ  {total_messages} ä¸ªè®°å¿†å•å…ƒ")
    return total_messages

def format_enhanced_context(retrieved_units: List[Tuple]) -> str:
    """æ ¼å¼åŒ–å¢å¼ºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    if not retrieved_units:
        return "æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³è®°å¿†ã€‚"
    
    context_lines = ["--- æ£€ç´¢åˆ°çš„ç›¸å…³è®°å¿†ä¸Šä¸‹æ–‡ ---"]
    
    for unit, score in retrieved_units:
        data_source = unit.metadata.get('data_source', 'unknown')
        
        if data_source == 'locomo_summary':
            session = unit.raw_data.get('session', 'unknown')
            content = unit.raw_data.get('text_content', '')
            speakers = unit.raw_data.get('speakers', 'Unknown')
            line = f"[æ‘˜è¦-{session}] {speakers}: {content[:200]}... (ç›¸ä¼¼åº¦: {score:.4f})"
            
        elif data_source == 'locomo_dialog':
            speaker = unit.raw_data.get('speaker', 'unknown')
            content = unit.raw_data.get('message_content', '')
            session = unit.metadata.get('session', 'unknown')
            line = f"[å¯¹è¯-{session}] {speaker}: {content} (ç›¸ä¼¼åº¦: {score:.4f})"
            
        elif data_source == 'locomo_qa':
            question = unit.raw_data.get('question', '')
            answer = unit.raw_data.get('answer', '')
            category = unit.metadata.get('qa_category', 0)
            line = f"[QA-ç±»åˆ«{category}] Q: {question} A: {answer} (ç›¸ä¼¼åº¦: {score:.4f})"
            
        else:
            content = unit.raw_data.get('text_content', '')[:100]
            line = f"[{data_source}] {content}... (ç›¸ä¼¼åº¦: {score:.4f})"
        
        context_lines.append(line)
    
    context_lines.append("-" * 60)
    return "\n".join(context_lines)

def extract_answer_from_context(question: str, context: str, retrieved_units: List[Tuple]) -> str:
    """
    ä»æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­æå–ç­”æ¡ˆ
    è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ç­”æ¡ˆæå–é€»è¾‘ï¼Œå®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨LLM
    """
    if not retrieved_units:
        return "æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
    
    # ç®€å•çš„ç­”æ¡ˆæå–ï¼šå–ç›¸ä¼¼åº¦æœ€é«˜çš„å†…å®¹ä½œä¸ºç­”æ¡ˆçš„åŸºç¡€
    best_unit, best_score = retrieved_units[0]
    
    # æ ¹æ®ä¸åŒçš„æ•°æ®æºæå–ç­”æ¡ˆ
    data_source = best_unit.metadata.get('data_source', 'unknown')
    
    if data_source == 'locomo_qa' and best_score > 0.3:
        # å¦‚æœæ£€ç´¢åˆ°çš„æ˜¯QAå¯¹ï¼Œç›´æ¥ä½¿ç”¨ç­”æ¡ˆ
        return best_unit.raw_data.get('answer', 'æœªæ‰¾åˆ°ç­”æ¡ˆ')
    elif data_source in ['locomo_dialog', 'locomo_summary'] and best_score > 0.3:
        # å¦‚æœæ£€ç´¢åˆ°çš„æ˜¯å¯¹è¯æˆ–æ‘˜è¦ï¼Œæå–ç›¸å…³å†…å®¹
        content = best_unit.raw_data.get('text_content', '')
        return f"æ ¹æ®å¯¹è¯è®°å½•: {content[:200]}"
    else:
        return "æœªæ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„ä¿¡æ¯"

def enhanced_search_and_evaluate(graph: SemanticGraph, qa_pairs: List[Dict], eval_model: SentenceTransformer) -> Dict[str, Any]:
    """å¢å¼ºçš„æœç´¢å’Œè¯„ä¼°å‡½æ•°"""
    results = {
        'total_questions': len(qa_pairs),
        'category_results': {},
        'retrieval_results': [],
        'all_scores': []
    }
    
    for qa in qa_pairs:
        question = qa.get('question')
        golden_answer = qa.get('answer')
        category = qa.get('category', 0)
        evidence = qa.get('evidence', [])
        
        if not question or not golden_answer:
            continue
        
        # 1. å¤šç­–ç•¥æ£€ç´¢
        search_results = {}
        
        # å…¨å±€æœç´¢
        search_results['global'] = graph.search_similarity_in_graph(
            query_text=question, k=5
        )
        
        # åœ¨å¯¹è¯ç©ºé—´ä¸­æœç´¢
        try:
            search_results['dialog_only'] = graph.search_similarity_in_graph(
                query_text=question, k=5, space_name="locomo_dialogs"
            )
        except:
            search_results['dialog_only'] = []
        
        # åœ¨åŒç±»åˆ«QAä¸­æœç´¢
        if category > 0:
            try:
                search_results[f'category_{category}'] = graph.search_similarity_in_graph(
                    query_text=question, k=3, space_name=f"locomo_qa_category_{category}"
                )
            except:
                search_results[f'category_{category}'] = []
        
        # 2. èåˆæ£€ç´¢ç»“æœ
        all_retrieved = []
        for strategy, units in search_results.items():
            for unit, score in units:
                all_retrieved.append((unit, score, strategy))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶å»é‡
        seen_uids = set()
        unique_retrieved = []
        for unit, score, strategy in sorted(all_retrieved, key=lambda x: x[1], reverse=True):
            if unit.uid not in seen_uids:
                unique_retrieved.append((unit, score, strategy))
                seen_uids.add(unit.uid)
        
        # 3. ç”Ÿæˆä¸Šä¸‹æ–‡å’Œç­”æ¡ˆ
        top_retrieved = unique_retrieved[:TOP_K_RESULTS]
        context_str = format_enhanced_context([(u, s) for u, s, _ in top_retrieved])
        predicted_answer = extract_answer_from_context(question, context_str, [(u, s) for u, s, _ in top_retrieved])
        
        # 4. è¯„ä¼°
        try:
            # ä½¿ç”¨F1å¾—åˆ†è¯„ä¼°
            f1_score_result = f1_score(predicted_answer, golden_answer)
            
            # ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°
            embedding1 = eval_model.encode(predicted_answer, convert_to_tensor=True)
            embedding2 = eval_model.encode(golden_answer, convert_to_tensor=True)
            similarity_score = util.cos_sim(embedding1, embedding2).item()
            
            # ç»¼åˆå¾—åˆ†
            combined_score = (f1_score_result + similarity_score) / 2
            
        except Exception as e:
            logging.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            f1_score_result = 0
            similarity_score = 0
            combined_score = 0
        
        # 5. è®°å½•ç»“æœ
        result_entry = {
            'question': question,
            'golden_answer': golden_answer,
            'predicted_answer': predicted_answer,
            'category': category,
            'evidence': evidence,
            'retrieved_count': len(unique_retrieved),
            'f1_score': f1_score_result,
            'similarity_score': similarity_score,
            'combined_score': combined_score,
            'context': context_str,
            'search_strategies': list(search_results.keys())
        }
        
        results['retrieval_results'].append(result_entry)
        results['all_scores'].append(combined_score)
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        if category not in results['category_results']:
            results['category_results'][category] = {
                'count': 0,
                'scores': [],
                'f1_scores': [],
                'similarity_scores': [],
                'avg_retrieved': 0
            }
        
        results['category_results'][category]['count'] += 1
        results['category_results'][category]['scores'].append(combined_score)
        results['category_results'][category]['f1_scores'].append(f1_score_result)
        results['category_results'][category]['similarity_scores'].append(similarity_score)
        results['category_results'][category]['avg_retrieved'] += len(unique_retrieved)
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print(f"\n{'='*60}")
        print(f"ç±»åˆ« {category} | è¯æ®: {evidence}")
        print(f"â“ é—®é¢˜: {question}")
        print(f"âœ… æ ‡å‡†ç­”æ¡ˆ: {golden_answer}")
        print(f"ğŸ¤– é¢„æµ‹ç­”æ¡ˆ: {predicted_answer}")
        print(f"ğŸ” æ£€ç´¢ç­–ç•¥: {', '.join(search_results.keys())}")
        print(f"ğŸ“„ æ£€ç´¢åˆ° {len(unique_retrieved)} ä¸ªç›¸å…³å•å…ƒ")
        print(f"ğŸ“Š F1å¾—åˆ†: {f1_score_result:.4f}")
        print(f"ğŸ“Š è¯­ä¹‰ç›¸ä¼¼åº¦: {similarity_score:.4f}")
        print(f"ğŸ“Š ç»¼åˆå¾—åˆ†: {combined_score:.4f}")
        print(context_str)
    
    return results

def calculate_final_metrics(results: Dict[str, Any]) -> Dict[str, float]:
    """è®¡ç®—æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡"""
    total_questions = results['total_questions']
    metrics = {}
    
    if total_questions > 0:
        # æ£€ç´¢æˆåŠŸç‡
        successful_retrievals = sum(1 for r in results['retrieval_results'] if r['retrieved_count'] > 0)
        metrics['retrieval_success_rate'] = successful_retrievals / total_questions
        
        # å¹³å‡å¾—åˆ†
        if results['all_scores']:
            metrics['avg_combined_score'] = np.mean(results['all_scores'])
            metrics['std_combined_score'] = np.std(results['all_scores'])
        
        # æŒ‰ç±»åˆ«çš„æŒ‡æ ‡
        for category, cat_results in results['category_results'].items():
            if cat_results['count'] > 0:
                cat_results['avg_retrieved'] = cat_results['avg_retrieved'] / cat_results['count']
                
                if cat_results['scores']:
                    metrics[f'category_{category}_avg_score'] = np.mean(cat_results['scores'])
                    metrics[f'category_{category}_avg_f1'] = np.mean(cat_results['f1_scores'])
                    metrics[f'category_{category}_avg_similarity'] = np.mean(cat_results['similarity_scores'])
    
    return metrics

def save_results(results: Dict[str, Any], metrics: Dict[str, float], timestamp: str):
    """ä¿å­˜è¯„ä¼°ç»“æœ"""
    result_file = RESULTS_DIR / f'locomo_evaluation_{timestamp}.json'
    
    output_data = {
        'timestamp': timestamp,
        'config': {
            'dataset_path': str(DATASET_PATH),
            'top_k_results': TOP_K_RESULTS,
            'evaluation_model': EVALUATION_MODEL
        },
        'results': results,
        'metrics': metrics
    }
    
    # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
    def convert_numpy(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return obj
    
    def deep_convert(data):
        if isinstance(data, dict):
            return {k: deep_convert(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [deep_convert(item) for item in data]
        else:
            return convert_numpy(data)
    
    output_data = deep_convert(output_data)
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    logging.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_file}")

def print_evaluation_summary(results: Dict[str, Any], metrics: Dict[str, float]):
    """æ‰“å°è¯„ä¼°æ€»ç»“"""
    print(f"\n{'='*80}")
    print("ğŸ“Š LoCoMo è¯„ä¼°æ€»ç»“")
    print(f"{'='*80}")
    
    if results['all_scores']:
        print(f"æ€»é—®é¢˜æ•°: {results['total_questions']}")
        print(f"æ£€ç´¢æˆåŠŸç‡: {metrics.get('retrieval_success_rate', 0):.4f}")
        print(f"å¹³å‡ç»¼åˆå¾—åˆ†: {metrics.get('avg_combined_score', 0):.4f} Â± {metrics.get('std_combined_score', 0):.4f}")
        
        print(f"\næŒ‰ç±»åˆ«ç»Ÿè®¡:")
        for category, cat_results in results['category_results'].items():
            if cat_results['scores']:
                cat_avg = metrics.get(f'category_{category}_avg_score', 0)
                cat_f1 = metrics.get(f'category_{category}_avg_f1', 0)
                cat_sim = metrics.get(f'category_{category}_avg_similarity', 0)
                print(f"  ç±»åˆ« {category}: {cat_results['count']} é—®é¢˜, "
                      f"ç»¼åˆå¾—åˆ† {cat_avg:.4f}, "
                      f"F1å¾—åˆ† {cat_f1:.4f}, "
                      f"è¯­ä¹‰ç›¸ä¼¼åº¦ {cat_sim:.4f}, "
                      f"å¹³å‡æ£€ç´¢æ•° {cat_results['avg_retrieved']:.1f}")

def analyze_dataset_structure(data: List[Dict]) -> Dict[str, Any]:
    """åˆ†ææ•°æ®é›†ç»“æ„"""
    stats = {
        'total_samples': len(data),
        'speakers': {},
        'qa_categories': {},
        'sessions_per_sample': [],
        'qa_per_sample': []
    }
    
    all_speakers = set()
    all_categories = set()
    
    for sample in data:
        sample_id = sample.get('sample_id', 'unknown')
        
        # åˆ†æå¯¹è¯
        if 'conversation' in sample:
            conversation_data = sample['conversation']
            speaker_a = conversation_data.get('speaker_a')
            speaker_b = conversation_data.get('speaker_b')
            
            if speaker_a:
                all_speakers.add(speaker_a)
            if speaker_b:
                all_speakers.add(speaker_b)
            
            # ç»Ÿè®¡sessionæ•°é‡
            session_count = sum(1 for key in conversation_data.keys() 
                              if key.startswith('session_') and not key.endswith('_date_time'))
            stats['sessions_per_sample'].append(session_count)
        
        # åˆ†æQA
        if 'qa' in sample:
            qa_count = len(sample['qa'])
            stats['qa_per_sample'].append(qa_count)
            
            for qa in sample['qa']:
                category = qa.get('category', 0)
                if category:
                    all_categories.add(category)
    
    stats['speakers'] = {
        'total_unique': len(all_speakers),
        'list': sorted(list(all_speakers))
    }
    
    stats['qa_categories'] = {
        'total_categories': len(all_categories),
        'list': sorted(list(all_categories))
    }
    
    if stats['sessions_per_sample']:
        stats['avg_sessions_per_sample'] = np.mean(stats['sessions_per_sample'])
    
    if stats['qa_per_sample']:
        stats['avg_qa_per_sample'] = np.mean(stats['qa_per_sample'])
    
    return stats

# ä¿®æ”¹ main å‡½æ•°ï¼Œå»æ‰LLMå¢å¼ºè¯„ä¼°éƒ¨åˆ†ï¼Œä½¿ç”¨åŸºç¡€è¯„ä¼°
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        logging.info("åˆå§‹åŒ–AgentMemorySystem...")
        hippo_graph = SemanticGraph()
        
        # åŠ è½½æ•°æ®é›†
        logging.info(f"åŠ è½½æ•°æ®é›†: {DATASET_PATH}")
        raw_data = load_dataset(DATASET_PATH)
        if not raw_data:
            logging.error("æ•°æ®é›†åŠ è½½å¤±è´¥")
            return
        
        # åˆ†ææ•°æ®é›†ç»“æ„
        dataset_stats = analyze_dataset_structure(raw_data)
        logging.info(f"æ•°æ®é›†ç»Ÿè®¡: {dataset_stats}")
        
        # è§£ææ•°æ®
        conversations, qa_pairs = parse_locomo_data(raw_data)
        
        # æ³¨å…¥å¯¹è¯å†å²
        total_messages = ingest_conversation_history(hippo_graph, raw_data)
        if total_messages == 0:
            logging.error("æ²¡æœ‰æˆåŠŸæ³¨å…¥ä»»ä½•å¯¹è¯æ•°æ®")
            return
        
        # æ„å»ºç´¢å¼•
        logging.info("æ„å»ºå‘é‡ç´¢å¼•...")
        hippo_graph.build_semantic_map_index()
        logging.info("ç´¢å¼•æ„å»ºå®Œæˆ")

        # æ˜¾ç¤ºå›¾è°±ç»Ÿè®¡ä¿¡æ¯
        logging.info("æ˜¾ç¤ºå›¾è°±ç»Ÿè®¡ä¿¡æ¯...")
        hippo_graph.display_graph_summary()
        
        # åŠ è½½è¯„ä¼°æ¨¡å‹
        logging.info(f"åŠ è½½è¯„ä¼°æ¨¡å‹: {EVALUATION_MODEL}...")
        eval_model = SentenceTransformer(EVALUATION_MODEL)
        
        # æ‰§è¡ŒåŸºç¡€è¯„ä¼°
        results = enhanced_search_and_evaluate(hippo_graph, qa_pairs, eval_model)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = calculate_final_metrics(results)
        
        # æ‰“å°æ€»ç»“
        print_evaluation_summary(results, metrics)
        
        # ä¿å­˜ç»“æœ
        save_results(results, metrics, timestamp)
        
    except Exception as e:
        logging.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
    
    logging.info("è¯„ä¼°å®Œæˆ")

if __name__ == "__main__":
    main()
