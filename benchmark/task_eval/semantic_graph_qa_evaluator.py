import logging
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import numpy as np
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from dev.semantic_graph import SemanticGraph
from benchmark.extractor.dataset_inserter import ConversationSemanticStorage
from benchmark.llm_utils.llm_client import LLMClient
from benchmark.task_eval.evaluation import eval_question_answering, calculate_f1_score


class SemanticGraphQAEvaluator:
    """
    åŸºäºè¯­ä¹‰å›¾è°±çš„QAè¯„ä¼°å™¨
    ç»“åˆä¿¡æ¯æ£€ç´¢ã€æç¤ºè¯å·¥ç¨‹å’Œç­”æ¡ˆç”Ÿæˆçš„å®Œæ•´è¯„ä¼°æµç¨‹
    """
    
    def __init__(self, 
                 semantic_graph: SemanticGraph,
                 llm_client: LLMClient,
                 output_dir: str = "benchmark/task_eval/results"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            semantic_graph: å·²æ„å»ºå¥½çš„è¯­ä¹‰å›¾è°±
            llm_client: LLMå®¢æˆ·ç«¯
            output_dir: è¾“å‡ºç›®å½•
        """
        self.semantic_graph = semantic_graph
        self.llm_client = llm_client
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger(__name__)
        
        # QAç±»åˆ«å¯¹åº”çš„æ£€ç´¢ç­–ç•¥
        self.category_strategies = {
            1: "multi_hop",      # å¤šè·³æ¨ç†
            2: "single_hop",     # å•è·³äº‹å®
            3: "temporal",       # æ—¶é—´ç›¸å…³
            4: "open_domain",    # å¼€æ”¾åŸŸ
            5: "adversarial"     # å¯¹æŠ—æ€§
        }
        
        # æ£€ç´¢é…ç½®
        self.retrieval_config = {
            1: {"top_k": 8, "include_relations": True, "expand_neighbors": True},
            2: {"top_k": 5, "include_relations": False, "expand_neighbors": False},
            3: {"top_k": 6, "include_relations": True, "expand_neighbors": False},
            4: {"top_k": 10, "include_relations": True, "expand_neighbors": True},
            5: {"top_k": 3, "include_relations": False, "expand_neighbors": False}
        }

    def retrieve_context_for_question(self, 
                                     question: str, 
                                     category: int, 
                                     conversation_id: str,
                                     evidence: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        æ ¹æ®é—®é¢˜ç±»åˆ«æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            category: QAç±»åˆ« (1-5)
            conversation_id: å¯¹è¯ID
            evidence: è¯æ®åˆ—è¡¨ï¼ˆç”¨äºè¯„ä¼°å¬å›ç‡ï¼‰
            
        Returns:
            æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        config = self.retrieval_config.get(category, self.retrieval_config[2])
        strategy = self.category_strategies.get(category, "single_hop")
        
        context = {
            "question": question,
            "category": category,
            "strategy": strategy,
            "conversation_id": conversation_id,
            "retrieved_nodes": [],
            "retrieved_relations": [],
            "context_text": "",
            "evidence_recall": 0.0
        }
        
        try:
            # 1. åŸºç¡€è¯­ä¹‰æ£€ç´¢
            search_results = self.semantic_graph.search_similarity_in_graph(
                query_text=question,
                k=config["top_k"],
                ms_names=[f"conversation_{conversation_id}"],
                recursive=True
            )
            
            context["retrieved_nodes"] = [
                {
                    "uid": unit.uid,
                    "score": float(score),
                    "content": unit.raw_data.get("text_content", "")[:200],
                    "data_type": unit.raw_data.get("data_type", "unknown"),
                    "data_source": unit.raw_data.get("data_source", "unknown")
                }
                for unit, score in search_results
            ]
            
            # 2. æ ¹æ®ç±»åˆ«è¿›è¡Œç‰¹æ®Šå¤„ç†
            if strategy == "multi_hop" and config["expand_neighbors"]:
                # å¤šè·³ï¼šæ‰©å±•é‚»å±…èŠ‚ç‚¹
                expanded_nodes = self._expand_with_neighbors(search_results[:3], conversation_id)
                context["retrieved_nodes"].extend(expanded_nodes)
            
            elif strategy == "temporal":
                # æ—¶é—´ç›¸å…³ï¼šä¼˜å…ˆæ£€ç´¢æ—¶é—´ç›¸å…³ä¿¡æ¯
                temporal_nodes = self._retrieve_temporal_context(question, conversation_id)
                context["retrieved_nodes"].extend(temporal_nodes)
            
            # 3. æ£€ç´¢å…³ç³»ä¿¡æ¯
            if config["include_relations"]:
                context["retrieved_relations"] = self._retrieve_relations(
                    [node["uid"] for node in context["retrieved_nodes"][:5]]
                )
            
            # 4. æ„å»ºä¸Šä¸‹æ–‡æ–‡æœ¬
            context["context_text"] = self._build_context_text(
                context["retrieved_nodes"], 
                context["retrieved_relations"],
                strategy
            )
            
            # 5. è®¡ç®—è¯æ®å¬å›ç‡
            if evidence:
                context["evidence_recall"] = self._calculate_evidence_recall(
                    context["retrieved_nodes"], evidence
                )
                
        except Exception as e:
            self.logger.error(f"æ£€ç´¢ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            context["error"] = str(e)
        
        return context

    def _expand_with_neighbors(self, 
                              top_results: List[Tuple], 
                              conversation_id: str) -> List[Dict]:
        """æ‰©å±•é‚»å±…èŠ‚ç‚¹ï¼ˆç”¨äºå¤šè·³æ¨ç†ï¼‰"""
        expanded_nodes = []
        
        for unit, score in top_results:
            # è·å–æ˜¾å¼é‚»å±…
            explicit_neighbors = self.semantic_graph.traverse_explicit_nodes(
                uid=unit.uid,
                space_name=f"conversation_{conversation_id}"
            )
            
            for neighbor in explicit_neighbors[:2]:  # é™åˆ¶é‚»å±…æ•°é‡
                expanded_nodes.append({
                    "uid": neighbor.uid,
                    "score": float(score * 0.8),  # é™ä½é‚»å±…åˆ†æ•°
                    "content": neighbor.raw_data.get("text_content", "")[:200],
                    "data_type": neighbor.raw_data.get("data_type", "unknown"),
                    "data_source": neighbor.raw_data.get("data_source", "unknown"),
                    "relation_type": "neighbor"
                })
        
        return expanded_nodes

    def _retrieve_temporal_context(self, question: str, conversation_id: str) -> List[Dict]:
        """æ£€ç´¢æ—¶é—´ç›¸å…³ä¸Šä¸‹æ–‡"""
        temporal_keywords = ["when", "date", "time", "day", "week", "month", "year", "ago", "recently", "yesterday", "today"]
        
        # æ£€æŸ¥é—®é¢˜æ˜¯å¦åŒ…å«æ—¶é—´å…³é”®è¯
        if not any(keyword in question.lower() for keyword in temporal_keywords):
            return []
        
        # ä¼˜å…ˆæ£€ç´¢åŒ…å«æ—¶é—´ä¿¡æ¯çš„èŠ‚ç‚¹
        time_results = self.semantic_graph.semantic_map.filter_memory_units(
            ms_names=[f"conversation_{conversation_id}"],
            filter_condition={
                "data_type": {"in": ["summary", "event", "observation"]}
            }
        )
        
        temporal_nodes = []
        for unit in time_results[:3]:
            temporal_nodes.append({
                "uid": unit.uid,
                "score": 0.9,
                "content": unit.raw_data.get("text_content", "")[:200],
                "data_type": unit.raw_data.get("data_type", "unknown"),
                "data_source": unit.raw_data.get("data_source", "unknown"),
                "relation_type": "temporal"
            })
        
        return temporal_nodes

    def _retrieve_relations(self, node_uids: List[str]) -> List[Dict]:
        """æ£€ç´¢èŠ‚ç‚¹é—´çš„å…³ç³»"""
        relations = []
        
        for i, source_uid in enumerate(node_uids):
            for target_uid in node_uids[i+1:]:
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ˜¾å¼å…³ç³»
                if self.semantic_graph.nx_graph.has_edge(source_uid, target_uid):
                    edge_data = self.semantic_graph.nx_graph.get_edge_data(source_uid, target_uid)
                    relations.append({
                        "source": source_uid,
                        "target": target_uid,
                        "type": edge_data.get("type", "RELATED_TO"),
                        "properties": {k: v for k, v in edge_data.items() if k != "type"}
                    })
        
        return relations

    def _build_context_text(self, 
                           nodes: List[Dict], 
                           relations: List[Dict], 
                           strategy: str) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡æ–‡æœ¬"""
        context_parts = []
        
        # æ·»åŠ èŠ‚ç‚¹ä¿¡æ¯
        if strategy == "multi_hop":
            context_parts.append("=== CONVERSATION CONTEXT (Multi-hop Analysis) ===")
        elif strategy == "temporal":
            context_parts.append("=== CONVERSATION CONTEXT (Temporal Information) ===")
        else:
            context_parts.append("=== CONVERSATION CONTEXT ===")
        
        for i, node in enumerate(nodes[:8]):  # é™åˆ¶èŠ‚ç‚¹æ•°é‡
            data_type = node.get("data_type", "content")
            content = node.get("content", "")
            
            if content.strip():
                context_parts.append(f"[{i+1}] {data_type.upper()}: {content}")
        
        # æ·»åŠ å…³ç³»ä¿¡æ¯
        if relations:
            context_parts.append("\n=== RELATIONSHIPS ===")
            for rel in relations[:5]:  # é™åˆ¶å…³ç³»æ•°é‡
                context_parts.append(
                    f"- {rel['source']} --[{rel['type']}]--> {rel['target']}"
                )
        
        return "\n".join(context_parts)

    def _calculate_evidence_recall(self, retrieved_nodes: List[Dict], evidence: List[str]) -> float:
        """è®¡ç®—è¯æ®å¬å›ç‡"""
        if not evidence:
            return 1.0
        
        retrieved_content = " ".join([node.get("content", "") for node in retrieved_nodes])
        
        found_evidence = 0
        for ev in evidence:
            # ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…æ£€æŸ¥
            if any(keyword in retrieved_content.lower() for keyword in ev.lower().split()):
                found_evidence += 1
        
        return found_evidence / len(evidence) if evidence else 0.0
    
    # åœ¨ semantic_graph_qa_evaluator.py ä¸­ä¿®å¤ generate_answer æ–¹æ³•

    def generate_answer(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ - ä½¿ç”¨å¢å¼ºç‰ˆLLMClient"""
        question = context["question"]
        category = context["category"]
        context_text = context["context_text"]
        
        # æ ¹æ®ç±»åˆ«é€‰æ‹©æç¤ºè¯æ¨¡æ¿
        prompt = self._build_prompt(question, context_text, category)
        
        try:
            # ä½¿ç”¨å¢å¼ºç‰ˆLLMClientï¼Œæ”¯æŒæ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†
            if category == 5:  # å¯¹æŠ—æ€§é—®é¢˜
                answer = self.llm_client.generate_answer(
                    prompt=prompt,
                    temperature=0.1, 
                    max_tokens=100,
                    json_response=False,
                    context_text=context_text,  # æä¾›åˆ†ç¦»çš„ä¸Šä¸‹æ–‡
                    question=question           # æä¾›é—®é¢˜ç”¨äºæ™ºèƒ½æˆªæ–­
                )
            else:
                answer = self.llm_client.generate_answer(
                    prompt=prompt,
                    temperature=0.1, 
                    max_tokens=150,
                    context_text=context_text,
                    question=question
                )
            
            return {
                "question": question,
                "category": category,
                "generated_answer": answer,  # ç¡®ä¿è¿™é‡Œæ˜¯ answerï¼Œä¸æ˜¯å…¶ä»–é”®å
                "prompt": prompt,
                "context_nodes_count": len(context["retrieved_nodes"]),
                "context_relations_count": len(context["retrieved_relations"]),
                "evidence_recall": context.get("evidence_recall", 0.0),
                "generation_strategy": context["strategy"],
                "llm_context_info": self.llm_client.get_context_info()  # æ·»åŠ LLMé…ç½®ä¿¡æ¯
            }
            
        except Exception as e:
            self.logger.error(f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return {
                "question": question,
                "category": category,
                "generated_answer": f"ç”Ÿæˆå¤±è´¥: {str(e)}",  # ç¡®ä¿é”™è¯¯æƒ…å†µä¸‹ä¹Ÿæœ‰è¿™ä¸ªé”®
                "error": str(e)
            }

    def _build_prompt(self, question: str, context_text: str, category: int) -> str:
        """æ„å»ºé’ˆå¯¹ä¸åŒç±»åˆ«çš„æç¤ºè¯"""
        
        base_instruction = f"""Based on the following conversation context, answer the question accurately and concisely.

        {context_text}

        """
        
        if category == 1:  # å¤šè·³æ¨ç†
            instruction = base_instruction + """
            This question requires multi-step reasoning. Consider the relationships between different pieces of information in the context.

            Question: {question}

            Provide a concise answer based on the connected information in the context:"""
            
        elif category == 2:  # å•è·³äº‹å®
            instruction = base_instruction + """
            This question asks for a specific fact. Look for direct information in the context.

            Question: {question}

            Provide a short, factual answer:"""
            
        elif category == 3:  # æ—¶é—´ç›¸å…³
            instruction = base_instruction + """
            This question is about timing or dates. Pay attention to temporal information and dates mentioned in the context.

            Question: {question}

            Provide a specific date or time-related answer:"""
            
        elif category == 4:  # å¼€æ”¾åŸŸ
            instruction = base_instruction + """
            This is an open-domain question. Use all available information in the context to provide a comprehensive answer.

            Question: {question}

            Provide a detailed answer based on the context:"""
            
        elif category == 5:  # å¯¹æŠ—æ€§
            instruction = base_instruction + """
            This question tests if the information is actually mentioned in the conversation. Be very careful to only answer based on what is explicitly stated.

            Question: {question}

            If the answer is clearly stated in the context, provide it. If the information is not mentioned or unclear, respond with "No information available":"""
        
        else:
            instruction = base_instruction + f"""
            Question: {question}

            Answer:"""
        
        return instruction.format(question=question)
    
    def evaluate_single_conversation(self, 
                                conversation_id: str, 
                                qa_data: List[Dict]) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªå¯¹è¯çš„æ‰€æœ‰QA - ä¿®å¤æ•°æ®æ ¼å¼é—®é¢˜"""
        results = {
            "conversation_id": conversation_id,
            "total_questions": len(qa_data),
            "results_by_category": {},
            "overall_metrics": {},
            "detailed_results": []
        }
        
        # æŒ‰ç±»åˆ«åˆ†ç»„å¤„ç†
        qa_by_category = {}
        for qa in qa_data:
            category = qa.get("category", 2)
            if category not in qa_by_category:
                qa_by_category[category] = []
            qa_by_category[category].append(qa)
        
        all_predictions = []
        
        for category, qa_list in qa_by_category.items():
            self.logger.info(f"å¤„ç†ç±»åˆ« {category}: {len(qa_list)} ä¸ªé—®é¢˜")
            
            category_results = {
                "category": category,
                "strategy": self.category_strategies.get(category, "single_hop"),
                "questions_count": len(qa_list),
                "predictions": [],
                "metrics": {}
            }
            
            for qa in tqdm(qa_list, desc=f"Category {category}"):
                try:
                    # ä¿®å¤å¯¹æŠ—æ€§é—®é¢˜çš„æ•°æ®æ ¼å¼
                    if category == 5:  # å¯¹æŠ—æ€§é—®é¢˜
                        # å¯¹æŠ—æ€§é—®é¢˜ä½¿ç”¨ adversarial_answer ä½œä¸ºæ ‡å‡†ç­”æ¡ˆ
                        if "adversarial_answer" not in qa or "question" not in qa:
                            self.logger.warning(f"å¯¹æŠ—æ€§QAæ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡: {qa}")
                            continue
                        standard_answer = qa["adversarial_answer"]
                    else:
                        # å…¶ä»–ç±»åˆ«ä½¿ç”¨ answer å­—æ®µ
                        if not all(key in qa for key in ["question", "answer"]):
                            self.logger.warning(f"QAæ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡: {qa}")
                            continue
                        standard_answer = qa["answer"]
                    
                    # æ£€ç´¢ä¸Šä¸‹æ–‡
                    context = self.retrieve_context_for_question(
                        question=qa["question"],
                        category=category,
                        conversation_id=conversation_id,
                        evidence=qa.get("evidence", [])
                    )
                    
                    # ç”Ÿæˆç­”æ¡ˆ
                    answer_result = self.generate_answer(context)
                    
                    # ç¡®ä¿answer_resultåŒ…å«å¿…è¦çš„é”®
                    if "generated_answer" not in answer_result:
                        self.logger.error(f"ç­”æ¡ˆç”Ÿæˆç»“æœç¼ºå°‘ 'generated_answer' é”®: {answer_result}")
                        continue
                    
                    # å‡†å¤‡è¯„ä¼°æ•°æ®
                    prediction_data = {
                        "question": qa["question"],
                        "category": category,
                        "answer": standard_answer,  # ä½¿ç”¨æ­£ç¡®çš„æ ‡å‡†ç­”æ¡ˆå­—æ®µ
                        "prediction": answer_result["generated_answer"],
                        "evidence": qa.get("evidence", []),
                        "context_nodes_count": answer_result.get("context_nodes_count", 0),
                        "evidence_recall": answer_result.get("evidence_recall", 0.0)
                    }
                    
                    category_results["predictions"].append(prediction_data)
                    all_predictions.append(prediction_data)
                    
                    # è¯¦ç»†ç»“æœ
                    results["detailed_results"].append({
                        **prediction_data,
                        "retrieval_context": context,
                        "generation_details": answer_result
                    })
                    
                except KeyError as e:
                    self.logger.error(f"å¤„ç†QAæ—¶ç¼ºå°‘å¿…è¦é”® {e}: {qa}")
                    continue
                except Exception as e:
                    self.logger.error(f"å¤„ç†QAæ—¶å‘ç”Ÿé”™è¯¯: {e}, qa: {qa}")
                    continue
            
            # è®¡ç®—ç±»åˆ«æŒ‡æ ‡
            if category_results["predictions"]:
                category_metrics = self._evaluate_predictions(category_results["predictions"])
                category_results["metrics"] = category_metrics
                results["results_by_category"][category] = category_results
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        if all_predictions:
            results["overall_metrics"] = self._evaluate_predictions(all_predictions)
        
        return results
    
    # åœ¨ semantic_graph_qa_evaluator.py ä¸­ä¿®æ”¹è¯„ä¼°æ–¹æ³•

    def _evaluate_predictions(self, predictions: List[Dict]) -> Dict[str, float]:
        """è¯„ä¼°é¢„æµ‹ç»“æœ - ä½¿ç”¨evaluation.pyçš„è¯„åˆ†å‡½æ•°"""
        if not predictions:
            return {}
        
        try:
            # ä½¿ç”¨evaluation.pyçš„è¯„ä¼°å‡½æ•°
            scores, _, recall_scores = eval_question_answering(predictions, eval_key="prediction")
            
            metrics = {
                "avg_score": float(np.mean(scores)) if scores else 0.0,
                "std_score": float(np.std(scores)) if scores else 0.0,
                "avg_recall": float(np.mean(recall_scores)) if recall_scores else 0.0,
                "std_recall": float(np.std(recall_scores)) if recall_scores else 0.0,
                "total_questions": len(predictions),
                "individual_scores": [float(s) for s in scores],  # ä¿å­˜æ¯ä¸ªé—®é¢˜çš„å…·ä½“åˆ†æ•°
                "individual_recalls": [float(r) for r in recall_scores]  # ä¿å­˜æ¯ä¸ªé—®é¢˜çš„å¬å›ç‡
            }
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡è¯¦ç»†åˆ†æ•°
            by_category = {}
            for i, pred in enumerate(predictions):
                cat = pred["category"]
                if cat not in by_category:
                    by_category[cat] = {
                        "scores": [], 
                        "recalls": [], 
                        "questions": [],
                        "predictions": [],
                        "answers": []
                    }
                
                if i < len(scores):
                    by_category[cat]["scores"].append(float(scores[i]))
                    by_category[cat]["questions"].append(pred["question"])
                    by_category[cat]["predictions"].append(pred["prediction"])
                    by_category[cat]["answers"].append(pred["answer"])
                if i < len(recall_scores):
                    by_category[cat]["recalls"].append(float(recall_scores[i]))
            
            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
            for cat, data in by_category.items():
                if data["scores"]:
                    metrics[f"category_{cat}_avg_score"] = float(np.mean(data["scores"]))
                    metrics[f"category_{cat}_std_score"] = float(np.std(data["scores"]))
                    metrics[f"category_{cat}_individual_scores"] = data["scores"]
                    metrics[f"category_{cat}_count"] = len(data["scores"])
                    
                    # ä¿å­˜å…·ä½“çš„é—®é¢˜å’Œç­”æ¡ˆç”¨äºåˆ†æ
                    metrics[f"category_{cat}_details"] = {
                        "questions": data["questions"],
                        "predictions": data["predictions"], 
                        "answers": data["answers"],
                        "scores": data["scores"]
                    }
                
                if data["recalls"]:
                    metrics[f"category_{cat}_avg_recall"] = float(np.mean(data["recalls"]))
                    metrics[f"category_{cat}_std_recall"] = float(np.std(data["recalls"]))
                    metrics[f"category_{cat}_individual_recalls"] = data["recalls"]
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
            return {"error": str(e)}

    def _save_detailed_results(self, results: Dict[str, Any], output_file: str):
        """ä¿å­˜è¯¦ç»†çš„è¯„ä¼°ç»“æœï¼ŒåŒ…å«æ¯ä¸ªé—®é¢˜çš„å…·ä½“åˆ†æ•°"""
        detailed_output = {
            "evaluation_info": results["evaluation_info"],
            "sample_results": []
        }
        
        # ä¸ºæ¯ä¸ªå¯¹è¯ä¿å­˜è¯¦ç»†ç»“æœ
        for conv_id, conv_results in results["conversation_results"].items():
            sample_result = {
                "sample_id": conv_id,
                "overall_metrics": conv_results.get("overall_metrics", {}),
                "category_results": {},
                "question_level_results": []
            }
            
            # æŒ‰ç±»åˆ«æ•´ç†ç»“æœ
            for category, cat_data in conv_results.get("results_by_category", {}).items():
                sample_result["category_results"][category] = {
                    "strategy": cat_data["strategy"],
                    "questions_count": cat_data["questions_count"],
                    "metrics": cat_data.get("metrics", {}),
                }
            
            # æ•´ç†é—®é¢˜çº§åˆ«çš„è¯¦ç»†ç»“æœ
            for detail in conv_results.get("detailed_results", []):
                question_result = {
                    "question": detail["question"],
                    "category": detail["category"],
                    "ground_truth": detail["answer"],
                    "prediction": detail["prediction"], 
                    "evidence": detail.get("evidence", []),
                    "context_nodes_count": detail.get("context_nodes_count", 0),
                    "evidence_recall": detail.get("evidence_recall", 0.0),
                    "retrieval_strategy": detail.get("generation_details", {}).get("generation_strategy", "unknown")
                }
                
                # è®¡ç®—è¯¥é—®é¢˜çš„å…·ä½“åˆ†æ•°
                question_result["f1_score"] = self._calculate_question_f1_score(
                    detail["prediction"], detail["answer"], detail["category"]
                )
                
                sample_result["question_level_results"].append(question_result)
            
            detailed_output["sample_results"].append(sample_result)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_output, f, ensure_ascii=False, indent=2)

    def _calculate_question_f1_score(self, prediction: str, answer: str, category: int) -> float:
        """è®¡ç®—å•ä¸ªé—®é¢˜çš„F1åˆ†æ•°"""
        try:
            if category == 5:  # å¯¹æŠ—æ€§é—®é¢˜
                if 'no information available' in prediction.lower() or 'not mentioned' in prediction.lower():
                    return 1.0 if 'not mentioned' in answer.lower() else 0.0
                else:
                    return 0.0 if 'not mentioned' in answer.lower() else calculate_f1_score(prediction, answer)
            # elif category == 1:  # å¤šè·³æ¨ç†
            #     return f1(prediction, answer)  # ä½¿ç”¨å¤šç­”æ¡ˆF1
            else:  # å…¶ä»–ç±»åˆ«
                return calculate_f1_score(prediction, answer)
        except Exception as e:
            self.logger.error(f"è®¡ç®—F1åˆ†æ•°å¤±è´¥: {e}")
            return 0.0
    
    # def _evaluate_predictions(self, predictions: List[Dict]) -> Dict[str, float]:
    #     """è¯„ä¼°é¢„æµ‹ç»“æœ"""
    #     if not predictions:
    #         return {}
        
    #     # è½¬æ¢ä¸ºevaluation.pyéœ€è¦çš„æ ¼å¼
    #     eval_data = []
    #     for pred in predictions:
    #         eval_data.append({
    #             "category": pred["category"],
    #             "answer": pred["answer"],
    #             "prediction": pred["prediction"],
    #             "evidence": pred.get("evidence", [])
    #         })
        
    #     try:
    #         # ä½¿ç”¨evaluation.pyçš„è¯„ä¼°å‡½æ•°
    #         scores, _, recall_scores = eval_question_answering(eval_data, eval_key="prediction")
            
    #         metrics = {
    #             "avg_score": float(np.mean(scores)) if scores else 0.0,
    #             "avg_recall": float(np.mean(recall_scores)) if recall_scores else 0.0,
    #             "total_questions": len(predictions)
    #         }
            
    #         # æŒ‰ç±»åˆ«ç»Ÿè®¡
    #         by_category = {}
    #         for i, pred in enumerate(predictions):
    #             cat = pred["category"]
    #             if cat not in by_category:
    #                 by_category[cat] = {"scores": [], "recalls": []}
    #             if i < len(scores):
    #                 by_category[cat]["scores"].append(scores[i])
    #             if i < len(recall_scores):
    #                 by_category[cat]["recalls"].append(recall_scores[i])
            
    #         for cat, data in by_category.items():
    #             metrics[f"category_{cat}_score"] = float(np.mean(data["scores"])) if data["scores"] else 0.0
    #             metrics[f"category_{cat}_recall"] = float(np.mean(data["recalls"])) if data["recalls"] else 0.0
    #             metrics[f"category_{cat}_count"] = len(data["scores"])
            
    #         return metrics
            
    #     except Exception as e:
    #         self.logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
    #         return {"error": str(e)}

    def run_full_evaluation(self, 
                           qa_test_data: Dict[str, List[Dict]],
                           save_results: bool = True) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´è¯„ä¼°
        
        Args:
            qa_test_data: QAæµ‹è¯•æ•°æ® {conversation_id: [qa_items]}
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            
        Returns:
            å®Œæ•´è¯„ä¼°ç»“æœ
        """
        self.logger.info(f"å¼€å§‹å®Œæ•´è¯„ä¼°ï¼Œå…± {len(qa_test_data)} ä¸ªå¯¹è¯")
        
        full_results = {
            "evaluation_info": {
                "timestamp": datetime.now().isoformat(),
                "total_conversations": len(qa_test_data),
                "llm_model": self.llm_client.model_name,
                "retrieval_strategies": self.category_strategies
            },
            "conversation_results": {},
            "overall_summary": {}
        }
        
        all_conversation_metrics = []
        
        for conv_id, qa_list in qa_test_data.items():
            self.logger.info(f"è¯„ä¼°å¯¹è¯ {conv_id}: {len(qa_list)} ä¸ªé—®é¢˜")
            
            try:
                conv_results = self.evaluate_single_conversation(conv_id, qa_list)
                full_results["conversation_results"][conv_id] = conv_results
                
                if conv_results.get("overall_metrics"):
                    all_conversation_metrics.append(conv_results["overall_metrics"])
                
            except Exception as e:
                self.logger.error(f"è¯„ä¼°å¯¹è¯ {conv_id} å¤±è´¥: {e}")
                full_results["conversation_results"][conv_id] = {"error": str(e)}
        
        # è®¡ç®—æ€»ä½“æ‘˜è¦
        if all_conversation_metrics:
            full_results["overall_summary"] = self._calculate_overall_summary(all_conversation_metrics)
        
        # ä¿å­˜ç»“æœ
        if save_results:
            self._save_results(full_results)
        
        return full_results

    def _calculate_overall_summary(self, conversation_metrics: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—æ€»ä½“æ‘˜è¦ç»Ÿè®¡"""
        summary = {}
        
        # å¹³å‡åˆ†æ•°
        avg_scores = [m.get("avg_score", 0) for m in conversation_metrics if "avg_score" in m]
        if avg_scores:
            summary["overall_avg_score"] = float(np.mean(avg_scores))
            summary["overall_std_score"] = float(np.std(avg_scores))
        
        # å¹³å‡å¬å›ç‡
        avg_recalls = [m.get("avg_recall", 0) for m in conversation_metrics if "avg_recall" in m]
        if avg_recalls:
            summary["overall_avg_recall"] = float(np.mean(avg_recalls))
            summary["overall_std_recall"] = float(np.std(avg_recalls))
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        for category in range(1, 6):
            cat_scores = [m.get(f"category_{category}_score", 0) for m in conversation_metrics 
                         if f"category_{category}_score" in m]
            if cat_scores:
                summary[f"category_{category}_avg_score"] = float(np.mean(cat_scores))
                summary[f"category_{category}_std_score"] = float(np.std(cat_scores))
        
        # æ€»é—®é¢˜æ•°
        total_questions = sum(m.get("total_questions", 0) for m in conversation_metrics)
        summary["total_questions_evaluated"] = total_questions
        
        return summary

    def _save_results(self, results: Dict[str, Any]):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        full_results_file = self.output_dir / f"semantic_graph_qa_evaluation_{timestamp}.json"
        with open(full_results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
        summary_file = self.output_dir / f"evaluation_summary_{timestamp}.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("=== Semantic Graph QA Evaluation Summary ===\n\n")
            f.write(f"Timestamp: {results['evaluation_info']['timestamp']}\n")
            f.write(f"LLM Model: {results['evaluation_info']['llm_model']}\n")
            f.write(f"Total Conversations: {results['evaluation_info']['total_conversations']}\n\n")
            
            if "overall_summary" in results:
                summary = results["overall_summary"]
                f.write("=== Overall Results ===\n")
                f.write(f"Average Score: {summary.get('overall_avg_score', 0):.4f} Â± {summary.get('overall_std_score', 0):.4f}\n")
                f.write(f"Average Recall: {summary.get('overall_avg_recall', 0):.4f} Â± {summary.get('overall_std_recall', 0):.4f}\n")
                f.write(f"Total Questions: {summary.get('total_questions_evaluated', 0)}\n\n")
                
                f.write("=== Results by Category ===\n")
                for category in range(1, 6):
                    cat_score = summary.get(f"category_{category}_avg_score")
                    if cat_score is not None:
                        cat_std = summary.get(f"category_{category}_std_score", 0)
                        strategy = self.category_strategies.get(category, "unknown")
                        f.write(f"Category {category} ({strategy}): {cat_score:.4f} Â± {cat_std:.4f}\n")
        
        self.logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {full_results_file}")
        self.logger.info(f"æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºè¯­ä¹‰å›¾è°±QAè¯„ä¼°"""
    
    # 1. è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # 2. åŠ è½½å·²å­˜å‚¨çš„è¯­ä¹‰å›¾è°±
    print("ğŸ”„ åŠ è½½è¯­ä¹‰å›¾è°±...")
    storage = ConversationSemanticStorage()
    
    # å‡è®¾æ‚¨å·²ç»è¿è¡Œäº†dataset_inserterå­˜å‚¨äº†æ•°æ®
    # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å·²æœ‰çš„è¯­ä¹‰å›¾è°±
    semantic_graph = storage.semantic_graph
    
    # 3. åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
    print("ğŸ”„ åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
    llm_client = LLMClient(model_name="deepseek-chat")  # æˆ–æ‚¨å–œæ¬¢çš„æ¨¡å‹
    
    # 4. åˆ›å»ºè¯„ä¼°å™¨
    evaluator = SemanticGraphQAEvaluator(
        semantic_graph=semantic_graph,
        llm_client=llm_client
    )
    
    # 5. è·å–QAæµ‹è¯•æ•°æ®
    print("ğŸ“‹ è·å–QAæµ‹è¯•æ•°æ®...")
    qa_test_data = storage.get_qa_test_data(["conv-26", "conv-30"])  # æµ‹è¯•æŒ‡å®šå¯¹è¯
    
    if not qa_test_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°QAæµ‹è¯•æ•°æ®")
        return
    
    print(f"ğŸ“Š å‘ç°æµ‹è¯•æ•°æ®: {sum(len(qa_list) for qa_list in qa_test_data.values())} ä¸ªé—®é¢˜")
    
    # 6. è¿è¡Œè¯„ä¼°
    print("ğŸš€ å¼€å§‹è¯„ä¼°...")
    results = evaluator.run_full_evaluation(qa_test_data)
    
    # 7. æ˜¾ç¤ºç»“æœ
    print("\nğŸ¯ è¯„ä¼°å®Œæˆï¼")
    if "overall_summary" in results:
        summary = results["overall_summary"]
        print(f"ğŸ“ˆ æ€»ä½“å¹³å‡åˆ†æ•°: {summary.get('overall_avg_score', 0):.4f}")
        print(f"ğŸ“ˆ æ€»ä½“å¹³å‡å¬å›ç‡: {summary.get('overall_avg_recall', 0):.4f}")
        print(f"ğŸ“Š æ€»è®¡è¯„ä¼°é—®é¢˜: {summary.get('total_questions_evaluated', 0)} ä¸ª")
        
        print("\nğŸ“‹ æŒ‰ç±»åˆ«ç»“æœ:")
        for category in range(1, 6):
            cat_score = summary.get(f"category_{category}_avg_score")
            if cat_score is not None:
                strategy = evaluator.category_strategies.get(category, "unknown")
                print(f"  ç±»åˆ« {category} ({strategy}): {cat_score:.4f}")


if __name__ == "__main__":
    main()