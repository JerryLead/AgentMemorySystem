import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
intel_omp_path = r'C:\Program Files\Dell\DTP\IPDT'
os.environ['PATH'] = intel_omp_path + os.pathsep + os.environ['PATH']

from matplotlib.lines import Line2D
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import requests
import os
import arxiv
from bs4 import BeautifulSoup 
from semantic_data_structure.semantic_simple_graph import SemanticSimpleGraph
from semantic_data_structure.semantic_map import SemanticMap
import time
from typing import List, Dict, Any,Optional,Union
import re
from collections import defaultdict
import traceback
import networkx as nx
import matplotlib.pyplot as plt

class ArxivSemanticGraph:
    def __init__(self, text_embedding_model="all-MiniLM-L6-v2", embedding_dim=384):
        self.semantic_graph = SemanticSimpleGraph(SemanticMap(text_embedding_model, embedding_dim))
        self.preferences = {"liked": [], "disliked": []}
        self.similarity_threshold = 0.8
        self.embedding_model = SentenceTransformer(text_embedding_model)
        
        # åˆå§‹åŒ–FAISSç´¢å¼•
        self.reference_index = faiss.IndexFlatL2(embedding_dim)
        self.reference_embeddings = []
        self.root_node_embeddings = []
        self.root_node_keys = []


    def _get_text_embedding(self, text: str) -> np.ndarray:
        return self.embedding_model.encode([text], convert_to_numpy=True)

    def _calculate_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))

    def _node_exists(self, search_key: str, content: Any, node_type: str) -> bool:
        for item in self.semantic_graph.semantic_map.data:
            key, value, _ = item
            if key.startswith(search_key):
                if node_type == "reference":
                    ref_embedding = self._get_text_embedding(content)
                    existing_embedding = self._get_text_embedding(value)
                    if self._calculate_similarity(ref_embedding, existing_embedding) >= self.similarity_threshold:
                        return True
                elif value == content:
                    return True
        return False

    def insert(self, paper_id: str, title: str, authors: list, abstract: str,
               chapters: list, references: list, tables: list = None):
        # æ’å…¥æ ¹èŠ‚ç‚¹
        root_key = f"{paper_id}_title_authors"
        root_value = {
            "title": title,
            "authors": sorted(authors),
            "root_key": root_key,
            "paper_id": paper_id
        }
        if not self._node_exists(root_key, root_value, "root"):
            self.semantic_graph.add_node(
                root_key, root_value,
                text_for_embedding=f"{title} {' '.join(authors)}"
            )
            root_embedding = self._get_text_embedding(f"{title} {' '.join(authors)}")
            self.root_node_embeddings.append(root_embedding)
            self.root_node_keys.append(root_key)

        # æ’å…¥æ‘˜è¦èŠ‚ç‚¹
        abstract_key = f"{paper_id}_abstract"
        if not self._node_exists(abstract_key, abstract, "abstract"):
            self.semantic_graph.add_node(
                abstract_key, abstract,
                parent_keys=[root_key],
                parent_relation="has_abstract",
                text_for_embedding=abstract
            )

        prev_chapter_key = None
        # å¤„ç†ç« èŠ‚
        prev_chapter_key = None
        for idx, chapter in enumerate(chapters):
            chapter_title = chapter.get('title', f"Untitled Chapter {idx}")
            chapter_key = f"{paper_id}_chapter_{idx}"

            if not self._node_exists(chapter_key, chapter_title, "chapter"):
                self.semantic_graph.add_node(
                    chapter_key, chapter_title,
                    parent_keys=[root_key],
                    parent_relation="has_chapter",
                    text_for_embedding=chapter_title
                )

                # æ·»åŠ ç« èŠ‚ä¹‹é—´çš„åŒå‘ä¸Šä¸‹æ–‡å…³ç³»
                if prev_chapter_key:
                    # å‰ç« èŠ‚æŒ‡å‘å½“å‰ç« èŠ‚ä¸º "next"
                    self.semantic_graph.insert_edge(prev_chapter_key, chapter_key, "next")
                    # å½“å‰ç« èŠ‚æŒ‡å‘å‰ç« èŠ‚ä¸º "previous"
                    self.semantic_graph.insert_edge(chapter_key, prev_chapter_key, "previous")

                prev_chapter_key = chapter_key

                prev_para_key = None
                # å¤„ç†æ®µè½
                for para_idx, para in enumerate(chapter.get('paragraphs', [])):
                    para_key = f"{chapter_key}_para_{para_idx}"
                    if not self._node_exists(para_key, para, "paragraph"):
                        self.semantic_graph.add_node(
                            para_key, para,
                            parent_keys=[chapter_key],
                            parent_relation="has_paragraph",
                            text_for_embedding=para
                        )

                        # æ·»åŠ æ®µè½ä¹‹é—´çš„åŒå‘ä¸Šä¸‹æ–‡å…³ç³»
                        if prev_para_key:
                            # å‰æ®µè½æŒ‡å‘å½“å‰æ®µè½ä¸º "next"
                            self.semantic_graph.insert_edge(prev_para_key, para_key, "next")
                            # å½“å‰æ®µè½æŒ‡å‘å‰æ®µè½ä¸º "previous" 
                            self.semantic_graph.insert_edge(para_key, prev_para_key, "previous")

                        prev_para_key = para_key

                # å¤„ç†å›¾ç‰‡
                for img_idx, img_data in enumerate(chapter.get('images', [])):
                    img_key = f"{chapter_key}_img_{img_idx}"
                    if not self._node_exists(img_key, f"Image in {chapter_title}", "image"):
                        self.semantic_graph.add_node(
                            img_key, img_data,
                            parent_keys=[chapter_key],
                            parent_relation="has_image",
                            text_for_embedding=f"Figure in {chapter_title}"
                        )

        # å¤„ç†å‚è€ƒæ–‡çŒ®
        for ref_idx, ref in enumerate(references):
            ref_key = f"{paper_id}_ref_{ref_idx}"
            ref_embedding = self._get_text_embedding(ref)

            # FAISSç›¸ä¼¼æ€§æ£€æŸ¥
            if len(self.reference_embeddings) > 0:
                D, I = self.reference_index.search(ref_embedding.reshape(1, -1), 1)
                if D[0][0] < (1 - self.similarity_threshold):
                    continue

            similar_root_key = None
            if len(self.root_node_embeddings) > 0:
                root_embeddings_array = np.vstack(self.root_node_embeddings)
                similarities = np.dot(ref_embedding, root_embeddings_array.T)
                max_similarity_idx = np.argmax(similarities)
                if similarities[0][max_similarity_idx] >= self.similarity_threshold:
                    similar_root_key = self.root_node_keys[max_similarity_idx]

            if not self._node_exists(ref_key, ref, "reference"):
                parents = [root_key]
                relations = "has_reference" 
                if similar_root_key:
                    parents.append(similar_root_key)
                    relations="is_referenced_by"

                self.semantic_graph.add_node(
                    ref_key, ref,
                    parent_keys=parents,
                    parent_relation=relations,
                    text_for_embedding=ref
                )
                self.reference_embeddings.append(ref_embedding)
                self.reference_index.add(ref_embedding.reshape(1, -1))

        # å¤„ç†è¡¨æ ¼
        if tables:
            for tbl_idx, table in enumerate(tables):
                tbl_key = f"{paper_id}_table_{tbl_idx}"
                table_meta = {
                    "columns": [],
                    "row_count": 0,
                    "root_key": tbl_key
                }

                if isinstance(table, list) and table:
                    first_item = table[0]
                    if isinstance(first_item, dict):
                        table_meta["columns"] = list(first_item.keys())
                        table_meta["row_count"] = len(table)
                    elif isinstance(first_item, (list, tuple)):
                        table_meta["columns"] = [f"Col{i}" for i in range(len(table[0]))]
                        table_meta["row_count"] = len(table)
                elif isinstance(table, dict):
                    table_meta["columns"] = list(table.keys())
                    table_meta["row_count"] = 1

                self.semantic_graph.add_node(
                    tbl_key, table_meta,
                    parent_keys=[root_key],
                    parent_relation="has_table",
                    text_for_embedding=f"Table with columns: {', '.join(table_meta['columns'])}"
                )

                if isinstance(table, list):
                    for row_idx, row in enumerate(table):
                        if isinstance(row, dict):
                            row_text = " | ".join([f"{k}:{v}" for k, v in row.items()])
                        elif isinstance(row, (list, tuple)):
                            row_text = " | ".join([str(item) for item in row])
                        else:
                            row_text = str(row)
                        row_key = f"{tbl_key}_row_{row_idx}"
                        if not self._node_exists(row_key, row_text, "table_row"):
                            self.semantic_graph.add_node(
                                row_key, row,
                                parent_keys=[tbl_key],
                                parent_relation="has_row",
                                text_for_embedding=row_text
                            )
                elif isinstance(table, dict):
                    row_text = " | ".join([f"{k}:{v}" for k, v in table.items()])
                    row_key = f"{tbl_key}_row_0"
                    if not self._node_exists(row_key, row_text, "table_row"):
                        self.semantic_graph.add_node(
                            row_key, table,
                            parent_keys=[tbl_key],
                            parent_relation="has_row",
                            text_for_embedding=row_text
                        )

        self.semantic_graph.build_index()


    def query(self, query_text: str, k: int = 5) -> List[Dict]:
        query_embedding = self._get_text_embedding(query_text)
        scores, indices = self.semantic_graph.semantic_map.index.search(query_embedding, k*3)
        
        results = []
        seen_papers = set()
        
        for score, idx in zip(scores[0], indices[0]):
            if idx >= len(self.semantic_graph.semantic_map.data):
                continue
                
            node = self.semantic_graph.semantic_map.data[idx]
            if not node:
                continue
                
            node_key = node[0]
            
            paper_root = self._get_root_node(node_key)
            if not paper_root:
                continue
                
            if paper_root["key"] in seen_papers:
                continue
                
            full_paper = self._get_full_paper(paper_root["key"])
            if full_paper:
                full_paper["similarity_score"] = float(1 - score)
                results.append(full_paper)
                seen_papers.add(paper_root["key"])
            
            if len(results) >= k:
                break
                
        return results

    def _get_root_node(self, key: str) -> Optional[Dict]:
        current_key = key
        while True:
            parents = self.semantic_graph.graph_relations.get(current_key, {}).get("parents", {})
            if not parents:
                break
            current_key = next(iter(parents.keys()))
        
        node = next((n for n in self.semantic_graph.semantic_map.data if n[0] == current_key), None)
        if node:
            return {
                "key": node[0],
                "value": node[1],
                "embedding": node[2]
            }
        return None

    def _get_full_paper(self, root_key: str) -> Optional[Dict]:
        root_node = next(
            (n for n in self.semantic_graph.semantic_map.data if n[0] == root_key), 
            None
        )
        if not root_node:
            return None

        paper_id = root_key.split("_")[0]
        paper = {
            "metadata": {
                "root_key": root_key,
                "paper_id": paper_id,
                "title": root_node[1].get("title", ""),
                "authors": root_node[1].get("authors", [])
            },
            "abstract": "",
            "chapters": [],
            "references": [],
            "tables": []
        }
        
        # è·å–æ‘˜è¦
        abstract_node = next((n for n in self.semantic_graph.semantic_map.data 
                             if n[0].startswith(paper_id) and "_abstract" in n[0]), None)
        if abstract_node:
            paper["abstract"] = abstract_node[1]
        
        # è·å–ç« èŠ‚
        chapter_nodes = [n for n in self.semantic_graph.semantic_map.data 
                        if n[0].startswith(paper_id) and "_chapter_" in n[0]]
        for chap in chapter_nodes:
            chapter = {
                "title": chap[1] if isinstance(chap[1], str) else chap[1].get("title", ""),
                "paragraphs": [],
                "images": []
            }
            
            # è·å–æ®µè½
            paragraph_nodes = [n for n in self.semantic_graph.semantic_map.data 
                             if n[0].startswith(chap[0]) and "_para_" in n[0]]
            for para in paragraph_nodes:
                chapter["paragraphs"].append(para[1])
            
            # è·å–å›¾ç‰‡
            image_nodes = [n for n in self.semantic_graph.semantic_map.data 
                          if n[0].startswith(chap[0]) and "_img_" in n[0]]
            chapter["images"] = [n[1] for n in image_nodes]
            
            paper["chapters"].append(chapter)
        
        # è·å–å‚è€ƒæ–‡çŒ®
        ref_nodes = [n for n in self.semantic_graph.semantic_map.data 
                    if n[0].startswith(paper_id) and "_ref_" in n[0]]
        paper["references"] = [n[1] for n in ref_nodes]
        
        # è·å–è¡¨æ ¼
        table_nodes = [n for n in self.semantic_graph.semantic_map.data 
                      if n[0].startswith(paper_id) and "_table_" in n[0]]
        for tbl in table_nodes:
            if "_row_" not in tbl[0]:
                table = {
                    "metadata": tbl[1],
                    "rows": []
                }
                row_nodes = [n for n in self.semantic_graph.semantic_map.data 
                            if n[0].startswith(tbl[0]) and "_row_" in n[0]]
                for row in row_nodes:
                    table["rows"].append(row[1])
                paper["tables"].append(table)
        
        return paper


    def delete(self, paper_id: str, cascade: bool = True) -> bool:
        """
        åˆ é™¤æŒ‡å®šè®ºæ–‡åŠå…¶æ‰€æœ‰å…³è”èŠ‚ç‚¹
        :param paper_id: è¦åˆ é™¤çš„è®ºæ–‡IDï¼ˆå¦‚"2305.12345"ï¼‰
        :param cascade: æ˜¯å¦çº§è”åˆ é™¤æ‰€æœ‰å­èŠ‚ç‚¹
        :return: æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        root_key = f"{paper_id}_title_authors"
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è¯¥è®ºæ–‡
        if not any(node[0] == root_key for node in self.semantic_graph.semantic_map.data):
            print(f"è®ºæ–‡ {paper_id} ä¸å­˜åœ¨")
            return False

        # çº§è”åˆ é™¤æ‰€æœ‰å­èŠ‚ç‚¹
        if cascade:
            children = self._get_all_children(root_key)
            for child_key in reversed(children + [root_key]):  # åå‘åˆ é™¤ä»å¶å­èŠ‚ç‚¹å¼€å§‹
                self._safe_delete_node(child_key)

        # æ›´æ–°å‚è€ƒæ–‡çŒ®ç´¢å¼•
        self._update_reference_index()

        # ä»åå¥½åˆ—è¡¨ä¸­ç§»é™¤
        self.preferences["liked"] = [n for n in self.preferences["liked"] if n[0] != root_key]
        self.preferences["disliked"] = [n for n in self.preferences["disliked"] if n[0] != root_key]

        self.semantic_graph.build_index()
        return True

    def _safe_delete_node(self, key: str):
        """å®‰å…¨åˆ é™¤èŠ‚ç‚¹å¹¶ç»´æŠ¤ç´¢å¼•"""
        try:
            # ä»è¯­ä¹‰å›¾ä¸­åˆ é™¤
            self.semantic_graph.delete_node(key)
            
            # å¦‚æœæ˜¯å‚è€ƒæ–‡çŒ®èŠ‚ç‚¹ï¼Œä»FAISSç´¢å¼•ç§»é™¤
            if "_ref_" in key:
                ref_index = next((i for i, (k, _, _) in enumerate(self.semantic_graph.semantic_map.data) 
                                if k == key), -1)
                if ref_index != -1 and ref_index < len(self.reference_embeddings):
                    self.reference_index.remove_ids(np.array([ref_index], dtype=np.int64))
                    del self.reference_embeddings[ref_index]
        except ValueError:
            pass

    def _get_all_children(self, root_key: str) -> List[str]:
        """é€’å½’è·å–æ‰€æœ‰å­èŠ‚ç‚¹"""
        children = []
        to_process = [root_key]
        
        while to_process:
            current_key = to_process.pop()
            relations = self.semantic_graph.graph_relations.get(current_key, {})
            for child_key in relations.get("children", {}).keys():
                children.append(child_key)
                to_process.append(child_key)
                
        return children

    def _update_reference_index(self):
        """å®Œå…¨é‡å»ºå‚è€ƒæ–‡çŒ®ç´¢å¼•"""
        self.reference_index.reset()
        self.reference_embeddings = []
        
        # é‡æ–°æ·»åŠ æ‰€æœ‰å‚è€ƒæ–‡çŒ®åµŒå…¥
        ref_nodes = [n for n in self.semantic_graph.semantic_map.data if "_ref_" in n[0]]
        for node in ref_nodes:
            embedding = self._get_text_embedding(node[1])
            self.reference_embeddings.append(embedding)
        
        if self.reference_embeddings:
            embeddings_array = np.vstack(self.reference_embeddings)
            self.reference_index.add(embeddings_array)


    def mark_as_liked(self, paper_id: str) -> bool:
        """æ ‡è®°è®ºæ–‡ä¸ºå–œæ¬¢"""
        root_key = f"{paper_id}_title_authors"
        node = next((n for n in self.semantic_graph.semantic_map.data if n[0] == root_key), None)
        
        if not node:
            print(f"è®ºæ–‡ {paper_id} ä¸å­˜åœ¨")
            return False
        
        # å¦‚æœå·²ç»å­˜åœ¨åˆ™ç§»é™¤æ—§è®°å½•
        self.preferences["liked"] = [n for n in self.preferences["liked"] if n[0] != root_key]
        self.preferences["liked"].append(node)
        
        # å¦‚æœåŒæ—¶åœ¨ä¸å–œæ¬¢åˆ—è¡¨ä¸­åˆ™ç§»é™¤
        self.preferences["disliked"] = [n for n in self.preferences["disliked"] if n[0] != root_key]
        
        return True

    def mark_as_disliked(self, paper_id: str) -> bool:
        """æ ‡è®°è®ºæ–‡ä¸ºä¸å–œæ¬¢"""
        root_key = f"{paper_id}_title_authors"
        node = next((n for n in self.semantic_graph.semantic_map.data if n[0] == root_key), None)
        
        if not node:
            print(f"è®ºæ–‡ {paper_id} ä¸å­˜åœ¨")
            return False
        
        # å¦‚æœå·²ç»å­˜åœ¨åˆ™ç§»é™¤æ—§è®°å½•
        self.preferences["disliked"] = [n for n in self.preferences["disliked"] if n[0] != root_key]
        self.preferences["disliked"].append(node)
        
        # å¦‚æœåŒæ—¶åœ¨å–œæ¬¢åˆ—è¡¨ä¸­åˆ™ç§»é™¤
        self.preferences["liked"] = [n for n in self.preferences["liked"] if n[0] != root_key]
        
        return True

    def recommend(self, query_text: str, k: int = 5, preference_weight: float = 2.0) -> List[Dict]:
        """
        ä¸ªæ€§åŒ–æ¨èè®ºæ–‡
        :param query_text: æŸ¥è¯¢æ–‡æœ¬
        :param k: è¿”å›æ•°é‡
        :param preference_weight: åå¥½æƒé‡ï¼ˆå–œæ¬¢è®ºæ–‡çš„åˆ†æ•°å€å¢å€¼ï¼‰
        """
        # è·å–åŸºç¡€æŸ¥è¯¢ç»“æœ
        base_results = self.query(query_text, k*3)
        
        # è®¡ç®—åå¥½åˆ†æ•°
        scored_results = []
        for paper in base_results:
            root_key = paper["metadata"]["root_key"]
            score = paper["similarity_score"]
            
            # åº”ç”¨åå¥½æƒé‡
            if any(n[0] == root_key for n in self.preferences["liked"]):
                score *= preference_weight
            elif any(n[0] == root_key for n in self.preferences["disliked"]):
                score *= 0.2  # é™ä½ä¸å–œæ¬¢è®ºæ–‡çš„åˆ†æ•°
                
            scored_results.append((score, paper))
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶å»é‡
        seen_papers = set()
        final_results = []
        for score, paper in sorted(scored_results, key=lambda x: -x[0]):
            root_key = paper["metadata"]["root_key"]
            if root_key not in seen_papers:
                final_results.append(paper)
                seen_papers.add(root_key)
            if len(final_results) >= k:
                break
                
        return final_results

    def get_preferences(self) -> Dict[str, List[Dict]]:
        """è·å–å½“å‰åå¥½è®ºæ–‡çš„å®Œæ•´ä¿¡æ¯"""
        return {
            "liked": [self._get_full_paper(n[0]) for n in self.preferences["liked"]],
            "disliked": [self._get_full_paper(n[0]) for n in self.preferences["disliked"]]
        }

    def batch_delete(self, paper_ids: List[str]) -> Dict[str, int]:
        """æ‰¹é‡åˆ é™¤è®ºæ–‡"""
        results = {"success": 0, "failures": 0}
        for pid in paper_ids:
            if self.delete(pid):
                results["success"] += 1
            else:
                results["failures"] += 1
        return results

    def export_preferences(self, file_path: str):
        """å¯¼å‡ºåå¥½åˆ—è¡¨åˆ°æ–‡ä»¶"""
        import json
        with open(file_path, 'w') as f:
            json.dump({
                "liked": [n[0] for n in self.preferences["liked"]],
                "disliked": [n[0] for n in self.preferences["disliked"]]
            }, f)

    def import_preferences(self, file_path: str):
        """ä»æ–‡ä»¶å¯¼å…¥åå¥½åˆ—è¡¨"""
        import json
        with open(file_path, 'r') as f:
            data = json.load(f)
            self.preferences["liked"] = [n for n in self.semantic_graph.semantic_map.data 
                                       if n[0] in data["liked"]]
            self.preferences["disliked"] = [n for n in self.semantic_graph.semantic_map.data 
                                          if n[0] in data["disliked"]]

    def list_all(self):
        """ç»Ÿè®¡å›¾è°±ä¸­è®ºæ–‡çš„æ•°é‡"""
        root_nodes = [node for node in self.semantic_graph.semantic_map.data if node[0].endswith("_title_authors")]
        return root_nodes
    
   

from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time

# å…¨å±€é…ç½®
MAX_RETRIES = 3
TIMEOUT = 60  # è¶…æ—¶æ—¶é—´å»¶é•¿åˆ°60ç§’

# åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„Session
retry_strategy = Retry(
    total=MAX_RETRIES,
    backoff_factor=1,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=["HEAD", "GET"]
)
adapter = HTTPAdapter(max_retries=retry_strategy)
http = requests.Session()
http.mount("https://", adapter)
http.mount("http://", adapter)

def fetch_arxiv_html(query, max_results=5, output_dir='arxiv_html'):
    """æ”¹è¿›åçš„ä¸‹è½½å‡½æ•°ï¼Œæ·»åŠ é‡è¯•æœºåˆ¶"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    client = arxiv.Client()
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.Relevance
    )

    html_paths = []
    for result in client.results(search):
        paper_id = result.get_short_id()
        html_url = result.entry_id.replace("abs", "html")
        file_name = os.path.join(output_dir, f"{paper_id}.html")
        
        try:
            # ä½¿ç”¨å¸¦é‡è¯•çš„Session
            response = http.get(html_url, timeout=TIMEOUT)
            response.raise_for_status()
            
            with open(file_name, 'w', encoding='utf-8') as f:
                f.write(response.text)
            html_paths.append(file_name)
            print(f"æˆåŠŸä¸‹è½½ï¼š{paper_id}")
        except Exception as e:
            print(f"ä¸‹è½½å¤±è´¥ [{paper_id}]: {str(e)}")
            continue  # è·³è¿‡å¤±è´¥é¡¹ç»§ç»­ä¸‹è½½å…¶ä»–è®ºæ–‡
            
    return html_paths
# fetch_arxiv_html("agent memory",100)
def decode_html(html_file_path, image_dir='arxiv_images'):
    paper_id = os.path.splitext(os.path.basename(html_file_path))[0]
    result = {
        'title': '',
        'authors': [],
        'abstract': '',
        'chapters': [],
        'references': [],
        'tables': []
    }

    try:
        with open(html_file_path, 'r', encoding='utf-8') as file:
            html_content = file.read()
            if not html_content:
                raise ValueError("ç©ºæ–‡ä»¶å†…å®¹")

            soup = BeautifulSoup(html_content, 'html.parser')

            # æ ‡é¢˜è§£æ
            try:
                title_tag = soup.find('title')
                if title_tag is None:
                    raise AttributeError("æœªæ‰¾åˆ°æ ‡é¢˜æ ‡ç­¾")
                result['title'] = title_tag.text.strip()
            except AttributeError as e:
                print(f"[{paper_id}] æ ‡é¢˜è§£æå¤±è´¥: {str(e)}")

            # ä½œè€…è§£æ
            try:
                author_elements = soup.select('.ltx_personname')
                if not author_elements:
                    raise ValueError("æœªæ‰¾åˆ°ä½œè€…å…ƒç´ ")
                result['authors'] = list({e.text.strip() for e in author_elements})
            except Exception as e:
                print(f"[{paper_id}] ä½œè€…è§£æå¤±è´¥: {str(e)}")

            # æ‘˜è¦è§£æ
            try:
                abstract_div = soup.find('div', class_='ltx_abstract')
                if abstract_div is None:
                    raise ValueError("æœªæ‰¾åˆ°æ‘˜è¦éƒ¨åˆ†")
                abstract_p = abstract_div.find('p')
                if abstract_p is None:
                    raise ValueError("æ‘˜è¦éƒ¨åˆ†æœªæ‰¾åˆ°æ®µè½")
                result['abstract'] = abstract_p.text
            except Exception as e:
                print(f"[{paper_id}] æ‘˜è¦è§£æå¤±è´¥: {str(e)}")

            # ç« èŠ‚è§£æï¼ˆå¸¦é”™è¯¯éš”ç¦»ï¼‰
            toc_entries = soup.select('.ltx_tocentry') or []
            for entry in toc_entries:
                try:
                    chapter = process_chapter(entry, paper_id, image_dir, soup)
                    result['chapters'].append(chapter)
                except Exception as e:
                    import traceback
                    print(f"[{paper_id}] ç« èŠ‚è§£æå¤±è´¥: ç« èŠ‚ {entry} å‘ç”Ÿé”™è¯¯ï¼Œå…·ä½“ä¿¡æ¯ï¼š{traceback.format_exc()}")
                    continue

            # å‚è€ƒæ–‡çŒ®è§£æ
            try:
                result['references'] = process_references(soup)
            except Exception as e:
                import traceback
                print(f"[{paper_id}] å‚è€ƒæ–‡çŒ®è§£æå¤±è´¥: {traceback.format_exc()}")

            # #è¡¨æ ¼è§£æ
            # try:
            #     result['tables'] = process_tables(soup)
            # except Exception as e:
            #     import traceback
            #     print(f"[{paper_id}] è¡¨æ ¼è§£æå¤±è´¥: {traceback.format_exc()}")

    except Exception as e:
        import traceback
        print(f"[{paper_id}] ä¸¥é‡é”™è¯¯: æ–‡ä»¶ {html_file_path} è§£ææ—¶å‘ç”Ÿé”™è¯¯ï¼Œå…·ä½“ä¿¡æ¯ï¼š{traceback.format_exc()}")
        return None  # è¿”å›Noneè¡¨ç¤ºè§£æå®Œå…¨å¤±è´¥

    return result

def process_chapter(entry, paper_id, image_dir, soup):
    """å¤„ç†å•ä¸ªç« èŠ‚çš„è¾…åŠ©å‡½æ•°"""
    chapter = {'title': '', 'paragraphs': [], 'images': []}

    try:
        # æ ‡é¢˜å¤„ç†
        title_span = entry.find('span', class_='ltx_text ltx_ref_title')
        chapter['title'] = title_span.text.strip() if title_span else "æœªå‘½åç« èŠ‚"

        # å†…å®¹å®šä½
        chapter_id = entry.find('a')['href'].split('#')[-1]
        section = soup.find('section', id=chapter_id)
        if not section:
            return chapter

        # æ®µè½å¤„ç†
        paragraphs = []
        for p in section.find_all('p'):
            try:
                text = p.get_text(separator=' ', strip=True)
                if len(text) > 30:  # è¿‡æ»¤çŸ­æ–‡æœ¬
                    paragraphs.append(text)
            except:
                continue
        chapter['paragraphs'] = paragraphs

        # # å›¾ç‰‡å¤„ç†ï¼ˆå¸¦é‡è¯•ï¼‰
        # for img in section.find_all('img'):
        #     try:
        #         src = img.get('src')
        #         if not src:
        #             continue

        #         img_url = f"https://arxiv.org/html/{paper_id}/{src}"
        #         img_data = download_image(img_url)

        #         if img_data:
        #             chapter['images'].append({
        #                 'data': img_data,
        #                 'description': img.get('alt', '')
        #             })
        #     except Exception as e:
        #         print(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {str(e)}")
        #         continue

    except Exception as e:
        raise RuntimeError(f"ç« èŠ‚å¤„ç†å¤±è´¥: {str(e)}")

    return chapter

def process_references(soup):
    """å¤„ç†å‚è€ƒæ–‡çŒ®è§£æ"""
    references = []
    if soup.find("ul", class_="ltx_biblist"):
        for item in soup.find_all("li", class_="ltx_bibitem"):
            ref_text = " ".join(item.stripped_strings)
            if len(ref_text) > 20:  # è¿‡æ»¤æ— æ•ˆå¼•ç”¨
                references.append(ref_text)
    return references

def process_tables(soup):
    """å¤„ç†è¡¨æ ¼è§£æ"""
    tables = []
    for table in soup.find_all("table", class_="ltx_tabular"):
        table_data = []
        headers = []

        # è¡¨å¤´æ£€æµ‹
        header_row = table.find("tr", class_="ltx_thead")
        if header_row:
            headers = [th.get_text(strip=True) for th in header_row.find_all(["th", "td"])]

        # è¡¨æ ¼å†…å®¹
        for row in table.find_all("tr", class_=lambda x: x != "ltx_thead"):
            cells = [cell.get_text(separator=" ", strip=True) for cell in row.find_all(["td", "th"])]
            if headers and len(cells) == len(headers):
                table_data.append(dict(zip(headers, cells)))
            elif cells:
                table_data.append(cells)

        if table_data:
            tables.append({
                "metadata": {"columns": headers} if headers else {},
                "rows": table_data
            })
    return tables


def download_image(url):
    """å¸¦é‡è¯•æœºåˆ¶çš„å›¾ç‰‡ä¸‹è½½"""
    for attempt in range(MAX_RETRIES):
        try:
            response = http.get(url, timeout=TIMEOUT)
            response.raise_for_status()
            return response.content
        except Exception as e:
            if attempt == MAX_RETRIES - 1:
                print(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥ [{url}] é‡è¯•{MAX_RETRIES}æ¬¡åä»ç„¶å¤±è´¥")
                return None
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

def parse_arxiv(query, max_results=5, output_dir='arxiv_html'):
    """æ”¹è¿›åçš„ä¸»è§£æå‡½æ•°"""
    html_paths = fetch_arxiv_html(query, max_results, output_dir)
    graph = ArxivSemanticGraph() 
    
    success_count = 0
    for idx, html_path in enumerate(html_paths, 1):
        paper_id = os.path.basename(html_path).replace('.html', '')
        try:
            decoded_info = decode_html(html_path)
            if not decoded_info:
                continue
                
            graph.insert(
                paper_id,
                decoded_info['title'],
                decoded_info['authors'],
                decoded_info['abstract'],
                decoded_info['chapters'],
                decoded_info['references'],
                decoded_info.get('tables', [])
            )
            success_count += 1
            print(f"({idx}/{len(html_paths)}) æˆåŠŸå¤„ç† {paper_id}")
        except Exception as e:
            print(f"({idx}/{len(html_paths)}) å¤„ç†å¤±è´¥: {str(e)}")
            continue  # è·³è¿‡å¤±è´¥è®ºæ–‡
            
    print(f"å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¯¼å…¥ {success_count}/{len(html_paths)} ç¯‡è®ºæ–‡")
    graph.semantic_graph.build_index()  # æœ€åç»Ÿä¸€æ„å»ºç´¢å¼•
    return graph

def parse_local_paper(html_dir='arxiv_html'):
    """æ”¹è¿›åçš„æœ¬åœ°è§£æå‡½æ•°"""
    graph = ArxivSemanticGraph()

    if not os.path.exists(html_dir):
        print(f"æŒ‡å®šçš„ç›®å½• {html_dir} ä¸å­˜åœ¨ã€‚")
        return graph

    html_files = [f for f in os.listdir(html_dir) if f.endswith('.html')]
    total_files = len(html_files)
    success_count = 0

    for idx, filename in enumerate(html_files, 1):
        html_path = os.path.join(html_dir, filename)
        paper_id = filename.replace('.html', '')

        try:
            decoded_info = decode_html(html_path)
            if not decoded_info:
                continue

            graph.insert(
                paper_id,
                decoded_info['title'],
                decoded_info['authors'],
                decoded_info['abstract'],
                decoded_info['chapters'],
                decoded_info['references'],
                decoded_info.get('tables', [])
            )
            success_count += 1
            print(f"({idx}/{total_files}) æˆåŠŸå¤„ç† {paper_id}")
        except Exception as e:
            import traceback
            print(f"({idx}/{total_files}) å¤„ç†å¤±è´¥: æ–‡ä»¶ {html_path} å‘ç”Ÿé”™è¯¯ï¼Œå…·ä½“ä¿¡æ¯ï¼š{traceback.format_exc()}")
            continue

    print(f"æœ¬åœ°å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¯¼å…¥ {success_count}/{total_files} ç¯‡è®ºæ–‡")
    graph.semantic_graph.build_index()
    return graph




def show_paper_structure(paper):
    print(f"æ ‡é¢˜: {paper['metadata']['title']}")
    print(f"ä½œè€…: {', '.join(paper['metadata']['authors'])}")
    print(f"æ‘˜è¦: {paper['abstract']}")

    print("\nç« èŠ‚ä¿¡æ¯:")
    for i, chapter in enumerate(paper["chapters"], start=1):
        print(f"  ç« èŠ‚ {i}: {chapter['title']}")
        if chapter["paragraphs"]:
            print("    æ®µè½ä¿¡æ¯:")
            for j, para in enumerate(chapter["paragraphs"], start=1):
                print(f"      æ®µè½ {j}: {para[:200]}...")
        if chapter["images"]:
            print("    å›¾ç‰‡ä¿¡æ¯:")
            for j, img in enumerate(chapter["images"], start=1):
                print(f"      å›¾ç‰‡ {j}: {img.get('description', 'æ— æè¿°')}")

    print("\nå‚è€ƒæ–‡çŒ®ä¿¡æ¯:")
    for i, ref in enumerate(paper["references"], start=1):
        print(f"  å‚è€ƒæ–‡çŒ® {i}: {ref}")

    print("\nè¡¨æ ¼ä¿¡æ¯:")
    for i, table in enumerate(paper["tables"], start=1):
        print(f"  è¡¨æ ¼ {i}:")
        print(f"    åˆ—å: {', '.join(table['metadata']['columns'])}")
        print("    è¡Œæ•°æ®:")
        for j, row in enumerate(table["rows"], start=1):
            if isinstance(row, dict):
                row_str = " | ".join([f"{k}: {v}" for k, v in row.items()])
            else:
                row_str = " | ".join(map(str, row))
            print(f"      è¡Œ {j}: {row_str}")


import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any
import requests
import logging
import os
from datetime import datetime
import sys

DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
DEEPSEEK_API_KEY = "sk-c4e7f78ec6fe48379839541971a2bfc7"
DEEPSEEK_MODEL = "deepseek-chat"
TIMEOUT = 60  


class ArxivAgent:
    def __init__(self, graph,
                 embedding_model: str = "all-MiniLM-L6-v2",
                 similarity_threshold: float = 0.4):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä»£ç†
        :param graph: å·²æ„å»ºçš„ArxivSemanticGraphå®ä¾‹
        :param embedding_model: åµŒå…¥æ¨¡å‹åç§°
        :param similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        self.setup_logging()
        self.graph = graph
        self.embedder = SentenceTransformer(embedding_model)
        self.similarity_threshold = similarity_threshold

        # åˆå§‹åŒ–è¯­ä¹‰ç´¢å¼•
        self.node_embeddings = []
        self.node_info = []
        self._build_hnsw_index()
        self.type_mapping = {
            "paper": "paper_root",
            "papers": "paper_root",
            "abstract": "abstract",
            "chapter": "chapter",
            "reference": "reference",
            "table": "table",
            "paragraph": "paragraph",

            "image": "image",
            "figure": "image",
            "table_row": "table_row",
            "row": "table_row",

            "root": "paper_root",
            "chapters": "chapter",
            "references": "reference",
            "tables": "table",
            "paragraphs": "paragraph",
            "images": "image"
        }
        self.relation_mapping = {
            "has_chapter": ["has_section", "contains_chapter", "chapter"],
            "has_abstract": ["abstract", "paper_abstract"],
            "has_paragraph": ["para", "paragraph", "text_section","has_paragraph"]
        }
        
        
    def _normalize_relation(self, rel: str) -> str:
        original = rel
        rel = rel.lower().strip().replace(" ", "_")
        for std_rel, variants in self.relation_mapping.items():
            if rel == std_rel or rel in variants:
                self.logger.debug(f"[NORMALIZE] å…³ç³»æ ‡å‡†åŒ–: {original} -> {std_rel}")
                return std_rel
        self.logger.debug(f"[NORMALIZE] æœªçŸ¥å…³ç³»ç±»å‹: {original} ä¿æŒåŸæ ·")
        return rel

    def setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        self.logger = logging.getLogger('ArxivAgent')
        self.logger.setLevel(logging.INFO)

        # é˜²æ­¢é‡å¤æ·»åŠ å¤„ç†å™¨
        if not self.logger.handlers:
            # æ–‡ä»¶å¤„ç†å™¨ï¼ˆæŒ‰å¤©åˆ†å‰²ï¼‰
            log_dir = 'logs'
            os.makedirs(log_dir, exist_ok=True)
            log_file = os.path.join(log_dir, f'arxiv_agent_{datetime.now().strftime("%Y%m%d")}.log')
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setFormatter(logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            ))

            # æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler(sys.stdout)
            console_handler.setFormatter(logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s',
                datefmt='%H:%M:%S'
            ))

            # æ·»åŠ å¤„ç†å™¨
            self.logger.addHandler(file_handler)
            self.logger.addHandler(console_handler)

    def _build_hnsw_index(self):
        """é‡å»ºè¯­ä¹‰ç´¢å¼•ï¼ˆåŒ…å«æ‰€æœ‰èŠ‚ç‚¹ï¼‰"""
        # åˆå§‹åŒ–æ•°æ®ç»“æ„
        self.node_embeddings = []
        self.node_info = []
        
        # HNSWå‚æ•°é…ç½®
        dim = self.embedder.get_sentence_embedding_dimension()
        self.index = faiss.IndexHNSWFlat(dim, 32)

        # èŠ‚ç‚¹è¿‡æ»¤å’ŒéªŒè¯
        valid_nodes = []
        for node in self.graph.semantic_graph.semantic_map.data:
            # åŸºæœ¬ç»“æ„éªŒè¯
            if not isinstance(node, tuple) or len(node) < 2:
                print(f"âš ï¸ æ— æ•ˆèŠ‚ç‚¹ç»“æ„: {node}")
                continue
                
            node_key = node[0]
            node_content = node[1]
            
            # é”®æ ¼å¼éªŒè¯
            if not isinstance(node_key, str) or len(node_key) < 3:
                print(f"âš ï¸ éæ³•èŠ‚ç‚¹é”®: {node_key}")
                continue
                
            valid_nodes.append((node_key, node_content))

        print(f"å¼€å§‹ç´¢å¼•æ„å»ºï¼Œæœ‰æ•ˆèŠ‚ç‚¹æ•°: {len(valid_nodes)}")

        # å¤„ç†æœ‰æ•ˆèŠ‚ç‚¹
        for node_key, node_content in valid_nodes:
            try:
                # ç”Ÿæˆæ–‡æœ¬æè¿°
                text = self._generate_node_text(node_key, node_content)
                if not text.strip():
                    print(f"ğŸˆ³ ç©ºå†…å®¹èŠ‚ç‚¹: {node_key}")
                    continue

                # ç”ŸæˆåµŒå…¥
                embedding = self.embedder.encode(text)
                if embedding.ndim == 2:
                    embedding = embedding.squeeze(0)

                # æ”¶é›†å…ƒæ•°æ®
                self.node_embeddings.append(embedding)
                self.node_info.append({
                    "key": node_key,
                    "type": self._get_node_type(node_key),
                    "content": node_content,
                    "paper_id": node_key.split("_")[0] if '_' in node_key else 'unknown'
                })

            except Exception as e:
                print(f"âŒ å¤„ç†èŠ‚ç‚¹ {node_key} å¤±è´¥: {str(e)}")
                traceback.print_exc()
                continue

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if len(self.node_embeddings) == 0:
            raise ValueError("æ— æœ‰æ•ˆèŠ‚ç‚¹å¯ä¾›ç´¢å¼•")
            
        self.node_embeddings = np.array(self.node_embeddings)
        self.index.add(self.node_embeddings)
        
        # å¥åº·æ£€æŸ¥
        print(f"\nç´¢å¼•æ„å»ºå®ŒæˆçŠ¶æ€:")
        print(f"- åµŒå…¥å‘é‡æ•°: {self.index.ntotal}")
        print(f"- èŠ‚ç‚¹ä¿¡æ¯æ•°: {len(self.node_info)}")
        print(f"- åµŒå…¥ç»´åº¦: {self.node_embeddings.shape[1]}")
        
        return True

    def _get_node_type(self, key: str) -> str:
        """æ ¹æ®èŠ‚ç‚¹é”®è¯†åˆ«ç±»å‹"""
        if "_abstract" in key:
            return "abstract"
        elif "_title_authors" in key:
            return "paper_root"
        elif "_para_" in key:
            return "paragraph"
        elif "_img_" in key:
            return "image"
        elif "_chapter_" in key:
            return "chapter"
        elif "_ref_" in key:
            return "reference"
        elif "_table_" in key:
            return "table"
        return "unknown"

    def _generate_node_text(self, key: str, content: Any) -> str:
        node_type = self._get_node_type(key)

        if isinstance(content, str):
            return content

        if node_type == "paper_root":
            return (
                f"Paper: {content.get('title', 'Unknown Title')} by {', '.join(content.get('authors', []))}"
                f" ({content.get('publication_date', 'unknown')})"
            )
        elif node_type == "chapter":
            return (
                f"Chapter {content.get('section_number', 'X')}: {content.get('title', 'Unknown Title')}"
                f" (parent chapter: {content.get('parent_chapter', 'None')})"
            )
        elif node_type == "paragraph":
            return (
                f"Paragraph {content.get('position_in_chapter', '?')}: {content.get('text', '').strip()[:200]}..."
                f" (in chapter: {content.get('chapter_title', 'Unknown')})"
            )
        elif node_type == "reference":
            return (
                f"Reference: {content.get('title', 'No Title')}"
                f" by {', '.join(content.get('authors', []))} ({content.get('publication_year', '?')})"
            )
        elif node_type == "table":
            return (
                f"Table with columns: {', '.join(content.get('columns', []))}"
                f" containing {content.get('row_count', 0)} rows"
            )
        return str(content)

    def _semantic_search(self, query: str, candidates: List[Dict], top_k: int = 5) -> List[Dict]:
        """å‚æ•°æ·»åŠ top_kå¹¶å®ç°æ­£ç¡®æœç´¢é€»è¾‘"""
        if not candidates:
            return []

        # è·å–å€™é€‰èŠ‚ç‚¹çš„ç´¢å¼•
        candidate_indices = [i for i, n in enumerate(self.node_info) if n in candidates]
        candidate_embeddings = self.node_embeddings[candidate_indices]

        # åˆ›å»ºä¸´æ—¶ç´¢å¼•
        temp_index = faiss.IndexFlatL2(candidate_embeddings.shape[1])
        temp_index.add(candidate_embeddings)

        # æ‰§è¡Œæœç´¢
        query_embedding = self.embedder.encode(query)
        if query_embedding.ndim == 2:
            query_embedding = query_embedding.squeeze(0)

        top_k = min(top_k, len(candidates))
        distances, indices = temp_index.search(query_embedding.reshape(1, -1), top_k)

        # è½¬æ¢ç»“æœ
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < 0:
                continue
            original_idx = candidate_indices[idx]
            results.append({
                **self.node_info[original_idx],
                "score": 1 - distances[0][i]
            })

        return sorted(results, key=lambda x: -x['score'])

    def _structural_search(self, target: str, input_data: List) -> List[Dict]:
        """ç»“æ„åŒ–æœç´¢"""
        results = []
        paper_ids = [item["paper_id"] for item in input_data]

        for node in self.node_info:
            if node["type"] == target and node["paper_id"] in paper_ids:
                results.append({
                    "key": node["key"],
                    "type": node["type"],
                    "paper_id": node["paper_id"],
                    "content": node["content"]
                })
        return results

    def _resolve_input_data(self, step: Dict, context: Dict) -> List:
        """è§£æè¾“å…¥ä¾èµ–"""
        input_data = []
        for inp in step.get("input", []):
            if "from step" in inp:
                ref_step = int(inp.split()[-1])
                input_data.extend(context.get(f"step_{ref_step}", []))
        return list({item["paper_id"] for item in input_data})

    def generate_answer(self, context: Dict, query: str) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆæœ€ç»ˆå›ç­”"""
        prompt = f"""
        æ ¹æ®ä»¥ä¸‹æŸ¥è¯¢æ‰§è¡Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„å›ç­”ã€‚
        æŸ¥è¯¢è¯·æ±‚ï¼š{query}
        ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context}
        """
        response = self._call_llm(prompt)
        return response

    def _call_llm(self, prompt: str) -> str:
        """æ”¹è¿›çš„APIè°ƒç”¨æ–¹æ³•ï¼Œæ·»åŠ é‡è¯•æœºåˆ¶"""
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
        }
        data = {
            "model": DEEPSEEK_MODEL,
            "messages": [{"role": "user", "content": prompt[:3000]}],
            "temperature": 0.7,
        }

        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    DEEPSEEK_API_URL,
                    headers=headers,
                    json=data,
                    timeout=TIMEOUT  # å»¶é•¿è¶…æ—¶æ—¶é—´
                )
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    self.logger.warning(f"APIè°ƒç”¨å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•... ({attempt + 1}/{max_retries})")
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                self.logger.error("æ— æ³•ç”ŸæˆæŠ¥å‘Šï¼šAPIè¯·æ±‚å¤±è´¥")
                raise
        return "æ— æ³•ç”ŸæˆæŠ¥å‘Šï¼šAPIè¯·æ±‚å¤±è´¥"

    def print_index_status(self):
        """æ‰“å°ç´¢å¼•çŠ¶æ€"""
        self.logger.info("å½“å‰ç´¢å¼•çŠ¶æ€:")
        self.logger.info(f"ç´¢å¼•åŒ…å« {self.index.ntotal} ä¸ªå‘é‡")
        self.logger.info(f"èŠ‚ç‚¹ä¿¡æ¯æ•°é‡: {len(self.node_info)}")
        self.logger.info(f"åµŒå…¥çŸ©é˜µå½¢çŠ¶: {self.node_embeddings.shape}")
        self.logger.info("å‰5ä¸ªèŠ‚ç‚¹ç±»å‹:")
        for info in self.node_info[:5]:
            self.logger.info(f"- {info['type']}: {info['key']}")

    def print_query_results(self, context: Dict[str, Any]):
        """æ‰“å°æŸ¥è¯¢è®¡åˆ’æ¯ä¸€æ­¥çš„æœç´¢ç»“æœ"""
        for step, results in context.items():
            self.logger.info(f"\n{'=' * 30} {step} æœç´¢ç»“æœ {'=' * 30}")
            if not results:
                self.logger.info(f"{step} æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚")
                continue
            for i, result in enumerate(results, start=1):
                self.logger.info(f"ç»“æœ {i}:")
                self.logger.info(f"  èŠ‚ç‚¹ç±»å‹: {result['type']}")
                self.logger.info(f"  è®ºæ–‡ ID: {result['paper_id']}")
                self.logger.info(f"  èŠ‚ç‚¹é”®: {result['key']}")
                if 'similarity' in result:
                    self.logger.info(f"  ç›¸ä¼¼åº¦: {result['similarity']:.4f}")
                self.logger.info(f"  å†…å®¹: {str(result['content'])[:200]}...")

    def check_index_health(self):
        """æ£€æŸ¥ç´¢å¼•å¥åº·çŠ¶å†µ"""
        self.logger.info("\n=== ç´¢å¼•è¯Šæ–­æŠ¥å‘Š ===")

        # åŸºç¡€ç»Ÿè®¡
        self.logger.info(f"ç´¢å¼•åŒ…å«å‘é‡æ•°: {self.index.ntotal}")
        self.logger.info(f"èŠ‚ç‚¹ä¿¡æ¯è®°å½•æ•°: {len(self.node_info)}")
        self.logger.info(f"åµŒå…¥çŸ©é˜µå½¢çŠ¶: {self.node_embeddings.shape}")

        # ç±»å‹åˆ†å¸ƒç»Ÿè®¡
        type_counts = {}
        for info in self.node_info:
            t = info['type']
            type_counts[t] = type_counts.get(t, 0) + 1
        self.logger.info("\nèŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ:")
        for t, cnt in type_counts.items():
            self.logger.info(f"- {t}: {cnt}")

        # éšæœºé‡‡æ ·æ£€æŸ¥
        self.logger.info("\néšæœºé‡‡æ ·æ£€æŸ¥:")
        for _ in range(3):
            idx = np.random.randint(0, len(self.node_info))
            node = self.node_info[idx]
            self.logger.info(f"Key: {node['key']}")
            self.logger.info(f"Type: {node['type']}")
            self.logger.info(f"Embedding Norm: {np.linalg.norm(self.node_embeddings[idx]):.2f}")
            self.logger.info(f"Content Sample: {str(node['content'])[:100]}...\n")

    def validate_query_plan(self, plan: List[Dict]):
        """éªŒè¯æŸ¥è¯¢è®¡åˆ’æœ‰æ•ˆæ€§"""
        valid_types = {'paper_root', 'abstract', 'chapter', 'reference', 'table'}

        for step in plan:
            # æ ¡éªŒæ­¥éª¤ç¼–å·
            if 'step' not in step or not isinstance(step['step'], int):
                self.logger.error(f"æ­¥éª¤ç¼ºå°‘æœ‰æ•ˆç¼–å·: {step}")
                raise ValueError(f"æ­¥éª¤ç¼ºå°‘æœ‰æ•ˆç¼–å·: {step}")

            # æ ¡éªŒç›®æ ‡ç±»å‹
            target = step.get('target', '').lower()
            if target not in valid_types:
                self.logger.error(f"æ— æ•ˆç›®æ ‡ç±»å‹: {target}ï¼Œæœ‰æ•ˆç±»å‹ä¸º: {valid_types}")
                raise ValueError(f"æ— æ•ˆç›®æ ‡ç±»å‹: {target}ï¼Œæœ‰æ•ˆç±»å‹ä¸º: {valid_types}")

            # æ ¡éªŒçº¦æŸæ¡ä»¶
            constraints = step.get('constraints', {})
            if not constraints:
                self.logger.error(f"æ­¥éª¤ {step['step']} ç¼ºå°‘çº¦æŸæ¡ä»¶")
                raise ValueError(f"æ­¥éª¤ {step['step']} ç¼ºå°‘çº¦æŸæ¡ä»¶")

            # æ ¡éªŒè¾“å…¥ä¾èµ–
            inputs = step.get('input', [])
            for inp in inputs:
                if not inp.startswith('step_'):
                    self.logger.error(f"æ— æ•ˆè¾“å…¥ä¾èµ–æ ¼å¼: {inp}ï¼Œåº”ç±»ä¼¼ 'step_1'")
                    raise ValueError(f"æ— æ•ˆè¾“å…¥ä¾èµ–æ ¼å¼: {inp}ï¼Œåº”ç±»ä¼¼ 'step_1'")

    def validate_query_plan(self, plan: List[Dict]):
        """æŸ¥è¯¢è®¡åˆ’éªŒè¯"""
        valid_types = set(self.type_mapping.values())  # ä½¿ç”¨æ˜ å°„åçš„ç±»å‹

        for step in plan:
            # æ ¡éªŒç›®æ ‡ç±»å‹
            target_type = step.get('target', '')
            if target_type not in valid_types:
                self.logger.error(
                    f"æ— æ•ˆç›®æ ‡ç±»å‹: '{target_type}'\n"
                    f"æœ‰æ•ˆç±»å‹åº”ä¸º: {', '.join(valid_types)}\n"
                    f"æç¤ºï¼šå¯ä»¥ä½¿ç”¨ç±»ä¼¼'paper_root'ç­‰ç³»ç»Ÿå®šä¹‰çš„ç±»å‹åç§°"
                )
                raise ValueError(
                    f"æ— æ•ˆç›®æ ‡ç±»å‹: '{target_type}'\n"
                    f"æœ‰æ•ˆç±»å‹åº”ä¸º: {', '.join(valid_types)}\n"
                    f"æç¤ºï¼šå¯ä»¥ä½¿ç”¨ç±»ä¼¼'paper_root'ç­‰ç³»ç»Ÿå®šä¹‰çš„ç±»å‹åç§°"
                )

            # æ ¡éªŒçº¦æŸæ¡ä»¶
            if "semantic_query" not in step["constraints"] and "selected_papers" not in step["constraints"]:
                self.logger.warning(f"è­¦å‘Šï¼šæ­¥éª¤ {step['step']} ç¼ºå°‘æœ‰æ•ˆçº¦æŸæ¡ä»¶")

    # function call
    def search_units(self,
                     entity_type: str,
                     attribute_filters: Dict = None,
                     relation_filters: Dict = None,
                     keywords: str = None,
                     top_k: int = 15) -> List[Dict]:
        """
        å¤šæ¡ä»¶ç»„åˆæœç´¢å‡½æ•°
        :param entity_type: ç›®æ ‡å®ä½“ç±»å‹ï¼ˆpaper_root/abstract/chapterç­‰ï¼‰
        :param attribute_filters: å±æ€§è¿‡æ»¤æ¡ä»¶ {"authors": ["John"], "year": 2023}
        :param relation_filters: å…³ç³»è¿‡æ»¤ {"has_reference": True}
        :param keywords: å…³é”®è¯è¯­ä¹‰æœç´¢
        :param top_k: è¿”å›ç»“æœæ•°é‡
        """
        if isinstance(top_k, list):
            top_k = int(top_k[0]) if len(top_k) > 0 else 5
        top_k = int(top_k)

        # ç¡®ä¿entity_typeæ˜¯å­—ç¬¦ä¸²
        if isinstance(entity_type, list):
            entity_type = entity_type[0] if len(entity_type) > 0 else "paper_root"

        # ç¡®ä¿keywordsæ˜¯å­—ç¬¦ä¸²
        if keywords and isinstance(keywords, list):
            keywords = " ".join(keywords)
        # æ­¥éª¤1ï¼šæŒ‰ç±»å‹è¿‡æ»¤
        candidates = [n for n in self.node_info if n['type'] == entity_type]

        # æ­¥éª¤2ï¼šå±æ€§è¿‡æ»¤
        if attribute_filters:
            candidates = self._apply_attribute_filters(candidates, attribute_filters)

        # æ­¥éª¤3ï¼šè¯­ä¹‰æœç´¢
        if keywords:
            candidates = self._semantic_search(keywords, candidates, top_k=top_k * 2)

        # æ­¥éª¤4ï¼šå…³ç³»è¿‡æ»¤
        if relation_filters:
            candidates = self._apply_relation_filters(candidates, relation_filters)

        # æœ€ç»ˆæ’åºå’Œæˆªå–
        results = sorted(
            candidates,
            key=lambda x: x.get('score', 0),
            reverse=True
        )[:top_k]
        self.logger.info(f"search_units æ‰¾åˆ° {len(results)} æ¡ç»“æœ")
        return results

    def _apply_attribute_filters(self, candidates, filters):
        filtered = []
        for node in candidates:
            match = True
            content = node['content']
            for attr, values in filters.items():
                # å¤„ç†ä¸åŒèŠ‚ç‚¹ç±»å‹çš„å±æ€§æå–
                value = None
                if node['type'] == 'paper_root':
                    value = content.get(attr, '')
                elif node['type'] == 'reference':
                    value = self._extract_ref_attr(content, attr)
                # å…¶ä»–ç±»å‹å¤„ç†

                if not self._check_value_match(value, values):
                    match = False
                    break
            if match:
                filtered.append(node)
        return filtered

    def _extract_ref_attr(self, ref_text, attr):
        # å®ç°å‚è€ƒæ–‡çŒ®å±æ€§æå–é€»è¾‘
        if attr == 'year':
            return re.findall(r'\b(19|20)\d{2}\b', ref_text)[-1]  # ç®€å•å®ç°
        return ''

    def _check_value_match(self, actual, expected_values):
        if isinstance(actual, list):
            return any(v in actual for v in expected_values)
        return actual in expected_values

    def _apply_relation_filters(self, candidates, filters):
        filtered = []
        for node in candidates:
            relations = self.graph.graph_relations.get(node["key"], {})
            match = True
            for rel_type, required in filters.items():
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æŒ‡å®šç±»å‹çš„å…³ç³»
                if required and not any(
                        r == rel_type
                        for r in relations.get("children", {}).values()
                ):
                    match = False
                    break
            if match:
                filtered.append(node)
        return filtered
    
    def _normalize_node_type(self, node_type: str) -> str: 
        node_type = node_type.lower().replace(" ", "_")
        return self.type_mapping.get(node_type, node_type)

    def traverse_relations(self, 
                         source_keys: List[str],
                         relation_types: List[str],
                         target_types: List[str],
                         max_depth: int = 1) -> Dict[str, List[Dict]]:
        # å…³ç³»ç±»å‹æ ‡å‡†åŒ–
        # std_relations = [self._normalize_relation(rt) for rt in relation_types]
        std_targets = [self._normalize_node_type(nt) for nt in target_types]
        # print(f"[DEBUG] å¼€å§‹éå† æºèŠ‚ç‚¹: {source_keys}")
        # print(f"[DEBUG] æ ‡å‡†åŒ–å‰å…³ç³»ç±»å‹: {relation_types}")
        std_relations = [self._normalize_relation(rt) for rt in relation_types]
        # print(f"[DEBUG] æ ‡å‡†åŒ–åå…³ç³»ç±»å‹: {std_relations}")
        # è°ƒè¯•æ—¥å¿—
        # self.logger.info(
        #     f"[TRAVERSE] å¼€å§‹éå†å…³ç³»ç±»å‹: {relation_types}->{std_relations} "
        #     f"ç›®æ ‡ç±»å‹: {target_types}->{std_targets} æœ€å¤§æ·±åº¦: {max_depth}"
        # )

        results = defaultdict(list)
        visited = set()

        def dfs(current_key: str, current_depth: int):
            # print(f"[DFS] å½“å‰èŠ‚ç‚¹: {current_key}, æ·±åº¦: {current_depth}")
            if current_key in visited or current_depth > max_depth:
                return
            visited.add(current_key)

            # èŠ‚ç‚¹ä¿¡æ¯éªŒè¯
            node = self._get_node_info(current_key)
            if not node:
                self.logger.warning(f"[TRAVERSE] æ— æ•ˆèŠ‚ç‚¹: {current_key}")
                return

            node_type = self._normalize_node_type(node["type"])
            # print("node_type"+node_type)
            # ç»“æœæ”¶é›†
            if node_type in std_targets:
                results[node_type].append(node)
                self.logger.info(
                    f"[TRAVERSE] å‘ç°ç›®æ ‡èŠ‚ç‚¹ {current_key} "
                    f"ç±»å‹: {node_type} æ·±åº¦: {current_depth}"
                )

            # å…³ç³»éå†
            child_relations = self.graph.semantic_graph.graph_relations.get(current_key, {})
            # print(f"[DFS] èŠ‚ç‚¹ {current_key} çš„å­å…³ç³»: {child_relations}")
            for raw_rel, children in child_relations.get("children", {}).items():
                std_rel = self._normalize_relation(raw_rel)
                # print(f"[DFS] å¤„ç†å…³ç³» {raw_rel} -> {std_rel}")
                if std_rel not in std_relations:
                    continue

                for child_key in children:
                    dfs(child_key, current_depth + 1)

        # æ‰§è¡Œéå†
        for key in source_keys:
            if not self._validate_node_key(key):
                continue
            dfs(key, 0)

        return dict(results)
    
    def _validate_node_key(self, key: str) -> bool:
        """éªŒè¯èŠ‚ç‚¹é”®æœ‰æ•ˆæ€§"""
        # å¤„ç†ç©ºå­—ç¬¦ä¸²
        if not key:
            self.logger.error("[VALIDATE] è·³è¿‡ç©ºèŠ‚ç‚¹é”®")
            return False
        # å¤„ç†å­—å…¸ç±»å‹
        if isinstance(key, dict):
            key = key.get("key", "")
        exists = any(n[0] == key for n in self.graph.semantic_graph.semantic_map.data)
        if not exists:
            self.logger.error(f"[VALIDATE] èŠ‚ç‚¹ä¸å­˜åœ¨: {key}")
        return exists
    
    def _get_node_info(self, key: str) -> Optional[Dict]:
        """å¸¦ç¼“å­˜çš„èŠ‚ç‚¹ä¿¡æ¯è·å–"""
        if not hasattr(self, "_node_cache"):
            self._node_cache = {n["key"]: n for n in self.node_info}
        
        if info := self._node_cache.get(key):
            return info
            
        # å®æ—¶å›é€€æŸ¥è¯¢
        node = next((n for n in self.graph.semantic_graph.semantic_map.data if n[0] == key), None)
        if node:
            return {
                "key": node[0],
                "type": self._get_node_type(node[0]),
                "content": node[1],
                "paper_id": node[0].split("_")[0]
            }
        return None
    
    def _post_process_results(self, raw_results: Dict) -> Dict:
        """ç»“æœåå¤„ç†"""
        processed = {}
        for result_type, nodes in raw_results.items():
            # å»é‡å¤„ç†
            seen = set()
            unique_nodes = []
            for node in nodes:
                if node["key"] not in seen:
                    seen.add(node["key"])
                    unique_nodes.append(node)
            # æŒ‰è®ºæ–‡IDæ’åº
            processed[result_type] = sorted(
                unique_nodes, 
                key=lambda x: x["paper_id"]
            )
        return processed

    def _resolve_param_item(self, item, context):
        """è¿”å›åŸºç¡€ç±»å‹æˆ–å­—å…¸/åˆ—è¡¨ç»“æ„"""
        # å¤„ç†å˜é‡å¼•ç”¨
        if isinstance(item, str) and item.startswith("$"):
            referred = context.get(item[1:], [])
            return self._resolve_param_item(referred, context)

        # å¤„ç†åˆ—è¡¨
        if isinstance(item, list):
            return [self._resolve_param_item(elem, context) for elem in item]

        # ä¿ç•™å­—å…¸ç»“æ„
        if isinstance(item, dict):
            return {k: self._resolve_param_item(v, context) for k, v in item.items()}

        # åŸºç¡€ç±»å‹ç›´æ¥è¿”å›
        return item

    def _resolve_params(self, params: Dict, context: Dict) -> Dict:
        """å‚æ•°è§£æï¼Œç¡®ä¿ç±»å‹å®‰å…¨"""
        resolved = {}
        for k, v in params.items():
            if k in ["source_keys", "node_keys"]:
                raw_items = self._resolve_param_item(v, context)
                resolved[k] = []
                # é€’å½’å±•å¹³åµŒå¥—ç»“æ„å¹¶æå–èŠ‚ç‚¹é”®
                self._extract_node_keys(raw_items, resolved[k])
                # å»é‡å¹¶è¿‡æ»¤ç©ºå€¼
                resolved[k] = [k for k in list(set(resolved[k])) if k]
            else:
                resolved_item = self._resolve_param_item(v, context)
                # å¤„ç†æ•°å€¼å‚æ•°
                if k in ["top_k", "max_depth", "depth"]:
                    resolved[k] = int(resolved_item) if resolved_item else 0
                # å¤„ç†å®ä½“ç±»å‹
                elif k == "entity_type":
                    resolved[k] = resolved_item[0] if isinstance(resolved_item, list) else str(resolved_item)
                # å¤„ç†å…³é”®è¯
                elif k == "keywords":
                    resolved[k] = " ".join(resolved_item) if isinstance(resolved_item, list) else str(resolved_item)
                else:
                    resolved[k] = resolved_item
        return resolved

    def _extract_node_keys(self, item, resolved_list):
        """é€’å½’æå–èŠ‚ç‚¹é”®"""
        if isinstance(item, list):
            for elem in item:
                self._extract_node_keys(elem, resolved_list)
        elif isinstance(item, dict):
            # ä¼˜å…ˆæå– key å­—æ®µ
            key = item.get("key", "")
            if key:
                resolved_list.append(key)
            else:
                # é€’å½’å¤„ç†æ‰€æœ‰å€¼
                for value in item.values():
                    self._extract_node_keys(value, resolved_list)
        elif isinstance(item, str):
            resolved_list.append(item)
    
    def generate_report(self, context: Dict, query: str) -> str:
        """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š"""
        prompt = f"""
        æ ¹æ®ä»¥ä¸‹åˆ†æç»“æœç”Ÿæˆä¸­æ–‡æŠ¥å‘Šï¼š
        åŸå§‹æŸ¥è¯¢ï¼š{query}
        ä¸Šä¸‹æ–‡æ•°æ®ï¼š{json.dumps(context, indent=2)}

        """

        report = self._call_llm(prompt)
        self.logger.info("æˆåŠŸç”Ÿæˆåˆ†ææŠ¥å‘Š")
        return report

    def semantic_traversal(self, source_keys: List[str], top_k: int = 3, depth: int = 2) -> Dict[str, List]:
        """è¯­ä¹‰éå†"""
        # å‚æ•°é¢„å¤„ç†
        def validate_key(key):
            """ä¸¥æ ¼æ ¡éªŒé”®æ ¼å¼å¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
            if isinstance(key, dict):
                return str(key.get("key", ""))
            return str(key)

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼éªŒè¯arxiv IDæ ¼å¼
        arxiv_id_pattern = re.compile(r"^\d+\.\d+[vV]\d+_.+")
        validated_keys = [
            key_str
            for key in source_keys
            if (key_str := validate_key(key))
               and arxiv_id_pattern.match(key_str)
               and any(n["key"] == key_str for n in self.node_info)
        ]

        self.logger.info(f"[DEBUG] è¯­ä¹‰éå†æœ‰æ•ˆèŠ‚ç‚¹é”® ({len(validated_keys)}): {validated_keys[:3]}...")

        results = defaultdict(list)
        visited = set()

        def dfs(current_key: str, current_depth: int):
            # ç±»å‹å®‰å…¨æ–­è¨€
            if not isinstance(current_key, str):
                raise TypeError(f"éæ³•èŠ‚ç‚¹é”®ç±»å‹: {type(current_key)} -> {current_key}")

            if current_depth > depth or current_key in visited:
                return
            visited.add(current_key)

            try:
                # è·å–å½“å‰èŠ‚ç‚¹çš„ç´¢å¼•
                node_index = next(i for i, n in enumerate(self.node_info) if n["key"] == current_key)
                query_embedding = self.node_embeddings[node_index]

                # æœç´¢ç›¸ä¼¼èŠ‚ç‚¹
                distances, indices = self.index.search(query_embedding.reshape(1, -1), top_k + 1)

                for idx in indices[0]:
                    if 0 <= idx < len(self.node_info):
                        neighbor = self.node_info[idx]
                        if neighbor["key"] != current_key:
                            results[current_key].append(neighbor)
                            dfs(neighbor["key"], current_depth + 1)
            except StopIteration:
                self.logger.warning(f"[WARN] èŠ‚ç‚¹ {current_key} ä¸å­˜åœ¨äºç´¢å¼•ä¸­")
            except Exception as e:
                self.logger.error(f"[ERROR] å¤„ç†èŠ‚ç‚¹ {current_key} æ—¶å‡ºé”™: {str(e)}")
                traceback.print_exc()

        for key in validated_keys:
            dfs(key, 0)

        total_nodes = sum(len(v) for v in results.values())
        self.logger.info(f"semantic_traversal æ‰¾åˆ° {total_nodes} ä¸ªç›¸ä¼¼èŠ‚ç‚¹")
        return dict(results)
  
    def refine_knowledge(self, node_keys: List[Union[str, dict]]) -> List[str]:
        """çŸ¥è¯†æç‚¼æ–¹æ³•"""
        # å‚æ•°é¢„å¤„ç†å’Œç±»å‹æ ¡éªŒ
        validated_keys = []
        for item in node_keys:
            try:
                if isinstance(item, dict):
                    key = str(item.get("key", ""))
                elif isinstance(item, str):
                    key = item.strip()
                else:
                    self.logger.warning(f"éæ³•å‚æ•°ç±»å‹: {type(item)}")
                    continue
                
                # é”®æ ¡éªŒ
                if not re.match(r"^[\w\.\-:_]+$", key):
                    self.logger.warning(f"éæ³•é”®æ ¼å¼: {key}")
                    continue
                    
                validated_keys.append(key)
            except Exception as e:
                self.logger.error(f"å‚æ•°å¤„ç†é”™è¯¯: {str(e)}")
                continue

        new_nodes = []
        for key in validated_keys:
            self.logger.info(f"\nå¤„ç†èŠ‚ç‚¹: {key}")
            try:
                # æ¸…ç†æ—§æ‘˜è¦å¹¶ä¿ç•™å…³ç³»ç»“æ„
                self._clean_old_summaries(key)
                
                # è·å–å¹¶æ ¡éªŒèŠ‚ç‚¹å†…å®¹
                node_content = self._get_node_content(key)
                if not node_content or not isinstance(node_content, dict):
                    self.logger.warning("æ— æ•ˆèŠ‚ç‚¹å†…å®¹ï¼Œè·³è¿‡")
                    continue
                    
                # ç”Ÿæˆæ‘˜è¦
                summary = self._generate_summary(node_content)
                if not summary or len(summary) < 50:
                    self.logger.warning("æ‘˜è¦ç”Ÿæˆå¤±è´¥æˆ–è¿‡çŸ­")
                    continue
                    
                # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ‘˜è¦èŠ‚ç‚¹
                timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                summary_key = f"{key}_summary_{timestamp}"
                
                # å…³ç³»ç»‘å®š
                self.graph.semantic_graph.add_node(
                    summary_key,
                    {"type": "summary", "content": summary},
                    parent_keys=[key],
                    parent_relation="has_summary"
                )
                parent_relations = self.graph.semantic_graph.graph_relations.get(key, {}).get("children", {})
                if "has_summary" not in parent_relations:
                    self.logger.error(f"å…³ç³»å»ºç«‹å¤±è´¥ï¼çˆ¶èŠ‚ç‚¹ {key} æ—  has_summary å…³ç³»")
                else:
                    self.logger.info(f"æˆåŠŸå»ºç«‹å…³ç³»: {key} â†’ {summary_key}")
                
                # ç«‹å³æ›´æ–°ç´¢å¼•
                self._update_index_with_new_node(summary_key, summary)
                
                new_nodes.append(summary_key)
                self.logger.info(f"åˆ›å»ºæ‘˜è¦èŠ‚ç‚¹: {summary_key}")
                self.logger.info(f"content:{summary}")
            except Exception as e:
                self.logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
                continue

        return new_nodes

    def _update_index_with_new_node(self, key: str, content: dict):
        """åŠ¨æ€æ›´æ–°ç´¢å¼•"""
        text = self._generate_node_text(key, content)
        embedding = self.embedder.encode(text)
        
        if embedding.ndim == 2:
            embedding = embedding.squeeze(0)
            
        # æ›´æ–°ç´¢å¼•æ•°æ®ç»“æ„
        self.node_embeddings = np.vstack([self.node_embeddings, embedding])
        self.node_info.append({
            "key": key,
            "type": "summary",
            "content": content,
            "paper_id": key.split("_")[0] if '_' in key else 'unknown'
        })
        
        # é‡å»ºHNSWç´¢å¼•
        self.index.add(np.array([embedding]))
        self.logger.info("åŠ¨æ€æ›´æ–°ç´¢å¼•æˆåŠŸ")
    def _get_node_content(self, node_key: str) -> dict:
        """å®‰å…¨è·å–èŠ‚ç‚¹å†…å®¹"""
        try:
            # åœ¨semantic_mapä¸­ç²¾ç¡®æŸ¥æ‰¾
            node = next(n for n in self.graph.semantic_graph.semantic_map.data 
                        if n[0] == node_key)
                        
            # å†…å®¹è§„èŒƒåŒ–
            if isinstance(node[1], dict):
                return node[1]
            else:
                return {"raw_content": node[1]}  # åŒ…è£…éå­—å…¸å†…å®¹
        except StopIteration:
            print(f"ğŸ” èŠ‚ç‚¹ä¸å­˜åœ¨: {node_key}")
            return {}
        except Exception as e:
            print(f"è·å–å†…å®¹å¤±è´¥: {str(e)}")
            return {}


    def _generate_summary(self, node_content: dict) -> Optional[dict]:
        # å†…å®¹æå–
        content_type = node_content.get("type", "unknown")
        raw_text = node_content.get("text") or str(node_content.get("raw_content", ""))

        # åŠ¨æ€é•¿åº¦æ ¡éªŒ
        min_length = {
            "paper_root": 30,
            "chapter": 50,
            "paragraph": 80,
            "reference": 20,
            "summary": 0  # é¿å…é‡å¤ç”Ÿæˆ
        }.get(content_type, 50)

        if len(raw_text) < min_length:
            self.logger.warning(f"å†…å®¹è¿‡çŸ­ ({len(raw_text)} < {min_length}): {raw_text[:50]}...")
            return None

        # æ„é€ æç¤ºè¯ï¼Œæ˜ç¡®è¦æ±‚è¾“å‡ºä¸ºJSONæ ¼å¼
        prompt = f"""
        è¯·æ ¹æ®ä»¥ä¸‹{self._type_mapping_zh(content_type)}å†…å®¹ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦ï¼Œè¾“å‡ºæ ¼å¼ä¸ºJSONï¼š
        {{
            "æ ¸å¿ƒè§‚ç‚¹": "ç”¨20 - 50å­—æ¦‚æ‹¬æ ¸å¿ƒè§‚ç‚¹",
            "å…³é”®æ–¹æ³•/æ•°æ®": ["åˆ—å‡º3 - 5é¡¹å…³é”®æ–¹æ³•æˆ–æ•°æ®"],
            "ç ”ç©¶ä»·å€¼ä¸å±€é™": "ç®€è¦é˜è¿°ç ”ç©¶ä»·å€¼ä¸å±€é™ï¼Œå„1 - 2é¡¹"
        }}

        åŸå§‹å†…å®¹ï¼š
        {raw_text[:3000]}
        """

        # è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦
        return self._call_llm(prompt)
    def _type_mapping_zh(self, en_type: str) -> str:
            """ç±»å‹ä¸­è‹±æ˜ å°„"""
            mapping = {
                "paper_root": "è®ºæ–‡å…ƒæ•°æ®",
                "chapter": "ç« èŠ‚",
                "paragraph": "æ®µè½",
                "reference": "å‚è€ƒæ–‡çŒ®",
                "table": "è¡¨æ ¼",
                "image": "å›¾ç‰‡",
                "summary": "æ‘˜è¦"
            }
            return mapping.get(en_type, "å†…å®¹")
    def _has_valid_summary(self, source_key: str, summary_key: str) -> bool:
        """æ·»åŠ ç±»å‹æ ¡éªŒ"""
        # æ ¡éªŒé”®ç±»å‹
        if not isinstance(source_key, str) or not isinstance(summary_key, str):
            return False

        # å­˜åœ¨æ€§æ£€æŸ¥
        if not any(n[0] == summary_key for n in self.graph.semantic_graph.semantic_map.data):
            return False

        # å…³ç³»æ£€æŸ¥ï¼ˆæ·»åŠ å¼‚å¸¸æ•è·ï¼‰
        try:
            relations = self.graph.semantic_graph.graph_relations.get(str(source_key), {})
            return any(
                rel == "has_summary" and summary_key in children
                for rel, children in relations.get("children", {}).items()
            )
        except TypeError:  # å¤„ç†ä¸å¯å“ˆå¸Œç±»å‹
            return False

    def _clean_old_summaries(self, source_key: str):
        """æ¸…ç†æ—§æ‘˜è¦èŠ‚ç‚¹"""
        # æŸ¥æ‰¾æ‰€æœ‰å†å²æ‘˜è¦
        old_summaries = [
            n[0] for n in self.graph.semantic_graph.semantic_map.data
            if n[0].startswith(f"{source_key}_summary")
        ]
        
        # åˆ é™¤èŠ‚ç‚¹å’Œå…³ç³»
        for skey in old_summaries:
            try:
                self.graph.semantic_graph.delete_node(skey)
                print(f"â™»ï¸ å·²æ¸…ç†æ—§æ‘˜è¦: {skey}")
            except KeyError:
                continue
                
        # æ›´æ–°å†…å­˜ç´¢å¼•
        self.node_info = [n for n in self.node_info if n["key"] not in old_summaries]
        self.node_embeddings = np.array([
            emb for emb, n in zip(self.node_embeddings, self.node_info)
        ])

    def generate_advanced_plan(self, query: str) -> Dict:
        schema_description = """
        èŠ‚ç‚¹ç±»å‹ (Node Types):
        1. paper_root: è®ºæ–‡æ ¹èŠ‚ç‚¹ï¼ˆåŒ…å«æ ‡é¢˜ã€ä½œè€…ï¼‰
        2. abstract: è®ºæ–‡æ‘˜è¦
        3. chapter: è®ºæ–‡ç« èŠ‚ï¼ˆåŒ…å«æ®µè½å’Œå›¾ç‰‡ï¼‰
        4. paragraph: ç« èŠ‚æ®µè½
        5. reference: å‚è€ƒæ–‡çŒ®
        6. table: è¡¨æ ¼æ•°æ®
        7. image: å›¾è¡¨ä¿¡æ¯

        å…³ç³»ç±»å‹ (Relation Types):
        1. has_abstract: è®ºæ–‡â†’æ‘˜è¦
        2. has_chapter: è®ºæ–‡â†’ç« èŠ‚
        3. has_paragraph: ç« èŠ‚â†’æ®µè½
        4. has_image: ç« èŠ‚â†’å›¾ç‰‡
        5. has_table: è®ºæ–‡â†’è¡¨æ ¼
        6. references: è®ºæ–‡â†’å‚è€ƒæ–‡çŒ®
        7. similar: èŠ‚ç‚¹ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼å…³ç³»
        8. has_summary: é€šè¿‡refine_knowledgeæ–¹æ³•ç”Ÿæˆçš„æ‘˜è¦å…³ç³»ï¼ˆä»…å½“è°ƒç”¨refine_knowledgeåå­˜åœ¨ï¼‰
        """

        function_descriptions = """
        å¯ç”¨å‡½æ•°ï¼š
        1. search_units(
            entity_type: å®ä½“ç±»å‹,
            [attribute_filters]: å±æ€§è¿‡æ»¤ï¼ˆå­—å…¸ï¼‰,
            [relation_filters]: å…³ç³»è¿‡æ»¤ï¼ˆå­—å…¸ï¼‰,
            [keywords]: å…³é”®è¯è¯­ä¹‰æœç´¢,
            [top_k]: è¿”å›æ•°é‡ï¼ˆé»˜è®¤15ï¼‰
        ) -> åŒ¹é…èŠ‚ç‚¹åˆ—è¡¨

        2. traverse_relations(
            source_keys: èµ·å§‹èŠ‚ç‚¹åˆ—è¡¨,
            relation_types: è¦éå†çš„å…³ç³»ç±»å‹åˆ—è¡¨,
            target_types: ç›®æ ‡èŠ‚ç‚¹ç±»å‹åˆ—è¡¨,
            [max_depth]: æœ€å¤§éå†æ·±åº¦ï¼ˆé»˜è®¤2ï¼‰
        ) -> æŒ‰ç±»å‹åˆ†ç»„çš„èŠ‚ç‚¹å­—å…¸

        3. semantic_traversal(
            source_keys: èµ·å§‹èŠ‚ç‚¹åˆ—è¡¨,
            [top_k]: æ¯å±‚æ‰©å±•æ•°é‡ï¼ˆé»˜è®¤3ï¼‰,
            [depth]: éå†æ·±åº¦ï¼ˆé»˜è®¤2ï¼‰
        ) -> ç›¸ä¼¼èŠ‚ç‚¹å­—å…¸ï¼ˆkeyä¸ºæºèŠ‚ç‚¹ï¼‰

        4. refine_knowledge(
            node_keys: è¦æç‚¼çš„èŠ‚ç‚¹åˆ—è¡¨
        ) -> æ–°ç”Ÿæˆçš„æ‘˜è¦èŠ‚ç‚¹åˆ—è¡¨ï¼ˆä¼šè‡ªåŠ¨åˆ›å»ºhas_summaryå…³ç³»ï¼‰
        """

        full_example = """
        å®Œæ•´ç¤ºä¾‹ï¼š
        ç”¨æˆ·æŸ¥è¯¢ï¼š"æ‰¾å…³äºæ·±åº¦å­¦ä¹ çš„è®ºæ–‡ï¼Œå‘ç°ç›¸å…³ç ”ç©¶å¹¶ç”Ÿæˆæ‘˜è¦"

        å“åº”è®¡åˆ’ï¼š
        {
            "steps": [
                {
                    "function": "search_units",
                    "params": {
                        "entity_type": "paper_root",
                        "keywords": "æ·±åº¦å­¦ä¹  ç¥ç»ç½‘ç»œ",
                        "top_k": 15
                    },
                    "output_key": "core_papers"
                },
                {
                    "function": "traverse_relations",
                    "params": {
                        "source_keys": "$core_papers",
                        "relation_types": ["references"],
                        "target_types": ["reference"],
                        "max_depth": 1
                    },
                    "output_key": "related_refs"
                },
                {
                    "function": "semantic_traversal",
                    "params": {
                        "source_keys": "$core_papers",
                        "top_k": 10,
                        "depth": 1
                    },
                    "output_key": "similar_studies"
                },
                {
                    "function": "refine_knowledge",
                    "params": {
                        "node_keys": "$core_papers"
                    },
                    "output_key": "paper_summaries"
                }
            ]
        }
        """

        prompt = f'''
        # Task
        Generate execution plan strictly following these rules:

        ## Language Rules
        1. Accept both Chinese and English queries
        2. Always respond with valid JSON format
        3. Never include explanations or comments

        ## Knowledge Graph Structure
        {schema_description}

        ## Function Examples
        {function_descriptions}

        ## User Query
        {query}

        ## Response Example
        {full_example}
    '''
        try:
            response = self._call_llm(prompt).strip()
            plan = self._parse_plan(response)
            self.logger.info(f"æˆåŠŸä¸ºæŸ¥è¯¢ '{query}' ç”Ÿæˆé«˜çº§è®¡åˆ’ï¼ŒåŒ…å« {len(plan.get('steps', []))} ä¸ªæ­¥éª¤")
            return plan
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè®¡åˆ’å¤±è´¥: {str(e)}")
            return {"steps": []}

    def _parse_plan(self, response: str) -> Dict:
        """è§£ææ–¹æ³•"""
        try:
            cleaned = re.sub(r'[â€œâ€]', '"', response)  # ç»Ÿä¸€å¼•å·
            cleaned = re.sub(r'/\*.*?\*/', '', cleaned, flags=re.DOTALL)  # ç§»é™¤æ³¨é‡Š
            cleaned = re.sub(r'(?<!\\)\\(?!["\\/bfnrt]|u[0-9a-fA-F]{4})', '', cleaned)  # æ¸…ç†è½¬ä¹‰

            # å°è¯•å¤šç§æ–¹å¼æå–JSON
            for candidate in re.findall(r'\{.*\}', cleaned, re.DOTALL):
                try:
                    plan = json.loads(candidate)
                    if "steps" in plan and isinstance(plan["steps"], list):
                        self.logger.info("æˆåŠŸè§£ææŸ¥è¯¢è®¡åˆ’")
                        return plan
                except:
                    continue
            raise ValueError("No valid JSON found")

        except Exception as e:
            self.logger.error(f"è§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”ï¼š\n{response[:300]}...")
            raise ValueError(f"è®¡åˆ’è§£æå¤±è´¥: {str(e)}")

    def _print_step_result(self, step: Dict, result: Any):
        """å®‰å…¨æ‰“å°ç»“æœ"""
        self.logger.info(f"ğŸ” {step['output_key']} ({step['function']})")

        if isinstance(result, dict):
            total_nodes = sum(len(v) for v in result.values())
            self.logger.info(f"å‘ç° {total_nodes} ä¸ªèŠ‚ç‚¹")
            for k, v in result.items():
                self.logger.info(f"  {k}:")
                for i, item in enumerate(v[:3], 1):
                    # ç±»å‹å®‰å…¨æ£€æŸ¥
                    if isinstance(item, dict):
                        self.logger.info(f"    {i}. [{item.get('type', '?')}] {item.get('key', 'æœªçŸ¥é”®')}")
                        self.logger.info(f"      å†…å®¹: {str(item.get('content', 'æ— å†…å®¹'))[:80]}...")
                    else:
                        self.logger.info(f"    {i}. [æ— æ•ˆæ ¼å¼] {str(item)[:80]}")
        elif isinstance(result, list):
            self.logger.info(f"è·å¾— {len(result)} ä¸ªç»“æœ")
            for i, item in enumerate(result[:5], 1):
                if isinstance(item, dict):
                    self.logger.info(f"  {i}. [{item.get('type', '?')}] {item.get('key', 'æœªçŸ¥é”®')}")
                else:
                    self.logger.info(f"  {i}. [æ— æ•ˆæ ¼å¼] {str(item)[:80]}")
        else:
            self.logger.info("æœªçŸ¥ç»“æœæ ¼å¼")
        self.logger.info("â”" * 50)

    def execute_advanced_plan(self, plan: Dict) -> Dict:
        """è®¡åˆ’æ‰§è¡Œæ–¹æ³•"""
        context = {}
        for step in plan.get("steps", []):
            try:
                self.logger.info(f"\nâ–¶ï¸ å¼€å§‹æ‰§è¡Œæ­¥éª¤ï¼š{step['output_key']}")
                self.logger.info(f"  åŸå§‹å‚æ•°: {json.dumps(step['params'], indent=2)}")

                # å‚æ•°è§£æ
                params = self._resolve_params(step["params"], context)
                self.logger.info(f"  è§£æåå‚æ•°ç±»å‹: { {k: type(v) for k, v in params.items()} }")
                self.logger.info(f"  è§£æåå‚æ•°å€¼æ ·ä¾‹: { {k: v[:3] if isinstance(v, list) else v for k, v in params.items()} }")

                # æ‰§è¡Œå‡½æ•°
                if step["function"] == "traverse_relations":
                    self.logger.info("æ‰§è¡Œå‰source_keysæ ·æœ¬:", params["source_keys"][:2] if params["source_keys"] else "ç©ºåˆ—è¡¨")

                if step["function"] == "search_units":
                    result = self.search_units(**params)
                elif step["function"] == "traverse_relations":
                    result = self.traverse_relations(**params)
                elif step["function"] == "semantic_traversal":
                    result = self.semantic_traversal(**params)
                elif step["function"] == "refine_knowledge":
                    result = self.refine_knowledge(**params)
                else:
                    raise ValueError(f"æœªçŸ¥å‡½æ•°: {step['function']}")

                # æ‰“å°ç»“æœ
                self._print_step_result(step, result)
                context[step["output_key"]] = result

            except Exception as e:
                self.logger.error(f"âŒ æ­¥éª¤æ‰§è¡Œå¤±è´¥: {str(e)}")
                context[step["output_key"]] = []
                # éƒ¨åˆ†ç»§ç»­æ‰§è¡Œ
                if isinstance(e, KeyError):
                    self.logger.warning("âš ï¸ å¿½ç•¥è¯¥æ­¥éª¤ç»§ç»­æ‰§è¡Œ...")
                else:
                    import traceback
                    traceback.print_exc()
                    break  # ä¸¥é‡é”™è¯¯ç»ˆæ­¢

        self.logger.info("é«˜çº§è®¡åˆ’æ‰§è¡Œå®Œæˆ")
        return context
    
    ##############################æµ‹è¯•æ–¹æ³•################################
    def validate_traverse_relations(self,
                                  source_key: str,
                                  relation_type: str,
                                  target_type: str,
                                  expected_min: int=1) -> Dict:
        """
        éªŒè¯æ–¹æ³•
        
        å‚æ•°ï¼š
        - source_key: æºèŠ‚ç‚¹é”®ï¼ˆå¦‚ "2404.13501v1_title_authors"ï¼‰
        - relation_type: é¢„æœŸçš„å…³ç³»ç±»å‹ï¼ˆå¦‚ "has_abstract"ï¼‰
        - target_type: é¢„æœŸçš„ç›®æ ‡èŠ‚ç‚¹ç±»å‹ï¼ˆå¦‚ "abstract"ï¼‰
        - expected_min: æœŸæœ›æ‰¾åˆ°çš„æœ€å°èŠ‚ç‚¹æ•°
        
        è¿”å›ï¼š
        {
            "total_nodes": æ€»æ•°,
            "valid_nodes": æœ‰æ•ˆèŠ‚ç‚¹åˆ—è¡¨,
            "invalid_nodes": æ— æ•ˆèŠ‚ç‚¹åˆ—è¡¨,
            "type_mismatch": ç±»å‹ä¸åŒ¹é…åˆ—è¡¨,
            "relation_issues": å…³ç³»é—®é¢˜åˆ—è¡¨,
            "expectation_met": æ˜¯å¦æ»¡è¶³æœŸæœ›èŠ‚ç‚¹æ•°,
            "validation_details": è¯¦ç»†éªŒè¯æ•°æ®
        }
        """
        # ====================== åˆå§‹åŒ–é˜¶æ®µ ======================
        self.logger.info(
            f"[VALIDATE] å¯åŠ¨éªŒè¯ | æº: {source_key} "
            f"å…³ç³»: {relation_type}->{self._normalize_relation(relation_type)} "
            f"ç›®æ ‡ç±»å‹: {target_type}->{self._normalize_node_type(target_type)}"
        )

        # ====================== æ‰§è¡Œéå† ======================
        traversal_results = self.traverse_relations(
            source_keys=[source_key],
            relation_types=[relation_type],
            target_types=[target_type],
            max_depth=2
        )
        found_nodes = traversal_results.get(self._normalize_node_type(target_type), [])
        
        result = {
            "total_nodes": len(found_nodes),
            "valid_nodes": [],
            "invalid_nodes": [],
            "type_mismatch": [],
            "relation_issues": [],
            "expectation_met": len(found_nodes) >= expected_min,
            "validation_details": []
        }
        if not self._validate_node_exists(source_key):
            self.logger.error(f"æºèŠ‚ç‚¹ä¸å­˜åœ¨: {source_key}")
            return {
                "error": f"Source node {source_key} not found",
                **result
            }
        # ====================== èŠ‚ç‚¹éªŒè¯å¾ªç¯ ======================
        for idx, node in enumerate(found_nodes, 1):
            node_key = node["key"]
            entry = {
                "node": node_key,
                "content": node.get("content"),
                "validation_steps": {}
            }

            try:
                # ------------------- èŠ‚ç‚¹å­˜åœ¨æ€§éªŒè¯ -------------------
                if not self._validate_node_exists(node_key):
                    result["invalid_nodes"].append(node_key)
                    entry["validation_steps"]["existence"] = False
                    continue
                entry["validation_steps"]["existence"] = True

                # ------------------- ç±»å‹åŒ¹é…éªŒè¯ -------------------
                actual_type = self._normalize_node_type(node["type"])
                expected_type = self._normalize_node_type(target_type)
                type_valid = actual_type == expected_type
                
                entry["validation_steps"].update({
                    "type_check": {
                        "expected": expected_type,
                        "actual": actual_type,
                        "valid": type_valid
                    }
                })
                
                if not type_valid:
                    result["type_mismatch"].append(node_key)
                    self.logger.warning(
                        f"[VALIDATE] ç±»å‹ä¸åŒ¹é… | èŠ‚ç‚¹: {node_key} "
                        f"é¢„æœŸ: {expected_type} å®é™…: {actual_type}"
                    )
                    continue

                # ------------------- å…³ç³»è·¯å¾„éªŒè¯ -------------------
                path_valid, path_details = self._validate_relation_path_with_details(
                    source_key, node_key, relation_type
                )
                entry["validation_steps"]["path_check"] = {
                    "valid": path_valid,
                    "path": path_details
                }

                if not path_valid:
                    result["relation_issues"].append({
                        "node": node_key,
                        "path": path_details
                    })
                    self.logger.warning(
                        f"[VALIDATE] è·¯å¾„éªŒè¯å¤±è´¥ | èŠ‚ç‚¹: {node_key}\n"
                        f"å®é™…è·¯å¾„: {json.dumps(path_details, indent=2)}"
                    )
                    continue

                # ------------------- æœ€ç»ˆæœ‰æ•ˆèŠ‚ç‚¹ -------------------
                result["valid_nodes"].append(node_key)
                entry["validation_steps"]["final_result"] = "VALID"
                
            except Exception as e:
                entry["validation_steps"]["error"] = str(e)
                self.logger.error(f"éªŒè¯èŠ‚ç‚¹ {node_key} æ—¶å‡ºé”™: {str(e)}")
            
            result["validation_details"].append(entry)

        # ====================== ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š ======================
        validation_rate = len(result["valid_nodes"]) / result["total_nodes"] if result["total_nodes"] > 0 else 0
        self.logger.info(
            f"[VALIDATE] éªŒè¯å®Œæˆ | æœ‰æ•ˆç‡: {validation_rate:.2%} "
            f"æœ‰æ•ˆ/æ€»æ•°: {len(result['valid_nodes'])}/{result['total_nodes']} "
            f"è¾¾æ ‡: {result['expectation_met']}"
        )
        
        return result
    def _get_child_relations(self, node_key: str) -> Dict[str, List]:
        """è·å–å­èŠ‚ç‚¹å…³ç³»"""
        return self.graph.semantic_graph.graph_relations.get(
            node_key, {}
        ).get("children", {})
    
    def _validate_relation_path_with_details(self, source: str, target: str, expected_rel: str) -> tuple[bool, List]:
        """è·¯å¾„éªŒè¯"""
        expected_rel = self._normalize_relation(expected_rel)
        valid_paths = []
        
        # æ­£å‘è·¯å¾„æœç´¢ï¼ˆä»sourceåˆ°targetï¼‰
        def dfs(current: str, path: List[Dict], found_rel: bool):
            if current == target:
                if found_rel:
                    valid_paths.append(path.copy())
                return
            
            for raw_rel, children in self._get_child_relations(current).items():
                for child in children:
                    if child in [p["to"] for p in path]:  # é¿å…å¾ªç¯
                        continue
                        
                    # æ ‡å‡†åŒ–å½“å‰å…³ç³»
                    norm_rel = self._normalize_relation(raw_rel)
                    new_found = found_rel or (norm_rel == expected_rel)
                    
                    new_step = {
                        "from": current,
                        "to": child,
                        "relation": norm_rel,
                        "original_relation": raw_rel
                    }
                    
                    dfs(child, path + [new_step], new_found)

        dfs(source, [], False)
        return (len(valid_paths) > 0, valid_paths)

    def _validate_node_exists(self, node_key: str) -> bool:
        """èŠ‚ç‚¹å­˜åœ¨æ€§æ£€æŸ¥"""
        return any(
            node[0] == node_key
            for node in self.graph.semantic_graph.semantic_map.data
        )

    def _get_relation_types(self, child: str, parent: str) -> List[str]:
        """è·å–ä¸¤ä¸ªèŠ‚ç‚¹é—´çš„æ‰€æœ‰å…³ç³»ç±»å‹"""
        return [
            rel for rel, children 
            in self.graph.semantic_graph.graph_relations.get(parent, {}).get("children", {}).items()
            if child in children
        ]

    def _get_relation_path(self, source: str, target: str) -> List[Dict]:
        """è·å–è¯¦ç»†å…³ç³»è·¯å¾„ï¼ˆç”¨äºæ—¥å¿—ï¼‰"""
        path = []
        current = target
        while current != source:
            parents = self.graph.semantic_graph.graph_relations.get(current, {}).get("parents", {})
            if not parents:
                break
            # å–ç¬¬ä¸€ä¸ªçˆ¶èŠ‚ç‚¹
            for rel, parent_list in parents.items():
                if parent_list:
                    current = parent_list[0]
                    path.append({
                        "from": current,
                        "to": path[-1]["from"] if path else target,
                        "relation": rel
                    })
                    break
        return path[::-1]  # åå‘æ˜¾ç¤ºè·¯å¾„

    def validate_refine_knowledge(self, 
                                node_key: str, 
                                min_summary_length: int = 50) -> Dict:
        """
        éªŒè¯çŸ¥è¯†æç‚¼åŠŸèƒ½
        :param node_key: è¦æµ‹è¯•çš„èŠ‚ç‚¹é”®ï¼ˆå¦‚"2301.12345v1_chapter_1"ï¼‰
        :param min_summary_length: æ‘˜è¦æœ€å°é•¿åº¦è¦æ±‚
        :return: åŒ…å«éªŒè¯ç»“æœçš„å­—å…¸
        """
        result = {
            "original_exists": False,
            "new_nodes": [],
            "relations_added": False,
            "index_updated": False,
            "content_valid": False,
            "errors": []
        }

        try:
            # æ£€æŸ¥åŸå§‹èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨
            original_node = next((n for n in self.graph.semantic_graph.semantic_map.data if n[0] == node_key), None)
            if not original_node:
                result["errors"].append(f"åŸå§‹èŠ‚ç‚¹ {node_key} ä¸å­˜åœ¨")
                return result
            result["original_exists"] = True

            # æ‰§è¡ŒçŸ¥è¯†æç‚¼
            new_nodes = self.refine_knowledge([node_key])
            result["new_nodes"] = new_nodes

            # éªŒè¯æ–°èŠ‚ç‚¹
            if not new_nodes:
                result["errors"].append("æœªç”Ÿæˆä»»ä½•æ‘˜è¦èŠ‚ç‚¹")
                return result

            summary_key = new_nodes[0]
            summary_node = next((n for n in self.graph.semantic_graph.semantic_map.data if n[0] == summary_key), None)

            # å­˜åœ¨æ€§æ£€æŸ¥
            if not summary_node:
                result["errors"].append(f"æ‘˜è¦èŠ‚ç‚¹ {summary_key} æœªæ’å…¥")
                return result

            # å…³ç³»éªŒè¯
            relations = self.graph.semantic_graph.graph_relations.get(node_key, {})
            result["relations_added"] = "has_summary" in relations.get("children", {})
            if not result["relations_added"]:
                result["errors"].append(f"ç¼ºå¤± {node_key} -> {summary_key} çš„ has_summary å…³ç³»")

            # ç´¢å¼•éªŒè¯
            result["index_updated"] = any(n["key"] == summary_key for n in self.node_info)
            if not result["index_updated"]:
                result["errors"].append("æ‘˜è¦èŠ‚ç‚¹æœªåŠ å…¥ç´¢å¼•")

            # å†…å®¹éªŒè¯
            summary_content = str(summary_node[1])
            result["content_valid"] = len(summary_content) >= min_summary_length
            if not result["content_valid"]:
                result["errors"].append(f"æ‘˜è¦å†…å®¹è¿‡çŸ­ ({len(summary_content)}<{min_summary_length})")

        except Exception as e:
            result["errors"].append(f"éªŒè¯å¼‚å¸¸: {str(e)}")
            traceback.print_exc()

        return result
    def validate_relation_traversal_extended(self):
        """æ‰©å±•å…³ç³»éå†æµ‹è¯•"""
        test_cases = [
            {
                "source_key": "2404.13501v1_title_authors",
                "relation_type": "has_chapter",
                "target_type": "chapter",
                "expected_min": 3
            },
            {
                "source_key": "2404.13501v1_chapter_1",
                "relation_type": "has_paragraph",
                "target_type": "paragraph",
                "expected_min": 5
            }
        ]
        
        for case in test_cases:
            result = self.validate_traverse_relations(
                case["source_key"],
                case["relation_type"],
                case["target_type"],
                case["expected_min"]
            )
            print(f"\næµ‹è¯•æ¡ˆä¾‹ {case['relation_type']}:")
            print(json.dumps(result, indent=2))

    def validate_summary_generation_diverse(self):
        """å¤šæ ·åŒ–æ‘˜è¦ç”Ÿæˆæµ‹è¯•"""
        test_nodes = [
            "2404.13501v1_chapter_0_para_0",
            "2404.13501v1_chapter_0_para_1"
        ]
        
        for node_key in test_nodes:
            result = self.validate_refine_knowledge(node_key)
            print(f"\næµ‹è¯•èŠ‚ç‚¹ {node_key}:")
            print(json.dumps(result, indent=2))


    def visualize_graph(self):
        """
        å°†å†…éƒ¨çš„ graph è¿›è¡Œå›¾ç¤ºåŒ–
        """
        # åˆ›å»ºä¸€ä¸ªæœ‰å‘å›¾å¯¹è±¡
        G = nx.DiGraph()

        # æ·»åŠ èŠ‚ç‚¹
        for node in self.graph.semantic_graph.semantic_map.data:
            node_key = node[0]
            node_type = self._get_node_type(node_key)
            G.add_node(node_key, type=node_type)

        # æ·»åŠ è¾¹
        for node_key, relations in self.graph.semantic_graph.graph_relations.items():
            children = relations.get("children", {})
            for rel_type, child_nodes in children.items():
                std_rel = self._normalize_relation(rel_type)
                for child_node in child_nodes:
                    G.add_edge(node_key, child_node, relation=std_rel)

            for dst, rel in relations.get("links", {}).items():
                normalized_rel = self._normalize_relation(rel)
                G.add_edge(node_key, dst,relation=normalized_rel)

        # è®¾ç½®èŠ‚ç‚¹é¢œè‰²å’Œå¤§å°
        node_colors = []
        node_sizes = []
        for node, data in G.nodes(data=True):
            node_type = data.get('type', 'unknown')
            if node_type == 'paper_root':
                node_colors.append('lightblue')
                node_sizes.append(800)
            elif node_type == 'abstract':
                node_colors.append('lightgreen')
                node_sizes.append(600)
            elif node_type == 'chapter':
                node_colors.append('orange')
                node_sizes.append(700)
            elif node_type == 'paragraph':
                node_colors.append('pink')
                node_sizes.append(500)
            elif node_type == 'reference':
                node_colors.append('purple')
                node_sizes.append(500)
            elif node_type == 'table':
                node_colors.append('yellow')
                node_sizes.append(500)
            elif node_type == 'image':
                node_colors.append('gray')
                node_sizes.append(500)
            elif node_type == 'summary':
                node_colors.append('brown')
                node_sizes.append(500)
            else:
                node_colors.append('lightgray')
                node_sizes.append(500)

        # ç»˜åˆ¶å›¾
        pos = nx.spring_layout(G)
        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes)
        nx.draw_networkx_edges(G, pos)
        nx.draw_networkx_labels(G, pos)
        edge_labels = nx.get_edge_attributes(G, 'relation')
        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)

        # è®¾ç½®å›¾å½¢æ ‡é¢˜
        plt.title('Arxiv Semantic Graph Visualization')

        # æ˜¾ç¤ºå›¾å½¢
        plt.show()

    
def create_simple_graph():
    graph = ArxivSemanticGraph()
    sample_data = {
    "paper_id": "2404.13501v1",
    "title": "Research on AI in Healthcare",
    "authors": ["Alice Smith", "Bob Johnson"],
    "abstract": "Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss â€œwhat isâ€ and â€œwhy do we needâ€ the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at https://github.com/nuster1128/LLM_Agent_Memory_Survey",
    "chapters": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Recently, large language models (LLMs) have achieved remarkable success in a large number of domains, ranging from artificial intelligence and software engineering to education and social science [1, 2, 3]. Original LLMs usually accomplish different tasks without interacting with environments. However, to achieve the final goal of artificial general intelligence (AGI), intelligent machines should be able to improve themselves by autonomously exploring and learning from the real world. For example, if a trip-planning agent intends to book a ticket, it should send an order request to the ticket website, and observe the response before taking the next action. A personal assistant agent should adjust its behaviors according to the userâ€™s feedback, providing personalized responses to improve userâ€™s satisfaction. To further push the boundary of LLMs towards AGI, recent years have witnessed a large number of studies on LLM-based agents [3, 4], where the key is to equip LLMs with additional modules to enhance their self-evolving capability in real-world environments.",
                "Among all the added modules, memory is a key component that differentiates the agents from original LLMs, making an agent truly an agent (see Figure 1). It plays an extremely important role in determining how the agent accumulates knowledge, processes historical experience, retrieves informative knowledge to support its actions, and so on. Around the memory module, people have devoted much effort to designing its information sources, storage forms, and operation mechanisms. For example, Shinn et al. [5] incorporate both in-trial and cross-trial information to build the memory module for enhancing the agentâ€™s reasoning capability. Zhong et al. [6] store memory information in the form of natural languages, which is explainable and friendly to the users. Modarressi et al. [7] design both memory reading and writing operations to interact with environments for task solving."
            ],
            "images": []
        }   
    ],
    "references": [
        "Smith, A. (2023). Recent Advances in Medical AI. Journal of Healthcare AI.",
        "Johnson, B. (2022). Deep Learning for Image Analysis. Cambridge University Press."
    ]
    }

    graph.insert(**sample_data)
    return graph

# åˆå§‹åŒ–æµ‹è¯•æ•°æ®
# graph = parse_local_paper()
# å¦‚æœè¿›è¡Œå›¾ç¤ºåŒ–,ä½¿ç”¨è¯¥æµ‹è¯•æ•°æ®
# agent = ArxivAgent(create_simple_graph())

# æµ‹è¯•å…³ç³»éå†åŠŸèƒ½
# traverse_result = agent.validate_traverse_relations(
#     source_key="2404.13501v1_title_authors",
#     relation_type="has_abstract",
#     target_type="abstract"
# )
# print("å…³ç³»éå†éªŒè¯ç»“æœï¼š")
# print(json.dumps(traverse_result, indent=2, ensure_ascii=False))
# agent.validate_relation_traversal_extended()
#æµ‹è¯•çŸ¥è¯†æç‚¼åŠŸèƒ½
# refine_result = agent.validate_refine_knowledge(
#     node_key="2404.13501v1_abstract"
# )
# print("\nçŸ¥è¯†æç‚¼éªŒè¯ç»“æœï¼š")
# print(json.dumps(refine_result, indent=2, ensure_ascii=False))
# agent.validate_summary_generation_diverse()
# agent.visualize_graph()
#################ä»¥ä¸Šæ˜¯æµ‹è¯•traverse_relationåŠrefine_knowledgeæ–¹æ³•çš„ä»£ç ###################################################


if __name__ == "__main__":
    graph = parse_local_paper()
    agent = ArxivAgent(graph)

    while True:
        try:
            query = input("\nè¯·è¾“å…¥æŸ¥è¯¢ï¼ˆè¾“å…¥qé€€å‡ºï¼‰: ")
            if query.lower() == 'q':
                break

            agent.logger.info("\nç”ŸæˆæŸ¥è¯¢è®¡åˆ’ä¸­...")
            plan = agent.generate_advanced_plan(query)
            agent.logger.info("ç”Ÿæˆçš„è®¡åˆ’: " + json.dumps(plan, indent=2, ensure_ascii=False))

            agent.logger.info("\næ‰§è¡ŒæŸ¥è¯¢è®¡åˆ’...")
            results = agent.execute_advanced_plan(plan)

            agent.logger.info("\nç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            report = agent.generate_report(results, query)
            agent.logger.info("\n=== æœ€ç»ˆæŠ¥å‘Š ===")
            agent.logger.info(report)

            with open(f"report_{int(time.time())}.txt", "w") as f:
                f.write(report)

        except KeyboardInterrupt:
            agent.logger.info("\næ“ä½œå·²å–æ¶ˆ")
            continue
        except Exception as e:
            agent.logger.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")